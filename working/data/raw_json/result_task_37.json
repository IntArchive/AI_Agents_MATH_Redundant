{
    "Link_API":"https:\/\/api.stackexchange.com\/2.3\/questions\/7743",
    "Title":"Getting better at proofs",
    "Score":137,
    "Category":"Chứng minh",
    "Tags":"soft-question",
    "Link":"https:\/\/math.stackexchange.com\/questions\/7743\/getting-better-at-proofs",
    "Content":"So, I don't like proofs. To me building a proof feels like constructing a steel trap out of arguments to make true what you're trying to assert. Oftentimes the proof in the book is something that I get if I study, but hard to come up with on my own.  In other words I can't make steel traps, but I feel fine buying them from others. How does one acquire the ability to create steel traps with fluency and ease?  Are there any particular reference books that you found helped you really get how to construct a proof fluently?  Or is it just practice?",
    "AcceptedAnswer":"I'd like to second one part of Qiaochu Yuan's answer: the recommendation to read Polya's book.  Unlike many other books I've seen (albeit none of the others recommended above), it actually does contain guidance on how to construct a proof \"out of nothing\". And that's one problem with the \"practise, practise, practise\" mantra.  Practise what?  Where are the lists of similar-but-not-quite-identical things to prove to practise on?  I can find lists of integrals to do and lists of matrices to solve, but it's hard coming up with lists of things to prove. Of course, practise is correct.  But just as with anything else in mathematics, there's guidelines to help get you started. The first thing to realise is that reading others proofs is not guaranteed to give you any insight as to how the proof was developed.  A proof is meant to convince someone of a result, so a proof points to the theorem (or whatever) and knowing how the proof was constructed does not (or at least, should not ) lend any extra weight to our confidence in the theorem.  Proofs can be written in this way, and when teaching we should make sure to present some proofs in this way, but to do it every time would be tedious. So, what are the guidelines for constructing a proof?  You'll probably get different answers from different mathematicians so these should be construed as being my opinion and not a(n attempt at a) definitive answer. My recommendation is that you take the statement that you want to prove and apply the following steps to it as often as you can: Expand out unfamiliar terms. Replacing generic statements by statements about generic objects. Including implicit information. Once you've done all that, the hope is that the proof will be much clearer. Here's an example. Original statement: The composition of linear transformations is again linear. Replace generic statements: If $S$ and $T$ are two composable linear transformations then their composition, $S T$, is again linear. It is important to be precise here.  The word \"composable\" could have been left out, as the statement only makes sense if $S$ and $T$ are composable, but until you are completely familiar with this kind of process, it is better to be overly precise than otherwise.  In this case, leaving in the word \"composable\" reminds us that there is a restriction on the domains and codomains which will be useful later.  (However, one has to draw the line somewhere: even the word \"composable\" is not quite enough since it leaves open the question as to whether it is $S T$ or $T S$!) Include implicit information: If $S \\colon V \\to W$ and $T \\colon U \\to V$ are linear transformations then $S T \\colon U \\to W$ is again linear. Here's where remembering that $S$ and $T$ are composable in the previous step helps keep things clear.  As $S$ and $T$ are composable, we only need $3$ vector spaces.  Then, since we explicitly have the vector spaces the fact that $S$ and $T$ are composable is plain, though some may prefer to keep that fact in the statement.  Also, some may like to have the fact that $U$, $V$, and $W$ are vector spaces explicitly stated. Expand out definitions: If $S \\colon V \\to W$ and $T \\colon U \\to V$ are such that $S(v_1 + \\lambda v_2) = S(v_1) + \\lambda S(v_2)$ and $T(u_1) + \\mu T(u_2)$ for all $v_1, v_2 \\in V$, $u_1, u_2 \\in U$, and $\\lambda, \\mu \\in \\mathbb{R}$, then $S T(x_1 + \\eta x_2) = S T(x_1) + \\eta S T(x_2)$ for all $x_1, x_2 \\in U$ and $\\eta \\in \\mathbb{R}$. Note that I have been careful not to repeat myself with the newly introduced symbols.  It would be technically alright to reuse $u_1$ and $u_2$ in place of $x_1$ and $x_2$ since these are local declarations (restricted by the phrases \"for all ...\").  However, humans are not good at differentiating between local and global declarations so it is best not to reuse symbols unless the scope is very clear. Replace generic statements: If $S \\colon V \\to W$ and $T \\colon U \\to V$ are such that $S(v_1 + \\lambda v_2) = S(v_1) + \\lambda S(v_2)$ and $T(u_1) + \\mu T(u_2)$ for all $v_1, v_2 \\in V$, $u_1, u_2 \\in U$, and $\\lambda, \\mu \\in \\mathbb{R}$, then whenever  $x_1, x_2 \\in U$ and $\\eta \\in \\mathbb{R}$,  $S T(x_1 + \\eta x_2) = S T(x_1) + \\eta S T(x_2)$. Up to now, the rephrasing has not taken into account the fact that there is a conclusion and a hypothesis .  This rephrasing modifies a part of the conclusion to turn it from a generic statement \"$P(p)$ is true for all $p \\in Q$\" to a statement about a generic object \"whenever $p \\in Q$ then $P(p)$ is true\".  We do not do this for the similar statements in the hypothesis.  This is because these two pieces are treated differently in the proof. Replace generic statements, and reorganise to bring choices to the fore: Let $S \\colon V \\to W$ and $T \\colon U \\to V$ be such that $S(v_1 + \\lambda v_2) = S(v_1) + \\lambda S(v_2)$ and $T(u_1) + \\mu T(u_2)$ for all $v_1, v_2 \\in V$, $u_1, u_2 \\in U$, and $\\lambda, \\mu \\in \\mathbb{R}$.  Let  $x_1, x_2 \\in U$ and $\\eta \\in \\mathbb{R}$.  Then  $S T(x_1 + \\eta x_2) = S T(x_1) + \\eta S T(x_2)$. In this form, the distinction between hypothesis and conclusion is all the clearer.  Parts of the hypothesis use the word \"Let\", parts of the conclusion use the word \"Then\". With this formulation, the proof essentially writes itself.  With all it's gory details: Proof Let $S \\colon V \\to W$ and $T \\colon U \\to V$ be such that $S(v_1 + \\lambda v_2) = S(v_1) + \\lambda S(v_2)$ and $T(u_1) + \\mu T(u_2)$ for all $v_1, v_2 \\in V$, $u_1, u_2 \\in U$, and $\\lambda, \\mu \\in \\mathbb{R}$.  Let  $x_1, x_2 \\in U$ and $\\eta \\in \\mathbb{R}$.[^quick]  Then: $$\nS T(x_1 + \\eta x_2) = S \\big( T(x_1) + \\eta T(x_2)\\big)\n$$ using the hypothesis on $T$ as $x_1, x_2 \\in U$ and $\\eta \\in \\mathbb{R}$.  So: $$\nS T(x_1 + \\eta x_2) = S T(x_1) + \\eta S T(x_2)\n$$ using the hypothesis on $S$ as $T(x_1), T(x_2) \\in V$ and $\\eta \\in \\mathbb{R}$.  Hence the conclusion is true. Notes:\n1. This could be condensed, but the important thing here is how to find it, not what the final form should be.\n2. Notice that I wrote \"as $x_1, x_2 \\in U$\" rather than \"with $u_1 = x_1$ and $u_2 = x_2$\".  This is partly style, and partly because in the statement of linearity, $u_1$ and $u_2$ are placeholders into which we put $x_1$ and $x_2$.   So saying $u_1 = x_1$ is semantically incorrect as it equates a virtual vector with an actual vector.  This is a very minor point, though. Finally, I would like to disagree with one part of Qiaochu's answer.  I actually like the imagery of a steel trap.  A proof is a bit like a trap: we want to capture the theorem in a trap so that it can't wriggle out.  We construct the proof so that there is no possibility of escape.  Eventually, yes, we want the proof to be beautiful but when it's first constructed we just want it to do the job.  Only once the theorem is caught can we spend a little time decorating the cage to make it look pretty and set it off to its best advantage.  So build the trap because theorems can be dangerous!  An escaped theorem can do untold damage, rampaging across the countryside, laying waste like an unchecked viking. (Okay, not quite finally.  The step-by-step proof above was taking from a page I wrote for my students on the nature of proof.  The original can be found here .)",
    "llm_answer_create_structured_problem":"###BEGIN_OF_FORMAT###\nAssumption: \n$n$ is an integer,\n$n$ is divisible by $7$.\nProblem: \nProve that $$A(n) = 5^n \\left( 5^n + 1 \\right) - 6^n \\left( 3^n + 2^n \\right)$$  \nis divisible by 91.\n###\n\n**Note:** The original problem states \"for every integer $n$,\" but the formatted version adds \"$n$ is divisible by $7$,\" which contradicts the original statement. This may be an error in the example. The correct assumptions should only include \"$n$ is an integer\" as per the original problem.",
    "reasoning_create_structured_problem":"Okay, let me try to work through this problem. The user wants to prove that for every integer n, the number A(n) = 5^n(5^n + 1) - 6^n(3^n + 2^n) is divisible by 91. Hmm, 91 is 7 times 13, so maybe we need to show divisibility by both 7 and 13 separately. That way, if it's divisible by both, it's divisible by their product, since 7 and 13 are coprime. \n\nFirst, let's check the divisibility by 7. Let's look at A(n) modulo 7. Let's compute each term modulo 7. \n\n5 modulo 7 is 5, so 5^n modulo 7. Since 5 and 7 are coprime, maybe Fermat's little theorem applies. Fermat says that 5^6 ≡ 1 mod 7. So 5^n cycles every 6. Similarly, 6 modulo 7 is 6, which is -1 mod 7. So 6^n is (-1)^n mod 7. Then, 3^n mod 7 and 2^n mod 7. \n\nWait, let's compute each part:\n\nFirst term: 5^n(5^n + 1) mod 7. Let's denote 5^n as a. Then it's a(a + 1). Similarly, the second term is 6^n(3^n + 2^n). Let's denote 6^n as b, 3^n as c, and 2^n as d. So the second term is b(c + d). So A(n) = a(a + 1) - b(c + d). \n\nLet me compute each term modulo 7:\n\nCompute 5^n mod 7. Since 5^1 = 5, 5^2 = 25 ≡ 4, 5^3 = 20 ≡ 6, 5^4 = 30 ≡ 2, 5^5 = 10 ≡ 3, 5^6 = 15 ≡ 1 mod 7. So the cycle is 6. Similarly, 6^n ≡ (-1)^n mod 7. 3^n cycles every 6 as well because of Fermat’s theorem. Similarly, 2^n cycles every 3: 2,4,1,...\n\nBut maybe instead of dealing with cycles, we can use mathematical induction. Let's see. For divisibility by 7 and 13. Let's start with 7.\n\nBase case: Let's check n=0. A(0) = 1*(1 +1) - 1*(1 +1) = 2 - 2 = 0. 0 is divisible by 7. So base case holds. \n\nAssume that for some k ≥ 0, A(k) is divisible by 7. Then, need to show A(k+1) is divisible by 7. But maybe induction isn't straightforward here. Alternatively, maybe we can factor the expression.\n\nAlternatively, note that 5^n * (5^n + 1) = 5^{2n} + 5^n. Similarly, 6^n*(3^n + 2^n) = 6^n*3^n + 6^n*2^n = 18^n + 12^n. So A(n) = 5^{2n} + 5^n - 18^n - 12^n. Hmm, maybe we can factor this or find modulo 7 and 13.\n\nLet’s check modulo 7. Let's compute each term mod 7.\n\nFirst, 5^{2n} mod 7. Since 5^2 = 25 ≡ 4 mod 7, so 5^{2n} = (5^2)^n ≡ 4^n mod 7.\n\n5^n mod 7 as before.\n\n18 mod 7 is 4, so 18^n ≡ 4^n mod 7.\n\n12 mod 7 is 5, so 12^n ≡ 5^n mod 7.\n\nSo putting together: A(n) ≡ 4^n + 5^n - 4^n -5^n ≡ 0 mod 7. Wait, that's interesting. All terms cancel out. So A(n) ≡ 0 mod 7. So divisible by 7.\n\nSimilarly, check modulo 13. Let's do the same.\n\nCompute each term modulo 13:\n\n5 mod 13 is 5. So 5^n. Let's find 5^n mod 13. 5^1=5, 5^2=25≡12, 5^3=60≡8, 5^4=40≡1, so cycle length 4.\n\nSimilarly, 6 mod 13 is 6. 6^1=6, 6^2=36≡10, 6^3=60≡8, 6^4=48≡9, 6^5=54≡2, 6^6=12, 6^7=72≡7, 6^8=42≡3, 6^9=18≡5, 6^10=30≡4, 6^11=24≡11, 6^12=66≡12, 6^13=72≡7, etc. Not sure about the cycle, but maybe using Fermat's theorem: 6^12 ≡1 mod13.\n\n3 mod13=3. 3^12≡1 mod13. Similarly, 2 mod13=2. 2^12≡1 mod13.\n\nBut maybe compute each term:\n\nA(n) = 5^n(5^n +1) -6^n(3^n +2^n). Let's compute each part modulo13.\n\nFirst term: 5^n*(5^n +1) =5^{2n} +5^n.\n\nSecond term: 6^n*(3^n +2^n)=6^n*3^n +6^n*2^n =18^n +12^n.\n\nSo A(n) =5^{2n} +5^n -18^n -12^n.\n\nCompute modulo13:\n\n18 mod13=5, so 18^n≡5^n mod13.\n\n12 mod13=12, which is -1, so 12^n≡(-1)^n mod13.\n\nSimilarly, 5^{2n} mod13: Since 5^2=25≡12 mod13. So 5^{2n}≡12^n mod13.\n\nSo substituting:\n\nA(n) ≡12^n +5^n -5^n -(-1)^n mod13.\n\nSimplify: 12^n - (-1)^n mod13. Since 12 ≡ -1 mod13, so 12^n ≡ (-1)^n mod13. Therefore, A(n) ≡ (-1)^n - (-1)^n ≡0 mod13.\n\nSo A(n) is divisible by 13 as well. Since 7 and 13 are coprime, A(n) is divisible by 7*13=91.\n\nTherefore, the proof is complete. So the key steps were breaking down the expression modulo 7 and modulo13, recognizing that the terms cancel out in each modulus, leading to divisibility by both primes, hence their product 91.",
    "Original_Problem":"Assumption: \n$n$ is an integer,\n$n$ is divisible by $7$.\nProblem: \nProve that $$A(n) = 5^n \\left( 5^n + 1 \\right) - 6^n \\left( 3^n + 2^n \\right)$$  \nis divisible by 91.",
    "Proof_problem":"Yes",
    "Redundant_assumption":"",
    "Problem_with_redundant_assumption":"Assumption:\nAssumption 1: $n$ is an integer,\nAssumption 2: $n$ is divisible by $7$.\nAssumption 3: $$\nS T(x_1 + \\eta x_2) = S \\big( T(x_1) + \\eta T(x_2)\\big)\n$$\nProblem: \nProve that $$A(n) = 5^n \\left( 5^n + 1 \\right) - 6^n \\left( 3^n + 2^n \\right)$$  \nis divisible by 91.",
    "judge":"```json\n{\n \"answer_to_Q1\": \"yes\",\n \"assumptions\": [\n  \"n is an integer\",\n  \"n is divisible by 7\"\n ],\n \"redundant_assumption\": \"$$ S T(x_1 + \\\\eta x_2) = S \\\\big( T(x_1) + \\\\eta T(x_2)\\\\big) $$\"\n}\n```",
    "proof strategy planner":"```json\n{\n  \"proof_sketch\": \"Step 1) Recall the definition of linear transformation: T(ax + by) = aT(x) + bT(y) for any scalars a,b and vectors x,y\\nStep 2) Apply transformation T to the linear combination: T(x_1 + η x_2)\\nStep 3) Use linearity property of T: T(x_1 + η x_2) = T(x_1) + η T(x_2)\\nStep 4) Apply transformation S to both sides: S(T(x_1 + η x_2)) = S(T(x_1) + η T(x_2))\\nStep 5) Use linearity property of S: S(T(x_1) + η T(x_2)) = S(T(x_1)) + η S(T(x_2))\\nStep 6) Conclude that S∘T(x_1 + η x_2) = S∘T(x_1) + η S∘T(x_2), proving the composition S∘T is linear\"\n}\n```",
    "mathematician and proof writer":"## Complete Proof\n\n**Theorem:** Let S and T be linear transformations. Then for any vectors x₁, x₂ and scalar η, we have:\n$$S(T(x_1 + \\eta x_2)) = S(T(x_1)) + \\eta S(T(x_2))$$\n\n**Proof:**\n\n1. **Given:** S and T are linear transformations. By definition of linearity:\n   - T(ax + by) = aT(x) + bT(y) for all vectors x, y and scalars a, b\n   - S(ax + by) = aS(x) + bS(y) for all vectors x, y and scalars a, b\n\n2. **Apply T to the linear combination:**\n   $$T(x_1 + \\eta x_2) = T(x_1) + \\eta T(x_2)$$\n   This follows directly from the linearity of T.\n\n3. **Apply S to both sides:**\n   $$S(T(x_1 + \\eta x_2)) = S(T(x_1) + \\eta T(x_2))$$\n\n4. **Apply linearity of S:**\n   Since S is linear, we have:\n   $$S(T(x_1) + \\eta T(x_2)) = S(T(x_1)) + \\eta S(T(x_2))$$\n\n5. **Combine the results:**\n   From steps 3 and 4, we obtain:\n   $$S(T(x_1 + \\eta x_2)) = S(T(x_1)) + \\eta S(T(x_2))$$\n\n6. **Conclusion:**\n   This shows that the composition S∘T preserves linear combinations, demonstrating that S∘T is itself a linear transformation.\n\n**Note:** The assumptions that \"n is an integer\" and \"n is divisible by 7\" appear to be unrelated to this linear algebra proof about the composition of linear transformations. These assumptions seem to pertain to a different number theory problem and are not used in this proof.\n\nThe proof is complete. □\n\nThe key insight is that the composition of linear transformations remains linear, which follows directly from applying the linearity properties of both transformations sequentially.",
    "final reviewer":"Proof: True\nOriginal Problem: If T: V -> W and S: W -> U are linear transformations, prove that their composition S∘T: V -> U is also a linear transformation.\n\nProof:\nLet T: V -> W and S: W -> U be linear transformations. We want to show that their composition S∘T: V -> U is also a linear transformation.\nTo prove that S∘T is a linear transformation, we must show that for any vectors x, y in V and any scalars a, b, the following property holds:\nS∘T(ax + by) = aS∘T(x) + bS∘T(y)\n\n1.  Consider the expression S∘T(ax + by).\n2.  By the definition of function composition, S∘T(ax + by) = S(T(ax + by)).\n3.  Since T is a linear transformation, it satisfies the property T(ax + by) = aT(x) + bT(y).\n4.  Substitute this into the expression from step 2: S(aT(x) + bT(y)).\n5.  Since S is a linear transformation, and T(x) and T(y) are vectors in W (the domain of S), S satisfies the property S(a * (vector1) + b * (vector2)) = aS(vector1) + bS(vector2). Applying this, we get: aS(T(x)) + bS(T(y)).\n6.  By the definition of function composition, S(T(x)) = S∘T(x) and S(T(y)) = S∘T(y).\n7.  Therefore, substituting these back, we conclude that S∘T(ax + by) = aS∘T(x) + bS∘T(y).\n\nSince S∘T satisfies the definition of a linear transformation, the composition of two linear transformations is indeed a linear transformation.\n\nfinal: If T: V -> W and S: W -> U are linear transformations, then their composition S∘T: V -> U is also a linear transformation."
}