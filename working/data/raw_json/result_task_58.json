{
    "Link_API":"https:\/\/api.stackexchange.com\/2.3\/questions\/124888",
    "Title":"Are the eigenvalues of $AB$ equal to the eigenvalues of $BA$?",
    "Score":99,
    "Category":"Chứng minh",
    "Tags":"linear-algebra, matrices, eigenvalues-eigenvectors",
    "Link":"https:\/\/math.stackexchange.com\/questions\/124888\/are-the-eigenvalues-of-ab-equal-to-the-eigenvalues-of-ba",
    "Content":"First of all, am I being crazy in thinking that if $\\lambda$ is an eigenvalue of $AB$, where $A$ and $B$ are both $N \\times N$ matrices (not necessarily invertible), then $\\lambda$ is also an eigenvalue of $BA$? If it's not true, then under what conditions is it true or not true? If it is true, can anyone point me to a citation?  I couldn't find it in a quick perusal of Horn & Johnson.  I have seen a couple proofs that the characteristic polynomial of $AB$ is equal to the characteristic polynomial of $BA$, but none with any citations. A trivial proof would be OK, but a citation is better.",
    "AcceptedAnswer":"If $v$ is an eigenvector of $AB$ for some nonzero $\\lambda$, then $Bv\\ne0$\nand $$\\lambda Bv=B(ABv)=(BA)Bv,$$ so $Bv$ is an eigenvector for $BA$ with the same eigenvalue. If $0$ is an eigenvalue of $AB$ then $0=\\det(AB)=\\det(A)\\det(B)=\\det(BA)$ so $0$ is also an eigenvalue of $BA$. More generally, Jacobson's lemma in operator theory states that for any two bounded operators $A$ and $B$ acting on a Hilbert space $H$ (or more generally, for any two elements of a Banach algebra), the non-zero points of the spectrum of $AB$ coincide with those of the spectrum of $BA$.",
    "llm_answer_create_structured_problem":"###BEGIN_OF_FORMAT###\nAssumption: \n$A$ is an $N \\times N$ matrix,\n$B$ is an $N \\times N$ matrix.\nProblem: \nProve that the eigenvalues of $AB$ are equal to the eigenvalues of $BA$.\n###",
    "reasoning_create_structured_problem":"Okay, let's try to tackle this problem. The user is asking whether the eigenvalues of AB are the same as those of BA for two N×N matrices A and B. They also want to know under what conditions this holds or doesn't hold, and if possible, a citation.\n\nFirst, I need to recall linear algebra concepts. I remember that for square matrices, there's a property related to eigenvalues of AB and BA. If either A or B is invertible, then AB and BA are similar matrices, which would mean they have the same eigenvalues. Because if A is invertible, then BA = A^{-1}(AB)A, so they're similar. Same if B is invertible.\n\nBut the question says A and B are not necessarily invertible. So what happens in that case? I think even if they aren't invertible, the non-zero eigenvalues should still be the same. Wait, how does that work?\n\nThere's a theorem that states that the characteristic polynomials of AB and BA are the same except for possibly a factor of λ^k where k is the difference in ranks or something. So the non-zero eigenvalues (counting multiplicities) should be the same for AB and BA. Therefore, the eigenvalues of AB and BA are the same, except that BA might have additional zero eigenvalues if the matrices are of different ranks. But wait, here both A and B are N×N, so AB and BA are also N×N, so maybe the multiplicities of zero eigenvalues would differ, but the set of eigenvalues (including zero) would still be the same? Or not?\n\nWait, let me think again. Suppose A is a singular matrix and B is also singular. For example, take A as a matrix with a zero column and B with a zero row. Then AB might have rank less than N, and BA as well. But maybe their eigenvalues still coincide. Let me take a simple example. Let A be [[0,1],[0,0]] and B be [[1,0],[0,0]]. Then AB is [[0,0],[0,0]], so its only eigenvalue is 0. BA would be [[0,1],[0,0]], which also has eigenvalues 0. So in that case, both have eigenvalue 0. Another example: Let A = [[0,1],[0,0]], B = [[0,0],[0,1]]. Then AB is [[0,1],[0,0]], which has eigenvalues 0. BA is [[0,0],[0,0]], eigenvalues 0. Hmm, same eigenvalues. Maybe even when they are singular, the eigenvalues (including zeros) are the same?\n\nWait, another example: Let A be a 2x2 matrix with first row [1,0] and second row [0,0], and B be [0,0;0,1]. Then AB is [0,0;0,0], eigenvalues 0. BA is [0,0;0,0], same eigenvalues. Hmm, maybe in all cases, the eigenvalues (including zero) are the same. Wait, but what if A and B are not square? Wait, in the problem they are both N×N. So maybe even for square matrices, regardless of invertibility, AB and BA have the same eigenvalues.\n\nBut I need to check. Wait, there's a theorem called Sylvester's determinant theorem, which says that det(I + AB) = det(I + BA) even for rectangular matrices. But eigenvalues are related to the characteristic polynomial. If we consider the characteristic polynomial of AB and BA, for square matrices, the eigenvalues are roots of det(λI - AB) and det(λI - BA). If A and B are square matrices, then det(λI - AB) = det(λI - BA). So their characteristic polynomials are the same, hence the eigenvalues (including algebraic multiplicities) are the same.\n\nWait, but this is only when they are square matrices. Let me confirm that. Suppose A and B are N×N. Then AB and BA are both N×N. So det(λI - AB) = det(λI - BA). If that's the case, then all eigenvalues (including multiplicities) are the same. Therefore, the answer would be yes, the eigenvalues are equal. But the user is asking for a citation. I think this is a standard result in linear algebra. Maybe in Horn and Johnson's Matrix Analysis, but the user says they checked and didn't find it.\n\nAlternatively, maybe when A and B are square matrices, AB and BA have the same eigenvalues. If A or B is invertible, then AB and BA are similar. If neither is invertible, then maybe the characteristic polynomials are still the same. Let's test with a 2x2 example where both A and B are singular. Let A be [[1,1],[1,1]] and B be [[1,0],[1,0]]. Then AB is [[2,0],[2,0]], which has eigenvalues 2 and 0. BA is [[1,1],[1,1]] times [[1,0],[1,0]] is [[2,0],[2,0]], same as AB. Wait, but that's a coincidence. Let me try another example. Let A = [[1,0],[0,0]] and B = [[0,1],[0,0]]. Then AB is [[0,1],[0,0]], which has eigenvalues 0. BA is [[0,0],[0,0]], which has eigenvalues 0. Hmm, same eigenvalues. Another example: A = [[0,1],[0,0]], B = [[1,1],[1,1]]. Then AB is [[1,1],[0,0]], eigenvalues 0 and 1. BA is [[0,1],[0,1]] times [[1,1],[1,1]]... Wait, let me compute BA: B is 2x2, A is 2x2. BA would be [[0*1 +1*1, 0*1 +1*1],[0*1 +1*1, 0*1 +1*1]] which is [[1,1],[1,1]], which has eigenvalues 2 and 0. AB was [[1,1],[0,0]], which has eigenvalues 0 and 1. Wait, that's different. Wait, so here AB has eigenvalues 0 and 1, BA has eigenvalues 0 and 2. So their non-zero eigenvalues differ. That contradicts the earlier idea. So in this case, AB and BA have different eigenvalues. Wait, so what's going on here?\n\nWait, in this example, A is [[0,1],[0,0]], B is [[1,1],[1,1]]. Then AB = A * B = [[0*1 +1*1, 0*1 +1*1],[0*1 +0*1, 0*1 +0*1]] = [[1,1],[0,0]]. The eigenvalues of AB are 0 and 1. Then BA is B * A = [[1*0 +1*0, 1*1 +1*0],[1*0 +1*0, 1*1 +1*0]] = [[0,1],[0,1]]. The eigenvalues of BA are 0 and 1. Wait, no, BA is [[0,1],[0,1]], which is a rank 1 matrix. Its trace is 0 +1=1, determinant is 0, so eigenvalues are 0 and 1. Wait, so AB and BA both have eigenvalues 0 and 1. Hmm, maybe my previous calculation was wrong.\n\nWait, let me check again. BA = [[1*0 +1*0, 1*1 +1*0], [1*0 +1*0, 1*1 +1*0]] = [[0,1], [0,1]]. Correct. So the trace is 1, determinant 0, so eigenvalues 0 and 1. So AB and BA have same eigenvalues. Then maybe my previous thought was wrong.\n\nWait, maybe another example. Let me try with A = [[0,1],[0,0]] and B = [[0,0],[1,0]]. Then AB = [[1,0],[0,0]], which has eigenvalues 1 and 0. BA = [[0,0],[0,1]], which has eigenvalues 0 and 1. So same eigenvalues. Hmm. Let me try another one where maybe they differ. Suppose A is [[1,0],[0,0]], B is [[0,1],[0,0]]. Then AB is [[0,1],[0,0]], eigenvalues 0. BA is [[0,0],[0,0]], eigenvalues 0. Same.\n\nWait, maybe I need a different example where AB and BA have different eigenvalues. Let me see. Take A as [[0,1,0],[0,0,1],[0,0,0]], B as some matrix. Hmm, maybe this is getting too complex. Alternatively, maybe it's always true that AB and BA have the same eigenvalues when A and B are square matrices. Because the characteristic polynomials are the same. Let me check that.\n\nFor square matrices A and B, the characteristic polynomial of AB and BA. Suppose A and B are n×n. Then, det(λI - AB) = det(λI - BA). Is this true? Let's test with λ=0. Then det(-AB) = (-1)^n det(AB) and det(-BA) = (-1)^n det(BA). But det(AB) = det(A)det(B) = det(BA). So for λ=0, it's the same. For general λ, is det(λI - AB) = det(λI - BA)? Wait, there's a theorem that says for square matrices, AB and BA have the same eigenvalues. But how?\n\nWait, if we consider the characteristic polynomial, det(λI - AB) = det(λI - BA). Let me see for 2x2 matrices. Let A = [[a,b],[c,d]], B = [[e,f],[g,h]]. Then AB and BA can be computed, and their characteristic polynomials calculated. Alternatively, maybe use the fact that trace(AB) = trace(BA), and det(AB) = det(BA). So if the trace and determinant are the same, then for 2x2 matrices, the characteristic polynomials are the same. So eigenvalues would be the same.\n\nIn general, trace(AB) = trace(BA), and more generally, the trace of any product is invariant under cyclic permutations. So for AB and BA, trace is same. Similarly, det(AB) = det(A)det(B) = det(B)det(A) = det(BA). For n×n matrices, the characteristic polynomial is determined by the traces of powers, determinants, etc. But maybe for AB and BA, all the coefficients of the characteristic polynomial are the same. Therefore, their eigenvalues (with algebraic multiplicities) are the same.\n\nTherefore, the answer is that yes, the eigenvalues of AB and BA are the same when A and B are square matrices of the same size. The user also asks under what conditions it's true. So if A and B are square matrices, then eigenvalues are the same. If they are rectangular, then it's not necessarily the case, but the question specifies both are N×N.\n\nSo in the problem, since A and B are both N×N matrices, the eigenvalues of AB and BA are equal. The assumption is that A and B are square matrices of the same size. The problem is to show that the eigenvalues of AB and BA are equal, or to determine under what conditions this holds.\n\nBut the original question mentions that they are not necessarily invertible, so the answer holds regardless of invertibility. Therefore, the problem structured would have the assumption that A and B are N×N matrices, with no invertibility required. The problem is to prove that the eigenvalues of AB are equal to those of BA, or to find conditions where this is true or not. But since in the square case, they are always equal, the answer would be that it's always true for square matrices, and if they are rectangular, then the non-zero eigenvalues are the same.\n\nBut the user's question specifically states that A and B are both N×N, so the answer would be that it's always true in that case. The example given in the original problem's assumption in the user's example was adding that n is divisible by 7, but here, the assumptions are that A and B are N×N matrices. So the formatted problem would have assumptions: A is an N×N matrix, B is an N×N matrix. Problem: Prove that the eigenvalues of AB are equal to the eigenvalues of BA.",
    "Original_Problem":"Assumption: \n$A$ is an $N \\times N$ matrix,\n$B$ is an $N \\times N$ matrix.\nProblem: \nProve that the eigenvalues of $AB$ are equal to the eigenvalues of $BA$.",
    "Proof_problem":"Yes",
    "Redundant_assumption":"",
    "Problem_with_redundant_assumption":"Assumption:\nAssumption 1: $A$ is an $N \\times N$ matrix,\nAssumption 2: $B$ is an $N \\times N$ matrix.\nAssumption 3: $$\\lambda Bv=B(ABv)=(BA)Bv,$$\nProblem: \nProve that the eigenvalues of $AB$ are equal to the eigenvalues of $BA$.",
    "judge":"```json\n{\n \"answer_to_Q1\": \"yes\",\n \"assumptions\": [\n  \"A is an N x N matrix\",\n  \"B is an N x N matrix\",\n  \"Let \\\\lambda be an eigenvalue of AB and v be its corresponding eigenvector, such that ABv = \\\\lambda v.\"\n ],\n \"redundant_assumption\": \"\\\\lambda Bv=B(ABv)=(BA)Bv\"\n}\n```",
    "proof strategy planner":"Based on my analysis, here is the proof sketch:\n\n```json\n{\n  \"proof_sketch\": \"Step 1) Given that λ is an eigenvalue of AB with eigenvector v, so ABv = λv\\nStep 2) Multiply both sides of ABv = λv by matrix B\\nStep 3) Apply associativity: B(ABv) = (BA)(Bv)\\nStep 4) Apply linearity: B(λv) = λ(Bv)\\nStep 5) Combine results: (BA)(Bv) = λ(Bv)\\nStep 6) This shows Bv is an eigenvector of BA with eigenvalue λ (provided Bv ≠ 0)\\nStep 7) Verify Bv ≠ 0: If Bv = 0, then from ABv = λv we get 0 = λv, which would imply either λ=0 or v=0 (contradiction to v being eigenvector)\"\n}\n```",
    "mathematician and proof writer":"The complete proof demonstrates that if λ is an eigenvalue of AB with eigenvector v, then λ is also an eigenvalue of BA. The key steps are:\n\n1. Starting from ABv = λv\n2. Multiplying both sides by B to get B(ABv) = B(λv)\n3. Using associativity: (BA)(Bv) = λ(Bv)\n4. Showing that either Bv is a nonzero eigenvector of BA, or λ = 0 is an eigenvalue of both matrices\n\nThis establishes that AB and BA have the same nonzero eigenvalues, which is a fundamental result in linear algebra about the similarity of spectra of matrix products.",
    "final reviewer":"Proof: True\n\nProblem: Prove that if λ is an eigenvalue of the matrix product AB, then λ is also an eigenvalue of the matrix product BA.\n\nProof:\nLet λ be an eigenvalue of AB with a corresponding eigenvector v. By definition, v ≠ 0 and we have:\n1.  ABv = λv\n\nMultiply both sides of the equation by the matrix B from the left:\n2.  B(ABv) = B(λv)\n\nUsing the associativity of matrix multiplication on the left side:\n3.  (BA)(Bv) = B(λv)\n\nUsing the linearity property of matrix multiplication (scalar multiplication commutes with matrix multiplication) on the right side:\n4.  (BA)(Bv) = λ(Bv)\n\nNow, we need to consider two cases for the vector Bv:\n\nCase 1: Bv ≠ 0\nIf Bv is not the zero vector, then equation (4) directly shows that Bv is an eigenvector of BA corresponding to the eigenvalue λ. Therefore, λ is an eigenvalue of BA.\n\nCase 2: Bv = 0\nIf Bv = 0, substitute this into equation (1):\nABv = λv\nA(0) = λv\n0 = λv\n\nSince v is an eigenvector, by definition v ≠ 0. For the equation 0 = λv to hold with v ≠ 0, it must be that λ = 0.\nSo, if Bv = 0, then λ must be 0. This means that 0 is an eigenvalue of AB.\nIf 0 is an eigenvalue of AB, then AB is a singular matrix, which implies det(AB) = 0.\nWe know that for square matrices, det(AB) = det(A)det(B).\nAlso, det(BA) = det(B)det(A).\nTherefore, det(AB) = det(BA).\nSince det(AB) = 0, it follows that det(BA) = 0.\nIf det(BA) = 0, then BA is a singular matrix, which means 0 is an eigenvalue of BA.\nThus, even in the case where Bv = 0 (which implies λ=0), λ is still an eigenvalue of BA.\n\nCombining both cases, we conclude that if λ is an eigenvalue of AB, then λ is also an eigenvalue of BA.\n\nfinal:True"
}