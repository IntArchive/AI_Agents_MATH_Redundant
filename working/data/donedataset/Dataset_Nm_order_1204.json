{
    "Link_API": "https://api.stackexchange.com/2.3/questions/492070",
    "Title": "How to prove &quot;eigenvalues of polynomial of matrix $A$ = polynomial of eigenvalues of matrix $A$ &quot;",
    "Score": 22,
    "Category": "Chứng minh",
    "Tags": "linear-algebra, polynomials, eigenvalues-eigenvectors",
    "Link": "https://math.stackexchange.com/questions/492070/how-to-prove-eigenvalues-of-polynomial-of-matrix-a-polynomial-of-eigenvalue",
    "Content": "Title looks a little bit twisted. What I want to say is the following: $A\\in\\mathbb{R}^{n\\times n}$, polynomial of matrix $A$: $P(A)=\\displaystyle \\sum_{k=0}^{n} c_k A^k$. $\\lambda(A)$ is the set of eigenvalues of $A$. So, we want to prove $\\lambda(P(A))=P(\\lambda(A))$ So far, I can prove it when $A$ is diagonalisable, but when $A$ is not diagonalisable, it seems we can't just simply use Jordan normal form to prove it. (Or this statement won't hold when it's not diagonalisable?) Can anyone suggest me something and help me out? Thanks!",
    "AcceptedAnswer": "The result is true only if you allow complex eigenvalues. If $\\lambda$ is a eigenvalue of $A$ then $Av=\\lambda v$ for a nonzero column vector $v\\in\\mathbb R^n$. It is easy to show that in this case $p(A)v=p(\\lambda)v$ for any nonconstant polynomial $p$, which shows that $p\\bigl(\\lambda(A)\\bigr)\\subseteq\\lambda\\bigl(p(A)\\bigr)$. For the converse, let $\\mu\\in\\lambda\\bigl(p(A)\\bigr)$, where $p$ is a nonconstant polynomial. You can factor $p(X)-\\mu$ on the complex numbers , obtaining $$p(X)-\\mu=a\\prod_{i=1}^m(X-a_i)\\,,$$ where $a,a_i\\in\\mathbb C$. Thus, we have $p(A)-\\mu I=a\\prod_{i=1}^m(A-a_iI)$. Since the matrix $p(A)-\\mu I$ is not invertible, then some matrix $A-a_iI$ is not invertible. This shows that $\\mu=p(a_i)$, where $a_i$ is a complex eigenvalue of $A$, that is, for some nonzero column vector $w\\in\\mathbb C^n$ you have $Aw=a_iw$. For the necessity of the inclusion of complex eigenvalues, consider the matrix $A=\\binom{0\\ -1}{1\\ \\ \\ 0}$, which has not real eigenvalues (check it). However $A^2=-I$ do has $-1$ as eigenvalue. What happens in this case is that $i$ and $-i$ are the complex eigenvalues of $A$, so in particular $\\lambda(A^2)=\\{(\\pm i)^2\\}=\\{-1\\}$.",
    "llm_answer_create_structured_problem": "###BEGIN_OF_FORMAT###\nAssumption: \n$A$ is a square matrix of size $n \\times n$,\n$A$ has eigenvalues $\\lambda(A)$,\n$P(A)$ is a polynomial of $A$ defined as $P(A) = \\sum_{k=0}^{n} c_k A^k$.\nProblem: \nProve that $\\lambda(P(A)) = P(\\lambda(A))$, i.e., the eigenvalues of the polynomial $P(A)$ are equal to the polynomial evaluated at each eigenvalue of $A$.\n###END_OF_FORMAT###",
    "reasoning_create_structured_problem": "Okay, let me try to tackle this problem. The user wants to prove that the eigenvalues of a polynomial of matrix A are equal to the polynomial evaluated at each eigenvalue of A. They mentioned they can prove it when A is diagonalizable but are stuck on the non-diagonalizable case.\n\nFirst, I need to recall what happens when a matrix is diagonalizable. If A is diagonalizable, then there exists an invertible matrix P such that P⁻¹AP = D, where D is diagonal. Then, for any polynomial P(A), we have P(A) = P(PDP⁻¹) = P P(D) P⁻¹. Since D is diagonal, P(D) is just applying the polynomial to each diagonal entry, which are the eigenvalues. So the eigenvalues of P(A) are exactly P(λ_i) for each eigenvalue λ_i of A. That makes sense.\n\nBut when A isn't diagonalizable, we use Jordan canonical form. So any matrix can be written in Jordan form, right? So J = Q⁻¹AQ, where J is a block diagonal matrix with Jordan blocks. Then, P(A) = Q P(J) Q⁻¹. The eigenvalues of P(J) should be the same as P(A) because similarity transformations preserve eigenvalues. Now, each Jordan block corresponds to an eigenvalue λ with multiplicity. When we apply the polynomial P to a Jordan block, what happens?\n\nLet me think. A Jordan block for λ is λ on the diagonal, 1s on the superdiagonal. Let's take a single Jordan block J of size m with eigenvalue λ. Then J = λI + N, where N is nilpotent (N^m = 0). So P(J) would be sum_{k=0}^n c_k (λI + N)^k. Since N is nilpotent, expanding (λI + N)^k using the binomial theorem gives terms up to N^{k}, but since N^m = 0, terms beyond N^{m-1} vanish. However, when we compute P(J), the result is upper triangular with P(λ) on the diagonal, and some entries above the diagonal from the nilpotent part. But the eigenvalues of P(J) would just be P(λ), repeated m times, because the eigenvalues of an upper triangular matrix are the diagonal entries. Therefore, even in the Jordan block case, the eigenvalues of P(J) are P(λ), so applying the polynomial to each eigenvalue. \n\nTherefore, even if A is not diagonalizable, by considering its Jordan form, the eigenvalues of P(A) are still P(λ_i) for each eigenvalue λ_i of A. So the statement should hold even when A is not diagonalizable. \n\nWait, but the user is suggesting that maybe it doesn't hold when A is not diagonalizable. Maybe there's a mistake in my reasoning? Let me check again. Suppose A is a Jordan block. Let’s take a simple example where A is a 2x2 Jordan block with eigenvalue λ. Then A = [[λ, 1], [0, λ]]. Let’s take P(t) = t^2. Then P(A) = A^2 = [[λ^2, 2λ], [0, λ^2]]. The eigenvalues of P(A) are both λ^2, which is P(λ). So that works. Another example: let’s take P(t) = t + c. Then P(A) = [[λ + c, 1], [0, λ + c]], eigenvalues still λ + c. So even though the matrix isn't diagonalizable, the eigenvalues of P(A) are still P(λ). \n\nTherefore, the user might have thought that Jordan form complicates things, but actually, the eigenvalues still work out. The key point is that when you apply the polynomial to a Jordan block, the diagonal entries become P(λ), and the eigenvalues are exactly those P(λ)s, regardless of the nilpotent part. Therefore, the statement holds even for non-diagonalizable matrices. \n\nSo maybe the confusion comes from thinking that the nilpotent part affects the eigenvalues, but since the Jordan form is upper triangular, the polynomial preserves the upper triangular structure, and the diagonal entries (hence eigenvalues) are just the polynomial applied to the original eigenvalues. Thus, the proof using Jordan form should work. \n\nTherefore, the answer should confirm that the statement holds even when A is not diagonalizable, and the Jordan form can be used to prove it by considering each Jordan block and showing that the eigenvalues of P(J) are P(λ) for each eigenvalue λ of A. The user might have been unsure whether the Jordan form approach works here, but it does. So the statement is true in general, not just for diagonalizable matrices.",
    "Original_Problem": "Assumption: \n$A$ is a square matrix of size $n \\times n$,\n$A$ has eigenvalues $\\lambda(A)$,\n$P(A)$ is a polynomial of $A$ defined as $P(A) = \\sum_{k=0}^{n} c_k A^k$.\nProblem: \nProve that $\\lambda(P(A)) = P(\\lambda(A))$, i.e., the eigenvalues of the polynomial $P(A)$ are equal to the polynomial evaluated at each eigenvalue of $A$.",
    "Original_Problem_with_numerical_assumption": "Assumption:\nAssumption 1: $A$ is a square matrix of size $n \\times n$,\nAssumption 2: $A$ has eigenvalues $\\lambda(A)$,\nAssumption 3: $P(A)$ is a polynomial of $A$ defined as $P(A) = \\sum_{k=0}^{n} c_k A^k$.\nProblem: \nProve that $\\lambda(P(A)) = P(\\lambda(A))$, i.e., the eigenvalues of the polynomial $P(A)$ are equal to the polynomial evaluated at each eigenvalue of $A$.",
    "Proof_problem": "Yes",
    "Redundant_assumption": "$$p(X)-\\mu=a\\prod_{i=1}^m(X-a_i)\\,,$$",
    "Problem_with_redundant_assumption": "Assumption:\nAssumption 1: $A$ is a square matrix of size $n \\times n$,\nAssumption 2: $A$ has eigenvalues $\\lambda(A)$,\nAssumption 3: $P(A)$ is a polynomial of $A$ defined as $P(A) = \\sum_{k=0}^{n} c_k A^k$.\nAssumption 4: $$p(X)-\\mu=a\\prod_{i=1}^m(X-a_i)\\,,$$\nProblem: \nProve that $\\lambda(P(A)) = P(\\lambda(A))$, i.e., the eigenvalues of the polynomial $P(A)$ are equal to the polynomial evaluated at each eigenvalue of $A$."
}