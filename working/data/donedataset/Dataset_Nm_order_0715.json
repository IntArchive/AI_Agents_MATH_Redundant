{
    "Link_API": "https://api.stackexchange.com/2.3/questions/60497",
    "Title": "Unbiased Estimator for a Uniform Variable Support",
    "Score": 28,
    "Category": "Chứng minh",
    "Tags": "statistics",
    "Link": "https://math.stackexchange.com/questions/60497/unbiased-estimator-for-a-uniform-variable-support",
    "Content": "Let $ x_i $ be iid observations in a sample from a uniform distribution over $ \\left[ 0, \\theta \\right] $.  Now I need to estimate $ \\theta $ based on $N$ observations and I want the estimator to be unbiased. I thought about simple estimator $ \\hat{\\theta} = \\max \\left( x_i \\right) $. Based on simulation it is not biased, yet I couldn't show it analytically. Could anyone, please, show it is unbiased? BTW, I could easily find another, easy to prove, unbiased estimator, $ \\hat{\\theta} = 2 \\mathrm{mean} \\left( {x}_{i} \\right) $",
    "AcceptedAnswer": "Of course $\\hat\\theta=\\max\\{x_i\\}$ is biased , simulations or not, since $\\hat\\theta<\\theta$ with full probability hence one can be sure that, for every $\\theta$, $E_\\theta(\\hat\\theta)<\\theta$. One usually rather considers $\\hat\\theta_N=\\frac{N+1}N\\cdot\\max\\{x_i\\}$, then $E_\\theta(\\hat\\theta_N)=\\theta$ for every $\\theta$. To show that $\\hat\\theta_N$ is unbiased, one must compute the expectation of $M_N=\\max\\{x_i\\}$, and the simplest way to do that might be to note that for every $x$ in $(0,\\theta)$, $P_\\theta(M_N\\le x)=(x/\\theta)^N$, hence\n$$\nE_\\theta(M_N)=\\int\\limits_0^{\\theta}P_\\theta(M_N\\ge x)\\text{d}x=\\int\\limits_0^{\\theta}(1-(x/\\theta)^N)\\text{d}x=\\theta-\\theta/(N+1)=\\theta N/(N+1).\n$$ Edit (Below is a remark due to @cardinal, which completes nicely this post.) It may be worth noting that the maximum $M_N=\\max\\limits_{1\\le k\\le N}X_k$ of an i.i.d. Uniform$(0,θ)$ random sample is a sufficient statistic for $θ$ and that it is one of the few statistics for distributions outside the exponential family which is also complete . An immediate consequence is that $\\hat\\theta_N=(N+1)M_N/N$ is the uniformly minimum variance unbiased estimator (UMVUE) for $θ$, that is, that any other unbiased estimator for $θ$ is a worse estimator in the $L^2$ sense.",
    "llm_answer_create_structured_problem": "###BEGIN_OF_FORMAT###\nAssumption: \n$ x_1, x_2, \\ldots, x_N $ are independent and identically distributed (iid) observations,\nEach $ x_i $ follows a uniform distribution over the interval $[0, \\theta]$,\nThe estimator $ \\hat{\\theta} $ is defined as $ \\hat{\\theta} = \\max\\{x_1, x_2, \\ldots, x_N\\} $,\nThe estimator $ \\hat{\\theta} $ is required to be unbiased for $ \\theta $.\nProblem: \nShow that $ \\hat{\\theta} = \\max\\{x_1, x_2, \\ldots, x_N\\} $ is an unbiased estimator of $ \\theta $.\n###END_OF_FORMAT###",
    "reasoning_create_structured_problem": "Okay, let's tackle this problem step by step. The user wants to show that the estimator \\(\\hat{\\theta} = \\max(x_i)\\) is unbiased for estimating \\(\\theta\\) when the data is drawn from a uniform distribution on \\([0, \\theta]\\). \n\nFirst, I need to recall what it means for an estimator to be unbiased. An estimator is unbiased if its expected value equals the true parameter. So, in this case, we need to find \\(E[\\max(x_i)]\\) and check if it equals \\(\\theta\\).\n\nThe data points \\(x_i\\) are independent and identically distributed (iid) from Uniform\\([0, \\theta]\\). The maximum of these observations, \\(\\max(x_i)\\), is a well-known estimator for \\(\\theta\\). I remember that for the uniform distribution, the distribution of the maximum can be found using order statistics. \n\nLet me recall the formula for the expectation of the maximum of n uniform variables. If \\(X_{(N)}\\) is the maximum of N uniform \\([0, \\theta]\\) variables, then its probability density function (pdf) is \\(f_{X_{(N)}}(x) = \\frac{N}{\\theta} \\left(\\frac{x}{\\theta}\\right)^{N-1}\\) for \\(0 \\leq x \\leq \\theta\\). \n\nTo find the expectation, we integrate \\(x\\) multiplied by this pdf from 0 to \\(\\theta\\):\n\n\\(E[X_{(N)}] = \\int_0^\\theta x \\cdot \\frac{N}{\\theta} \\left(\\frac{x}{\\theta}\\right)^{N-1} dx\\)\n\nSimplifying the integrand:\n\n\\(= \\frac{N}{\\theta^N} \\int_0^\\theta x^N dx\\)\n\nCalculating the integral:\n\n\\(\\int_0^\\theta x^N dx = \\left[\\frac{x^{N+1}}{N+1}\\right]_0^\\theta = \\frac{\\theta^{N+1}}{N+1}\\)\n\nSo,\n\n\\(E[X_{(N)}] = \\frac{N}{\\theta^N} \\cdot \\frac{\\theta^{N+1}}{N+1} = \\frac{N}{N+1} \\theta\\)\n\nAh, so the expectation of the maximum is \\(\\frac{N}{N+1} \\theta\\), which is less than \\(\\theta\\). That means that \\(\\max(x_i)\\) is actually a biased estimator, underestimating \\(\\theta\\) on average. But the user claims that based on simulation, it's not biased. Maybe there's a mistake in their simulation or understanding?\n\nWait, the problem mentions that they thought the maximum is unbiased but couldn't show it, and found another estimator \\(2 \\times \\text{mean}(x_i)\\) which is unbiased. Let's check that estimator. For a uniform distribution on \\([0, \\theta]\\), the mean is \\(\\theta/2\\), so \\(2 \\times \\text{mean}(x_i)\\) has expectation \\(2 \\times \\theta/2 = \\theta\\), which is indeed unbiased.\n\nBut according to the calculation for the maximum, it's biased. So why does the user think the maximum is unbiased based on simulation? Maybe they didn't account for the sample size? Let me double-check the expectation formula.\n\nYes, the expectation of the maximum of N uniform [0, θ] variables is indeed \\(\\frac{N}{N+1}\\theta\\). Therefore, to make an unbiased estimator, we can multiply the maximum by \\(\\frac{N+1}{N}\\). So the unbiased estimator should be \\(\\hat{\\theta} = \\frac{N+1}{N} \\max(x_i)\\).\n\nBut the user's original estimator is just the maximum, which is biased. So either the user made a mistake in their simulation, or perhaps there was a misunderstanding in the problem setup. The problem statement says \"based on N observations,\" so maybe they have a different setup. Wait, let's check the original problem again.\n\nThe original problem states: Let \\(x_i\\) be iid observations in a sample from a uniform distribution over \\([0, \\theta]\\). Need to estimate \\(\\theta\\) based on N observations and want the estimator to be unbiased. The user thought \\(\\hat{\\theta} = \\max(x_i)\\) is unbiased based on simulation but can't show it analytically. But according to the expectation, it's biased. However, they mention another estimator \\(2 \\times \\text{mean}(x_i)\\) which is unbiased.\n\nSo, the answer should point out that \\(\\max(x_i)\\) is actually biased, and perhaps the user's simulation had some error. However, the problem as presented by the user says \"based on simulation it is not biased,\" which contradicts the analytical result. Therefore, maybe there's a different angle here.\n\nWait, is there any scenario where the maximum could be unbiased? If the support is \\([0, \\theta)\\), but the maximum approaches \\(\\theta\\) as sample size increases. But for finite N, the expectation is still \\(\\frac{N}{N+1}\\theta\\). So unless N is approaching infinity, it's biased.\n\nTherefore, the correct answer is that the maximum is biased, but multiplying by \\(\\frac{N+1}{N}\\) makes it unbiased. The user might have confused it with another estimator or their simulation had a mistake. However, the problem asks to \"show it is unbiased,\" but according to the calculation, it's not. So maybe the original problem had a different context, or perhaps there's a typo.\n\nAlternatively, maybe the user is considering a discrete uniform distribution? But the problem states a uniform distribution over an interval, which is continuous.\n\nAlternatively, perhaps the user is using a different definition of the estimator. Wait, let me check again. The problem says \"the estimator to be unbiased. I thought about simple estimator \\(\\hat{\\theta} = \\max(x_i)\\). Based on simulation it is not biased, yet I couldn't show it analytically.\"\n\nBut analytically, we just showed it's biased. So there's a contradiction here. Maybe the user's simulation is incorrect. For example, if they used a small N and didn't average over enough trials, the bias might not be noticeable. For instance, if N=1, then the expectation of the max (which is just the single sample) is \\(\\theta/2\\), so bias is \\(-\\theta/2\\). For N=2, expectation is \\(2\\theta/3\\), so bias is \\(-\\theta/3\\), which might be less noticeable in simulations if theta is varying or if they didn't run enough samples.\n\nAlternatively, maybe the user considered a different parameterization. Wait, the problem says \"uniform distribution over [0, theta]\". So standard uniform is [0, theta], with pdf 1/theta.\n\nIn conclusion, the correct analytical result is that \\(\\max(x_i)\\) is biased, and the unbiased estimator is \\(\\frac{N+1}{N} \\max(x_i)\\). The user's other estimator, \\(2 \\times \\text{mean}(x_i)\\), is indeed unbiased because the expectation of the mean is theta/2, so doubling it gives theta.\n\nTherefore, the answer should explain that the maximum estimator is biased, derive its expectation, show the bias, and then adjust it to make it unbiased. However, the user is asking to show that the maximum is unbiased, which contradicts the derivation. Therefore, the correct response is to point out the mistake and show that the maximum is actually biased, but can be adjusted. Alternatively, if there's a misunderstanding in the problem setup, but given the information, the analytical result is clear.\n\nBut the problem format requires to structure the original question into assumptions and problem. The original problem states:\n\n\"Let \\( x_i \\) be iid observations in a sample from a uniform distribution over \\( [0, \\theta] \\). Now I need to estimate \\( \\theta \\) based on N observations and I want the estimator to be unbiased. I thought about simple estimator \\( \\hat{\\theta} = \\max(x_i) \\). Based on simulation it is not biased, yet I couldn't show it analytically. Could anyone, please, show it is unbiased? BTW, I could easily find another, easy to prove, unbiased estimator, \\( \\hat{\\theta} = 2 \\text{mean}(x_i) \\).\"\n\nSo the user believes that the maximum is unbiased, but the analytical derivation shows it's biased. The task is to structure their question into assumptions and problem. The assumptions would be:\n\n- \\( x_i \\) are iid observations.\n\n- The distribution of each \\( x_i \\) is uniform over \\([0, \\theta]\\).\n\n- Estimator \\( \\hat{\\theta} = \\max(x_i) \\).\n\n- The estimator should be unbiased.\n\nBut the problem is to show that \\( \\hat{\\theta} = \\max(x_i) \\) is unbiased for \\( \\theta \\).\n\nBut according to the analysis, this is not true. However, the user's question is to structure their problem as per the given format, not to correct their misunderstanding. So the formatted problem would list the assumptions as given and the problem as stated, even if there's an error in the user's perception.\n\nTherefore, the formatted problem would have assumptions:\n\n- \\( x_i \\) are independent and identically distributed (iid) observations,\n\n- Each \\( x_i \\) follows a uniform distribution over \\([0, \\theta]\\),\n\n- The estimator \\( \\hat{\\theta} \\) is defined as \\( \\hat{\\theta} = \\max(x_i) \\),\n\n- The estimator \\( \\hat{\\theta} \\) is required to be unbiased.\n\nProblem: Show that \\( \\hat{\\theta} = \\max(x_i) \\) is an unbiased estimator of \\( \\theta \\).\n\nBut in reality, this is false, but the user's question is structured this way. However, the example given in the initial instruction had assumptions like \"$n$ is divisible by 7\" which was not in the original problem but inferred. Wait, the example original problem was to prove A(n) is divisible by 91 for every integer n. The formatted assumption included \"n is an integer, n is divisible by 7\". But the original problem didn't state that n is divisible by 7; that was an assumption added by the formatter. So perhaps in this case, the assistant should only include explicit assumptions from the problem, not infer new ones.\n\nWait, the instructions say: \"filtered the unrelated informations, and structure that problem as the following format.\" The example added an assumption not present in the original problem. Wait, original example:\n\nOriginal problem was to prove A(n) is divisible by 91 for every integer n. The formatted assumption included \"n is an integer, n is divisible by 7\". But the original problem didn't state that n is divisible by 7. That seems like a mistake. Wait, perhaps the example was incorrect, but according to the user's instruction, the example is theirs. So perhaps in the example, they restructured the problem by adding assumptions. But in reality, the original problem says \"for every integer n\", but the formatted problem added \"n is divisible by 7\", which changes the problem. That seems like an error. But perhaps the user wants to extract the assumptions from the problem as stated, not add new ones.\n\nWait, the initial instruction says: \"read the mathematics question on Math Stack Exchange, filtered the unrelated informations, and structure that problem as the following format.\" So the task is to read the original question, filter out unrelated info, and structure into assumptions and problem. The example given by the user seems to have added an assumption not present in the original problem. The original problem says \"for every integer n\", but the formatted assumption includes \"n is divisible by 7\", which is not in the original. This suggests that maybe the user wants to split the problem's conditions into explicit assumptions, even if they are not separated in the original question.\n\nWait, the original problem in the example was to prove A(n) is divisible by 91 for every integer n. The formatted problem's assumption includes \"n is an integer, n is divisible by 7\". But the original problem's statement is for every integer n, so n being an integer is part of the problem's universal quantifier. However, the formatted assumption adds \"n is divisible by 7\", which was not in the original problem. This suggests that perhaps the example is incorrect, or the user wants to decompose the problem's conditions into assumptions, even if they are not explicitly stated. This is confusing.\n\nBut given that the user provided that example, we have to follow their lead. So in the example, the original problem says \"for every integer n\", which becomes \"n is an integer\" in assumptions, and the problem is to prove divisibility by 91, but the formatted problem adds \"n is divisible by 7\" as an assumption, which is not in the original problem. This seems like a mistake. However, perhaps the user intended to factor the problem into separate assumptions and problem. Maybe the original problem had a hidden assumption, but in reality, the example seems incorrect.\n\nGiven this confusion, perhaps the correct approach is to list as assumptions all the given conditions in the problem, each on a separate line, applied to one object. For the current problem about the uniform estimator:\n\nOriginal problem states:\n\n- \\( x_i \\) are iid observations from uniform over [0, θ].\n\n- Estimate θ based on N observations.\n\n- Estimator to be unbiased.\n\n- Considered estimator \\( \\hat{\\theta} = \\max(x_i) \\).\n\n- User wants to show this estimator is unbiased.\n\nSo the assumptions would be:\n\n- \\( x_i \\) are independent and identically distributed observations,\n\n- Each \\( x_i \\) is uniformly distributed over the interval [0, θ],\n\n- The estimator \\( \\hat{\\theta} \\) is defined as the maximum of the observations \\( x_i \\),\n\n- The estimator \\( \\hat{\\theta} \\) is required to be unbiased.\n\nProblem: Show that \\( \\hat{\\theta} = \\max(x_i) \\) is an unbiased estimator of θ.\n\nHowever, in reality, this is not true, but the user's question is structured this way. So we have to format it as per their question, not correct it.\n\nAnother point: In the example, the original problem didn't have the assumption that n is divisible by 7, but the formatted problem included it. This suggests that the user may want to decompose the problem into necessary assumptions, even if they are not explicitly stated. But that could be risky. Alternatively, the example might be a mistake.\n\nGiven the ambiguity, but following the user's example, perhaps the assumptions should list all the given conditions as separate lines. So for the current problem:\n\nAssumption:\n\n- \\( x_i \\) are independent and identically distributed (iid),\n\n- Each \\( x_i \\) follows a uniform distribution over [0, θ],\n\n- The estimator \\( \\hat{\\theta} \\) is defined as \\( \\hat{\\theta} = \\max(x_i) \\),\n\n- The estimator must be unbiased.\n\nProblem: Show that \\( \\hat{\\theta} = \\max(x_i) \\) is an unbiased estimator of θ.\n\nBut the original problem also mentions \"based on N observations\", so perhaps:\n\nAssumption:\n\n- \\( x_1, x_2, ..., x_N \\) are independent and identically distributed observations,\n\n- Each \\( x_i \\) follows a uniform distribution over [0, θ],\n\n- The estimator \\( \\hat{\\theta} \\) is defined as \\( \\hat{\\theta} = \\max\\{x_1, x_2, ..., x_N\\} \\),\n\n- The estimator \\( \\hat{\\theta} \\) is required to be unbiased for θ.\n\nProblem: Prove that \\( \\hat{\\theta} = \\max\\{x_1, x_2, ..., x_N\\} \\) is an unbiased estimator of θ.\n\nThat's more precise.",
    "Original_Problem": "Assumption: \n$ x_1, x_2, \\ldots, x_N $ are independent and identically distributed (iid) observations,\nEach $ x_i $ follows a uniform distribution over the interval $[0, \\theta]$,\nThe estimator $ \\hat{\\theta} $ is defined as $ \\hat{\\theta} = \\max\\{x_1, x_2, \\ldots, x_N\\} $,\nThe estimator $ \\hat{\\theta} $ is required to be unbiased for $ \\theta $.\nProblem: \nShow that $ \\hat{\\theta} = \\max\\{x_1, x_2, \\ldots, x_N\\} $ is an unbiased estimator of $ \\theta $.",
    "Original_Problem_with_numerical_assumption": "Assumption:\nAssumption 1: $ x_1, x_2, \\ldots, x_N $ are independent and identically distributed (iid) observations,\nAssumption 2: Each $ x_i $ follows a uniform distribution over the interval $[0, \\theta]$,\nAssumption 3: The estimator $ \\hat{\\theta} $ is defined as $ \\hat{\\theta} = \\max\\{x_1, x_2, \\ldots, x_N\\} $,\nAssumption 4: The estimator $ \\hat{\\theta} $ is required to be unbiased for $ \\theta $.\nProblem: \nShow that $ \\hat{\\theta} = \\max\\{x_1, x_2, \\ldots, x_N\\} $ is an unbiased estimator of $ \\theta $.",
    "Proof_problem": "Yes",
    "Redundant_assumption": "$$\nE_\\theta(M_N)=\\int\\limits_0^{\\theta}P_\\theta(M_N\\ge x)\\text{d}x=\\int\\limits_0^{\\theta}(1-(x/\\theta)^N)\\text{d}x=\\theta-\\theta/(N+1)=\\theta N/(N+1).\n$$",
    "Problem_with_redundant_assumption": "Assumption:\nAssumption 1: $ x_1, x_2, \\ldots, x_N $ are independent and identically distributed (iid) observations,\nAssumption 2: Each $ x_i $ follows a uniform distribution over the interval $[0, \\theta]$,\nAssumption 3: The estimator $ \\hat{\\theta} $ is defined as $ \\hat{\\theta} = \\max\\{x_1, x_2, \\ldots, x_N\\} $,\nAssumption 4: The estimator $ \\hat{\\theta} $ is required to be unbiased for $ \\theta $.\nAssumption 5: $$\nE_\\theta(M_N)=\\int\\limits_0^{\\theta}P_\\theta(M_N\\ge x)\\text{d}x=\\int\\limits_0^{\\theta}(1-(x/\\theta)^N)\\text{d}x=\\theta-\\theta/(N+1)=\\theta N/(N+1).\n$$\nProblem: \nShow that $ \\hat{\\theta} = \\max\\{x_1, x_2, \\ldots, x_N\\} $ is an unbiased estimator of $ \\theta $."
}