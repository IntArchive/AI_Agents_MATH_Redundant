{
    "Link_API": "https://api.stackexchange.com/2.3/questions/362559",
    "Title": "Show that every ideal of the matrix ring $M_n(R)$ is of the form $M_n(I)$ where $I$ is an ideal of $R$",
    "Score": 24,
    "Category": "Chứng minh",
    "Tags": "abstract-algebra, matrices, ring-theory",
    "Link": "https://math.stackexchange.com/questions/362559/show-that-every-ideal-of-the-matrix-ring-m-nr-is-of-the-form-m-ni-where",
    "Content": "Suppose $R$ is a commutative ring. Show that every ideal of $M_n(R)$ is of the form $M_n(I)$  where $I$ is an ideal of $R$. I have spent 30 minutes on this question and I still got nowhere. Can anyone give some hints ?",
    "AcceptedAnswer": "Let $J$ be an ideal of $M_n(R)$. The standard procedure is to let $I=\\{A_{11}\\mid A\\in J\\}\\subseteq R$ and then show that $I$ is an ideal of $R$. You will be able to show that $J=M_n(I)$ by using permutation matrices and sums of matrices. What I mean is, if $E_{ij}$ denotes a standard matrix unit, you can consider $A\\in J$ and $E_{ij}A{E_{k,l}}\\in J$, and sums of such products. In the other direction, it's clear that if you multiply anything in $M_n(R)$ by an element of $M_n(I)$, it will land back in $M_n(I)$.",
    "llm_answer_create_structured_problem": "###BEGIN_OF_FORMAT###\nAssumption: \n$R$ is a commutative ring,\n$M_n(R)$ is the ring of $n \\times n$ matrices over $R$,\n$I$ is an ideal of $R$.\nProblem: \nShow that every ideal of $M_n(R)$ is of the form $M_n(I)$.  \n###END_OF_FORMAT###",
    "reasoning_create_structured_problem": "Okay, let me try to work through this problem step by step. The problem states that if R is a commutative ring, then every ideal of the matrix ring M_n(R) is of the form M_n(I) where I is an ideal of R. \n\nFirst, I need to recall some basic concepts. M_n(R) is the ring of n×n matrices with entries from R. An ideal of a ring is a subset that is closed under addition and absorbs multiplication by any element from the ring. Since R is commutative, but matrix rings are generally non-commutative, even if R is commutative, M_n(R) isn't, except when n=1. So, the non-commutative nature might play a role here, but the problem claims that all ideals are of a specific form, which relates back to ideals in R.\n\nThe goal is to show that any ideal J in M_n(R) must be all matrices with entries from some ideal I of R. That is, J = M_n(I). So, how do I connect an ideal of matrices to an ideal in the base ring?\n\nLet me think about the structure of matrix rings. In M_n(R), we have matrix units e_{ij}, which are matrices with 1 in the (i,j) entry and 0 elsewhere. These matrix units are useful because they can be multiplied to move entries around. For example, e_{ij} * e_{kl} = δ_{jk} e_{il}, where δ is the Kronecker delta. This might help in generating entries in different positions.\n\nSuppose J is an ideal of M_n(R). Let me define I as the set of all elements a ∈ R such that a appears as the (1,1) entry of some matrix in J. Then, I need to show that I is an ideal of R, and that J = M_n(I).\n\nWait, why specifically the (1,1) entry? Maybe because if we can get an element in one entry, using matrix units, we can move it to any other entry. For example, if a is in the (1,1) position of some matrix A in J, then multiplying A on the left by e_{i1} and on the right by e_{1j} would give a matrix with a in the (i,j) position and zeros elsewhere. So, by multiplying by these matrix units, we can generate matrices with a in any position. Therefore, if I is the set of (1,1) entries, then M_n(I) would be contained in J.\n\nBut first, I need to verify that I is indeed an ideal of R. Let me check. Take a, b ∈ I. Then there are matrices A, B in J such that a is the (1,1) entry of A and b is the (1,1) entry of B. Then A + B is in J, and its (1,1) entry is a + b, so a + b ∈ I. Similarly, for any r ∈ R, if we multiply a matrix A (with (1,1) entry a) by the scalar matrix rI_n (where I_n is the identity matrix), then rI_n * A is in J, and its (1,1) entry is r*a. But wait, R is commutative, so scalar matrices are in the center of M_n(R)? Wait, in general, scalar matrices commute with all matrices in M_n(R), yes. So, if J is an ideal, then multiplying any element of J by a scalar matrix (which is in M_n(R)) would stay within J. Therefore, r*a would be in I. Hence, I is closed under addition and multiplication by elements of R, so it's an ideal.\n\nThen, we need to show that J is exactly M_n(I). Since I is the set of (1,1) entries, and using the matrix units, we can generate all matrices with entries from I. Therefore, M_n(I) is contained in J. Conversely, any matrix in J must have all its entries in I. Because, take any matrix A in J, and for any entry a_{ij} of A, we can multiply on the left by e_{1i} and on the right by e_{j1} to get a matrix with a_{ij} in the (1,1) position, hence a_{ij} is in I. Therefore, all entries of A are in I, so A ∈ M_n(I). Hence, J is contained in M_n(I), so they are equal.\n\nWait, that seems straightforward, but let me check for possible mistakes. The key steps are:\n\n1. Define I as the set of (1,1) entries of matrices in J.\n2. Show I is an ideal of R.\n3. Show that J = M_n(I).\n\nFor step 1, defining I as the (1,1) entries is arbitrary. Could we have chosen another position? Probably, but (1,1) is conventional. The idea is that by using matrix units, any entry can be moved to any position. \n\nIn step 2, showing that I is an ideal requires that for any a, b in I, a - b is in I, and for any r in R, ra and ar are in I. Since R is commutative, ar = ra, so that's fine. Since J is an ideal, multiplying a matrix A (with a in (1,1)) by scalar matrices (rI) on the left or right would scale all entries by r, so the (1,1) entry becomes ra, hence ra ∈ I. Similarly, adding two matrices gives a + b in (1,1). So yes, I is an ideal.\n\nFor step 3, to show J = M_n(I). First, M_n(I) is contained in J: if you take any matrix in M_n(I), each entry is in I, so each entry can be generated by some matrix in J. Then, using matrix units, you can build up the entire matrix as a sum of such entries multiplied by appropriate matrix units. Since J is closed under addition and multiplication by elements of M_n(R), the entire matrix would be in J.\n\nConversely, any matrix in J must have entries in I. Because, take any entry a_{ij} of a matrix A in J. Multiply A on the left by e_{1i} and on the right by e_{j1}. The result is a matrix with a_{ij} in the (1,1) position and zeros elsewhere. Since J is an ideal, this product is in J, so a_{ij} is in I. Therefore, every entry of A is in I, so A ∈ M_n(I). Hence, J is contained in M_n(I).\n\nTherefore, J = M_n(I). \n\nBut wait, the problem statement says \"Suppose R is a commutative ring.\" Is the commutativity of R necessary here? Let me think. In the argument above, when we showed that I is an ideal, we used that scalar matrices (with entries from R) commute with other matrices. Wait, scalar matrices do commute with all matrices even if R is non-commutative, right? Because a scalar matrix is r times the identity matrix, and multiplying on the left by rI is the same as multiplying each entry by r on the left, and on the right by rI is the same as multiplying each entry by r on the right. If R is non-commutative, then multiplying a matrix entry by r on the left vs. right would differ. However, in our case, R is commutative, so left and right scalar multiplications are the same, hence scalar matrices are in the center of M_n(R). Therefore, if R is not commutative, the argument might fail because ra might not equal ar, so I might not be a two-sided ideal. But since the problem states that R is commutative, this ensures that I is a two-sided ideal. Hence, the commutativity of R is necessary for the proof.\n\nTherefore, the key assumptions are that R is a commutative ring, and the problem is to show that every ideal of M_n(R) is M_n(I) for some ideal I of R.",
    "Original_Problem": "Assumption: \n$R$ is a commutative ring,\n$M_n(R)$ is the ring of $n \\times n$ matrices over $R$,\n$I$ is an ideal of $R$.\nProblem: \nShow that every ideal of $M_n(R)$ is of the form $M_n(I)$.",
    "Original_Problem_with_numerical_assumption": "Assumption:\nAssumption 1: $R$ is a commutative ring,\nAssumption 2: $M_n(R)$ is the ring of $n \\times n$ matrices over $R$,\nAssumption 3: $I$ is an ideal of $R$.\nProblem: \nShow that every ideal of $M_n(R)$ is of the form $M_n(I)$.",
    "Proof_problem": "Yes",
    "Redundant_assumption": "$J=M_n(I)$",
    "Problem_with_redundant_assumption": "Assumption:\nAssumption 1: $R$ is a commutative ring,\nAssumption 2: $M_n(R)$ is the ring of $n \\times n$ matrices over $R$,\nAssumption 3: $I$ is an ideal of $R$.\nAssumption 4: $J=M_n(I)$\nProblem: \nShow that every ideal of $M_n(R)$ is of the form $M_n(I)$."
}