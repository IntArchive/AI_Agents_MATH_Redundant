{
    "Link_API": "https://api.stackexchange.com/2.3/questions/111610",
    "Title": "If the product of two non-zero square matrices is zero, then both factors must be singular.",
    "Score": 27,
    "Category": "Chứng minh",
    "Tags": "linear-algebra, matrices",
    "Link": "https://math.stackexchange.com/questions/111610/if-the-product-of-two-non-zero-square-matrices-is-zero-then-both-factors-must-b",
    "Content": "In the textbook Contemporary Linear Algebra by Anton and Busby, there was a small question in section 3.2 page 101 concerning this. It asks if $A$ and $B$ are two non-zero square matrices such that $AB=0$, then $A$ and $B$ must both be singular. Why is this so? I can prove that if $A$ is non-singular then $B=I_nB=A^{-1}AB=0$, implying $B$ must be the zero matrix which is a contradiction. Similarly if $B$ is non-singular, then $A$ must be the zero matrix. Hence, both must be singular. But this doesn't really answer why, it just shows a contradiction for any case and hence must be the negation of our supposition that at least one is non-singular. I would like to know the essence and inherent property as to why they must be both singular (and why can't it be the case that only one is singular?) and what is the motivation for such a conclusion?",
    "AcceptedAnswer": "As Thomas points out, your proof is fine, but if you want another way to look at it, consider the following: Suppose $AB = 0$.  What is the $j$-th column on either side of this equation?  On the left, it is a linear combination of the columns $\\{\\mathbf a_j\\}$ of $A$, with coefficients from the $j$-th column of $B$, and on the right is the 0 vector: $$b_{1j}\\mathbf a_1 + b_{2j} \\mathbf a_2 + \\cdots + b_{nj}\\mathbf a_n = \\mathbf 0$$ This is true for each $j$, and there must be at least one non-zero $b_{ij}$ coefficient, since $B\\neq 0$, so the columns of $A$ are linearly dependent. Similarly, we can ask what are the rows on each side of the equation?  The $i$-th row is a linear combination of the rows of $B$, with coefficients from the $i$-th row of $A$.  So you see that the rows of $B$ must be linearly dependent.",
    "llm_answer_create_structured_problem": "###BEGIN_OF_FORMAT###\nAssumption: \n$A$ is a non-zero square matrix,\n$B$ is a non-zero square matrix,\n$A$ and $B$ have the same dimensions,\n$AB = 0$ (the zero matrix).\nProblem: \nProve that both $A$ and $B$ must be singular.\n###END_OF_FORMAT###",
    "reasoning_create_structured_problem": "Okay, let's tackle this problem. So, the question is about two non-zero square matrices A and B such that their product AB is the zero matrix. The goal is to show that both A and B must be singular. The user already presented a proof using contradiction, showing that if either A or B is non-singular, then the other must be zero, which contradicts the assumption that both are non-zero. But they want to understand the underlying reason why both must be singular, not just the contradiction part.\n\nFirst, I need to recall some linear algebra concepts. A matrix is singular if it doesn't have an inverse, which happens when its determinant is zero, or equivalently, when its columns are linearly dependent. So, if a matrix is singular, it doesn't have full rank. \n\nThe user's proof uses the property that if A is invertible (non-singular), then multiplying both sides of AB = 0 by A inverse gives B = 0, which contradicts B being non-zero. Similarly, if B is invertible, then A must be zero. So, if neither A nor B is invertible, then both must be singular. But this is a proof by contradiction, and the user is asking for the \"why\" behind it, maybe a more intuitive explanation.\n\nLet me think about the inherent properties. If AB = 0, then the columns of B are in the null space of A. Since B is non-zero, there's at least one non-zero column in B, which means the null space of A contains non-zero vectors. Therefore, the nullity of A is at least 1. By the rank-nullity theorem, the rank of A is less than the number of columns (which is the size of the matrix, since they're square). Hence, A is singular. Similarly, if we consider the rows of A, since AB = 0, the rows of A must be orthogonal to the columns of B. But since B is non-zero, the rows of A can't span the entire space, so the rank of A is deficient, making it singular. Wait, maybe that's complicating things.\n\nAlternatively, thinking about determinants. If A were non-singular, then det(A) ≠ 0, so A inverse exists. Then, as the user showed, B would have to be zero, which it's not. So, det(A) must be zero. Similarly for B. But this is the same as the original proof. The user wants to know why intuitively both have to be singular.\n\nMaybe another angle: the product AB = 0 implies that the composition of the linear transformations represented by A and B results in the zero transformation. If A were invertible, its transformation is bijective, so applying A inverse would undo it, leading to B being zero. Since B isn't zero, A can't be invertible. Similarly, if B were invertible, then its transformation is surjective, so the only way A applied after B gives zero is if A is zero. Since A isn't zero, B can't be invertible. So, both transformations must \"lose\" some information, which is what singularity implies—they collapse the space in some way, allowing their composition to collapse everything to zero.\n\nAlternatively, think in terms of solutions. If A is non-singular, the equation Ax = 0 has only the trivial solution. But AB = 0 implies each column of B is a solution to Ax = 0. Since B is non-zero, there are non-trivial solutions, so A must be singular. Similarly, for B, if B is non-singular, then its columns span the space, so A times every vector is zero, which would mean A is zero. But A isn't zero, so B must be singular. So, the essence is about the existence of non-trivial solutions when multiplying, which necessitates the matrices being singular.\n\nSo, structuring the problem: The assumptions are that A and B are square matrices, both non-zero, and their product is zero. The problem is to prove both must be singular. The user's existing proof is correct, but they want a deeper understanding. Maybe the key points are the rank-nullity theorem and the relation between matrix multiplication and linear transformations. But in terms of the problem structure, the original question already states the problem correctly. The user is asking for an explanation beyond the contradiction method.",
    "Original_Problem": "Assumption: \n$A$ is a non-zero square matrix,\n$B$ is a non-zero square matrix,\n$A$ and $B$ have the same dimensions,\n$AB = 0$ (the zero matrix).\nProblem: \nProve that both $A$ and $B$ must be singular.",
    "Original_Problem_with_numerical_assumption": "Assumption:\nAssumption 1: $A$ is a non-zero square matrix,\nAssumption 2: $B$ is a non-zero square matrix,\nAssumption 3: $A$ and $B$ have the same dimensions,\nAssumption 4: $AB = 0$ (the zero matrix).\nProblem: \nProve that both $A$ and $B$ must be singular.",
    "Proof_problem": "Yes",
    "Redundant_assumption": "$$b_{1j}\\mathbf a_1 + b_{2j} \\mathbf a_2 + \\cdots + b_{nj}\\mathbf a_n = \\mathbf 0$$",
    "Problem_with_redundant_assumption": "Assumption:\nAssumption 1: $A$ is a non-zero square matrix,\nAssumption 2: $B$ is a non-zero square matrix,\nAssumption 3: $A$ and $B$ have the same dimensions,\nAssumption 4: $AB = 0$ (the zero matrix).\nAssumption 5: $$b_{1j}\\mathbf a_1 + b_{2j} \\mathbf a_2 + \\cdots + b_{nj}\\mathbf a_n = \\mathbf 0$$\nProblem: \nProve that both $A$ and $B$ must be singular."
}