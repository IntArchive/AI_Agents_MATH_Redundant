{
    "Link_API": "https://api.stackexchange.com/2.3/questions/1288502",
    "Title": "Sequence satisfies weak law of large numbers but doesn&#39;t satisfy strong law of large numbers",
    "Score": 12,
    "Category": "Chứng minh",
    "Tags": "probability-theory, random-variables, law-of-large-numbers",
    "Link": "https://math.stackexchange.com/questions/1288502/sequence-satisfies-weak-law-of-large-numbers-but-doesnt-satisfy-strong-law-of-l",
    "Content": "Let $\\{X_n\\}_{n=1}^{\\infty}$ be a sequence of independent random variables such that $$P(X_n=n+1)=P(X_n=-(n+1))=\\frac{1}{2(n+1)\\log(n+1)}$$\n$$P(X_n=0)=1-\\frac{1}{(n+1)\\log(n+1)}$$\nProve that $X_n$ satisfies weak law of large numbers but doesn't satisfy strong law of large numbers. I thought that I could show that $\\mathbb EX_n=0$ and then I tried to show that $\\frac{ \\mathrm{Var}X_n}{n^2} \\rightarrow 0$. But I don't think this is the right way, I think I need to use $S_n$ somehowe. How should it be done?",
    "AcceptedAnswer": "To show convergence in probability, \n$$ \\mathbb E[X_n] = \\frac{n+1}{2(n+1)\\log(n+1)} - \\frac{n+1}{2(n+1)\\log(n+1)} = 0$$ \nand $$\n\\mathrm{Var}(X_n)=\\mathbb E[X_n^2]\n= \\frac{2(n+1)^2}{2(n+1)\\log(n+1)} = \\frac{n+1}{\\log(n+1)}.\n$$\nHence $\\mathbb E[S_n]=0$, and\n$$\\begin{align*}\n\\frac1{\\varepsilon^2}\\mathbb E\\left[\\left(\\frac{S_n}n\\right)^2\\right] &= \\frac1{n^2\\varepsilon^2} \\mathbb E[S_n^2]\\\\\n&= \\frac1{n^2\\varepsilon^2} \\mathrm{Var}\\left(\\sum_{i=1}^n X_i\\right)\\\\\n&= \\frac1{n^2\\varepsilon^2} \\sum_{i=1}^n \\mathrm{Var}(X_i)\\\\\n&= \\frac1{n^2\\varepsilon^2} \\sum_{i=1}^n \\frac{i+1}{\\log(i+1)}\\\\\n&\\leqslant \\frac1{n^2\\varepsilon^2}\\left(\\frac{n(n+1)}{\\log(n+1)}\\right)\\\\\n&= \\frac{n+1}{n\\log(n+1)\\varepsilon^2}\\\\\n&= \\frac1{\\log(n+1)\\varepsilon^2} + \\frac1{n\\log(n+1)\\varepsilon^2}\\stackrel{n\\to\\infty}{\\longrightarrow}0. \n\\end{align*}$$\nBy Markov's inequality,\n$$ \\mathbb P\\left(\\frac{S_n}n \\geqslant\\varepsilon \\right)\\leqslant \\frac{\\mathbb E\\left[\\left(\\frac{S_n}n\\right)^2 \\right]}{\\varepsilon^2}\\stackrel{n\\to\\infty}{\\longrightarrow}0.$$ To show that the convergence is not almost sure, for each $n$ we have, as pointed out by @Frank\n$$ \\{X_n=n+1\\} \\subset \\left\\{|S_n|\\geqslant \\frac n2\\right\\} \\cup \\left\\{|S_{n-1}|\\geqslant \\frac n2\\right\\}.$$\nSince $$\\sum_{n=1}^\\infty \\mathbb P(X_n=n+1) = \\sum_{n=1}^\\infty\\frac1{2(n+1)\\log(n+1)}=+\\infty,$$\nby Borel-Cantelli we have $$\\limsup_{n\\to\\infty} \\mathbb P\\left(\\frac{|S_n|}n\\geqslant \\frac12\\right)=1,$$\nand hence $$\\mathbb P\\left(\\lim_{n\\to\\infty} \\frac{S_n}n = 0\\right)<1.$$",
    "llm_answer_create_structured_problem": "###BEGIN_OF_FORMAT###\nAssumption: \n$\\{X_n\\}_{n=1}^{\\infty}$ is a sequence of independent random variables,\nFor each $n \\geq 1$, $P(X_n = n+1) = \\frac{1}{2(n+1)\\log(n+1)}$,\nFor each $n \\geq 1$, $P(X_n = -(n+1)) = \\frac{1}{2(n+1)\\log(n+1)}$,\nFor each $n \\geq 1$, $P(X_n = 0) = 1 - \\frac{1}{(n+1)\\log(n+1)}$.\nProblem:\nProve that $\\{X_n\\}$ satisfies the weak law of large numbers but does not satisfy the strong law of large numbers.\n###",
    "reasoning_create_structured_problem": "Okay, let's tackle this problem step by step. First, I need to understand the assumptions and what needs to be proven. The problem is about a sequence of independent random variables {Xₙ} with specific probabilities. We have to show that this sequence satisfies the weak law of large numbers (WLLN) but not the strong law of large numbers (SLLN).\n\nStarting with the assumptions. Each Xₙ has three possible values: n+1, -(n+1), and 0. The probabilities are given as P(Xₙ = n+1) = P(Xₙ = -(n+1)) = 1/[2(n+1)log(n+1)] and P(Xₙ = 0) = 1 - 1/[(n+1)log(n+1)]. So each Xₙ is symmetric around zero, which probably means the expectation E[Xₙ] is zero. Let me check that first because the user mentioned they thought E[Xₙ] = 0, which makes sense since the non-zero values are symmetric and cancel out.\n\nCalculating E[Xₙ]: (n+1)*(1/[2(n+1)log(n+1)]) + (-(n+1))*(1/[2(n+1)log(n+1)]) + 0*(the remaining probability). The first two terms cancel each other, so yes, E[Xₙ] = 0. That's correct.\n\nNext, for the WLLN, one common approach is to check that the variance of the average Sₙ/n converges to zero as n approaches infinity. Sₙ is the sum X₁ + X₂ + ... + Xₙ. The variance of Sₙ/n would be Var(Sₙ)/n². Since the variables are independent, Var(Sₙ) is the sum of Var(Xₙ). So if we can show that (1/n²) * sum Var(Xₖ) from k=1 to n tends to zero, that would satisfy the WLLN.\n\nLet's compute Var(Xₖ). Var(Xₖ) = E[Xₖ²] - (E[Xₖ])². Since E[Xₖ] is zero, Var(Xₖ) = E[Xₖ²]. Each Xₖ² is (k+1)² with probability 1/[(k+1)log(k+1)] and 0 otherwise. So E[Xₖ²] = (k+1)² * [2/(2(k+1)log(k+1))] = (k+1)² * [1/((k+1)log(k+1))] = (k+1)/log(k+1).\n\nTherefore, Var(Xₖ) = (k+1)/log(k+1). Then Var(Sₙ)/n² = [sum_{k=1}^n (k+1)/log(k+1)] / n². Wait, the user mentioned they tried to show Var(Xₙ)/n² → 0, but maybe they confused Var(Sₙ)/n² with Var(Xₙ)/n²? Because actually, Var(Sₙ)/n² = (1/n²) sum Var(Xₖ). So we need to check if the average of Var(Xₖ) up to n, divided by n, tends to zero. Wait, no: sum Var(Xₖ) is sum_{k=1}^n (k+1)/log(k+1), then divide by n². So the question is whether this sum grows slower than n². If the sum is asymptotically smaller than n², then the ratio tends to zero.\n\nBut let's approximate the sum. The terms (k+1)/log(k+1) for k from 1 to n. For large k, this behaves like k/log k. The sum of k/log k from k=2 to n+1. How does this sum behave? Let's compare to an integral. The integral of x/log x dx from 2 to n. Integration by parts: let u = log x, dv = x dx. Then du = 1/x dx, v = x²/2. So integral x²/(2 log x) - integral x²/(4 (log x)^2) dx. Hmm, this seems complicated, but asymptotically, the integral of x/log x dx is ~ (n²)/(2 log n). So the sum of k/log k from k=1 to n is approximately n²/(2 log n). Therefore, sum Var(Xₖ) ≈ n²/(2 log n). Then Var(Sₙ)/n² ≈ (n² / log n)/n² = 1/log n → 0 as n → ∞. Therefore, by Chebyshev's inequality, for any ε > 0, P(|Sₙ/n| ≥ ε) ≤ Var(Sₙ)/(n² ε²) → 0, which gives the WLLN.\n\nSo that part works. Now, why doesn't it satisfy SLLN? The SLLN requires almost sure convergence, which typically needs more stringent conditions. For independent variables, the Kolmogorov's SLLN states that if Var(Xₖ) is finite and sum Var(Xₖ)/k² converges, then SLLN holds. Wait, but in our case, Var(Xₖ) = (k+1)/log(k+1). So sum Var(Xₖ)/k² ≈ sum 1/(k log k). The series sum 1/(k log k) diverges (integral test: integral dx/(x log x) = log log x, which diverges). Therefore, Kolmogorov's SLLN condition is not met. Hence, the SLLN does not hold. Alternatively, using the converse: if SLLN holds, then Xₖ/k → 0 almost surely. But here, Xₖ can be ±(k+1) with probabilities ~ 1/(k log k). By Borel-Cantelli, if the events {Xₖ ≠ 0} are independent (which they are, since Xₖ are independent), then sum P(Xₖ ≠0) = sum 1/( (k+1) log(k+1) ) which converges (since integral dx/(x log x) = log log x, which from 3 to infinity is divergent. Wait, wait, sum 1/(k log k) from k=2 to infinity diverges. Wait, no: the integral of 1/(x log x) dx is log log x + C. So from, say, k=3 onwards, the integral diverges. Therefore, sum P(Xₖ ≠0) diverges, so by Borel-Cantelli, P(Xₖ ≠0 i.o.) = 1. That means infinitely often, Xₖ is non-zero. But each time Xₖ is non-zero, it's ±(k+1). Therefore, the terms Xₖ/k would be ±(k+1)/k ≈ ±1. So infinitely often, Xₖ/k is around ±1. Therefore, the average Sₙ/n = (1/n) sum Xₖ. If infinitely often, Xₖ is of order k, then when you sum them, the sum Sₙ could have large terms. But to check almost sure convergence, we might need to use the converse of SLLN. Since the variance condition for Kolmogorov's SLLN is not met (sum Var(Xₖ)/k² diverges), SLLN does not hold. Alternatively, use the fact that if SLLN holds, then Xₖ/k → 0 a.s. But here, as we saw, Xₖ/k is ±(k+1)/k /k = ±(1 + 1/k)/k, which is ±1/k + o(1), but wait, no. Wait, Xₖ is ±(k+1) with probability ~1/(k log k). So Xₖ/k is ±(1 + 1/k). So if Xₖ/k is ±(1 + 1/k) infinitely often, then even though these terms are going to ±1, but the sum Sₙ/n = (1/n) sum_{k=1}^n Xₖ. But the problem is that if infinitely often, Xₖ is on the order of k, then those terms would dominate the sum. For example, suppose that at some point n, Xₙ = n+1. Then Sₙ/n would have a term (n+1)/n ≈ 1. But if this happens infinitely often, then Sₙ/n might not converge to zero. However, since these events are rare (probability ~1/(n log n)), even though they happen infinitely often, maybe their contribution is negligible. But the Borel-Cantelli lemma says that since the sum of probabilities diverges, the events Xₖ ≠0 happen infinitely often almost surely. But each such event contributes a term of ±(k+1) to Sₖ. So when you take the average Sₙ/n, there are terms like (k+1)/n for some k ≤ n. If these happen infinitely often, but each time k is large, then (k+1)/n could be significant. For example, suppose that for each n, there is a term X_n = n+1, then S_n/n would have a term (n+1)/n ≈1. But since the X_n are independent, the occurrence of X_n ≠0 doesn't necessarily align with n. Wait, but even so, if infinitely often, there is some k where X_k = ±(k+1), then in the sum S_n, when n ≥k, the term X_k contributes ±(k+1) to S_n. But as n increases, the average S_n/n would have contributions from all previous X_k. However, the key point is that if the sum of variances diverges, then the SLLN may fail. Alternatively, to use the Three Series Theorem. For the SLLN to hold, three conditions must be satisfied for some c >0: 1. sum P(|Xₖ| >c) converges. 2. sum E[Xₖ I(|Xₖ| ≤c)] converges. 3. sum Var(Xₖ I(|Xₖ| ≤c)) converges. Let's check with c=1. For the first series: P(|Xₖ| >1) = P(Xₖ = ±(k+1)) = 1/( (k+1) log(k+1) ). Sum over k: sum 1/(k log k) which diverges. Therefore, the first series diverges, hence the Three Series Theorem tells us that the SLLN does not hold. Therefore, the conclusion is that WLLN holds but SLLN does not.\n\nSo putting this all together, the user needs to structure the problem by listing assumptions and the problem. The assumptions are about the sequence of independent random variables with the given probabilities. Each Xₙ has those specific probabilities, they are independent, and so on. The problem is to prove WLLN holds but SLLN does not.",
    "Original_Problem": "Assumption: \n$\\{X_n\\}_{n=1}^{\\infty}$ is a sequence of independent random variables,\nFor each $n \\geq 1$, $P(X_n = n+1) = \\frac{1}{2(n+1)\\log(n+1)}$,\nFor each $n \\geq 1$, $P(X_n = -(n+1)) = \\frac{1}{2(n+1)\\log(n+1)}$,\nFor each $n \\geq 1$, $P(X_n = 0) = 1 - \\frac{1}{(n+1)\\log(n+1)}$.\nProblem:\nProve that $\\{X_n\\}$ satisfies the weak law of large numbers but does not satisfy the strong law of large numbers.",
    "Original_Problem_with_numerical_assumption": "Assumption:\nAssumption 1: $\\{X_n\\}_{n=1}^{\\infty}$ is a sequence of independent random variables,\nAssumption 2: For each $n \\geq 1$, $P(X_n = n+1) = \\frac{1}{2(n+1)\\log(n+1)}$,\nAssumption 3: For each $n \\geq 1$, $P(X_n = -(n+1)) = \\frac{1}{2(n+1)\\log(n+1)}$,\nAssumption 4: For each $n \\geq 1$, $P(X_n = 0) = 1 - \\frac{1}{(n+1)\\log(n+1)}$.\nProblem:\nProve that $\\{X_n\\}$ satisfies the weak law of large numbers but does not satisfy the strong law of large numbers.",
    "Proof_problem": "Yes",
    "Redundant_assumption": "$$\n\\mathrm{Var}(X_n)=\\mathbb E[X_n^2]\n= \\frac{2(n+1)^2}{2(n+1)\\log(n+1)} = \\frac{n+1}{\\log(n+1)}.\n$$",
    "Problem_with_redundant_assumption": "Assumption:\nAssumption 1: $\\{X_n\\}_{n=1}^{\\infty}$ is a sequence of independent random variables,\nAssumption 2: For each $n \\geq 1$, $P(X_n = n+1) = \\frac{1}{2(n+1)\\log(n+1)}$,\nAssumption 3: For each $n \\geq 1$, $P(X_n = -(n+1)) = \\frac{1}{2(n+1)\\log(n+1)}$,\nAssumption 4: For each $n \\geq 1$, $P(X_n = 0) = 1 - \\frac{1}{(n+1)\\log(n+1)}$.\nAssumption 5: $$\n\\mathrm{Var}(X_n)=\\mathbb E[X_n^2]\n= \\frac{2(n+1)^2}{2(n+1)\\log(n+1)} = \\frac{n+1}{\\log(n+1)}.\n$$\nProblem:\nProve that $\\{X_n\\}$ satisfies the weak law of large numbers but does not satisfy the strong law of large numbers."
}