{
    "Link_API": "https://api.stackexchange.com/2.3/questions/101481",
    "Title": "Calculating maximum-likelihood estimation of the exponential distribution and proving its consistency",
    "Score": 25,
    "Category": "Chứng minh",
    "Tags": "probability, probability-theory, probability-distributions",
    "Link": "https://math.stackexchange.com/questions/101481/calculating-maximum-likelihood-estimation-of-the-exponential-distribution-and-pr",
    "Content": "The probability density function of the exponential distribution is defined as $$\nf(x;\\lambda)=\\begin{cases}\n\\lambda e^{-\\lambda x} &\\text{if } x \\geq 0 \\\\\n0 & \\text{if } x<0\n\\end{cases}\n$$ Its likelihood function is $$\n\\mathcal{L}(\\lambda,x_1,\\dots,x_n)=\\prod_{i=1}^n f(x_i,\\lambda)=\\prod_{i=1}^n \\lambda e^{-\\lambda x}=\\lambda^ne^{-\\lambda\\sum_{i=1}^nx_i}\n$$ To calculate the maximum likelihood estimator I solved the equation $$\n\\frac{d\\ln\\left(\\mathcal{L}(\\lambda,x_1,\\dots,x_n)\\right)}{d\\lambda}\\overset{!}{=}0\n$$ for $\\lambda$. $$\n\\begin{align}\n\\frac{d\\ln\\left(\\mathcal{L}(\\lambda,x_1,\\dots,x_n)\\right)}{d\\lambda}\n&= \\frac{d\\ln\\left(\\lambda^ne^{-\\lambda\\sum_{i=1}^nx_i}\\right)}{d\\lambda} \\\\\n&= \\frac{d\\ln\\left(n\\ln(\\lambda)-\\lambda\\sum_{i=1}^n x_i\\right)}{d\\lambda} \\\\\n&= \\frac{n}{\\lambda}-\\sum_{i=1}^n x_i\n\\end{align}\n$$ Finally we get $$\\lambda = \\frac{n}{\\sum\\limits_{i=1}^n x_i}$$ I hope this is correct this far. Where I am more uncertain is the proof for consistency. I understand that to be consistent is in this case equivalent to to converge in probability to $\\lambda$ . So I have a hinch, that something like $$\n\\lim_{n\\to\\infty}\\mathbb{P}\\left(\\mathcal{L}(\\lambda,x_1,\\dots,x_n)-\\lambda\\right)=0\n$$ will lead me to a solution. Am I correct this far? If yes, how can I solve this? A hint would be great. Update: Using hints by users @Did and @cardinal I will try to show the consistency by proving that $\\frac{1}{\\Lambda_n}\\to\\frac{1}{\\lambda}$ for $n\\to\\infty$ where $$\n\\Lambda_n=\\frac{n}{\\sum\\limits_{k=1}^nX_k}\n$$ Since $E(X_1)=\\int\\limits_0^\\infty\\lambda xe^{-\\lambda x}dx=\\frac{1}{\\lambda}$ and the random variables $X_i$ for $i\\ge1$ are independent the strong law of large numbers implies that $$\nP\\left(\\limsup_{n\\to\\infty}\\left|\\frac{1}{\\Lambda_n}-\\frac{1}{\\lambda}\\right|=0\\right)=P\\left(\\limsup_{n\\to\\infty}\\left|\\frac1n\\sum_{k=1}^nX_k-\\frac{1}{\\lambda}\\right|=0\\right)=1\n$$ is true which implies convergence almost everywhere. This implies convergence in probability of $\\Lambda_n$ to $\\lambda$, which is equivalent to consistency. Is this proof correct?",
    "AcceptedAnswer": "The computation of the MLE of $\\lambda$ is correct. The consistency is the fact that, if $(X_n)_{n\\geqslant1}$ is an i.i.d. sequence of random variables with exponential distribution of parameter $\\lambda$, then $\\Lambda_n\\to\\lambda$ in probability, where $\\Lambda_n$ denotes the random variable\n$$\n\\Lambda_n=\\frac{n}{\\sum\\limits_{k=1}^nX_k}.\n$$\nThus, one is asked to prove that, for every positive $\\varepsilon$, $\\mathrm P(|\\Lambda_n-\\lambda|\\geqslant\\varepsilon)\\to0$ when $n\\to\\infty$. In the case at hand, it might be easier to prove the stronger statement that $\\frac1{\\Lambda_n}\\to\\frac1\\lambda$ almost surely when $n\\to\\infty$. Hint: Law of large numbers.",
    "llm_answer_create_structured_problem": "###BEGIN_OF_FORMAT###\nAssumption: \nEach $X_i$ is an independent random variable,\nEach $X_i$ follows an exponential distribution with parameter $\\lambda$,\nThe likelihood function is $\\mathcal{L}(\\lambda; x_1, \\dots, x_n) = \\lambda^n e^{-\\lambda \\sum_{i=1}^n x_i}$,\nThe maximum likelihood estimator $\\Lambda_n$ is defined as $\\Lambda_n = \\frac{n}{\\sum_{i=1}^n X_i}$,\nConsistency of $\\Lambda_n$ is equivalent to $\\Lambda_n$ converging in probability to $\\lambda$ as $n \\to \\infty$,\nThe expected value of $X_i$ is $E[X_i] = \\frac{1}{\\lambda}$,\nThe strong law of large numbers applies to the sequence $\\{X_i\\}$.\nProblem:\nProve that the maximum likelihood estimator $\\Lambda_n$ is consistent for $\\lambda$.\n###END_OF_FORMAT###",
    "reasoning_create_structured_problem": "Okay, let me try to parse through this problem. The user is working on maximum likelihood estimation for the exponential distribution and then proving its consistency. Let's break it down step by step.\n\nFirst, the original question has two main parts: calculating the MLE and proving its consistency. The user derived the MLE as λ = n / sum(x_i), which is correct. Then they moved on to consistency, which they equate to convergence in probability. They mention using the strong law of large numbers, showing that 1/Λ_n converges to 1/λ almost surely, hence Λ_n converges to λ in probability. The user is asking if their approach is correct and for hints if needed.\n\nNow, formatting this into the required structure. The assumptions need to be listed line by line, each addressing one property or condition applied to one object. Let's see:\n\nThe problem is about the exponential distribution with parameter λ. The likelihood function is given, and the MLE is derived. For the assumptions:\n\n1. The random variables X_1, ..., X_n are independent. This is mentioned when applying the strong law of large numbers.\n2. The random variables X_1, ..., X_n follow an exponential distribution with parameter λ. The PDF is given, so each X_i has this distribution.\n3. The expected value E(X_1) = 1/λ. This is calculated by the user.\n4. The estimator Λ_n = n / sum(X_k). This is the MLE derived.\n5. The convergence in probability is equivalent to consistency. The user states this.\n6. The strong law of large numbers is applied to the sample mean. They use SLLN for the sum of X_i.\n\nEach of these points is a separate assumption. The problem then is to prove that the maximum likelihood estimator Λ_n is consistent for λ.\n\nWait, the user's problem is split into two parts: calculating the MLE (which they did) and proving consistency (which they are asking about). But in the formatted example, the problem is to prove divisibility. Here, the problem should be to \"Prove that the maximum likelihood estimator Λ_n is consistent for λ.\"\n\nBut looking at the original question, the user already attempted the proof using SLLN and convergence almost surely leading to convergence in probability. They are asking if their proof is correct. However, in the problem statement, we need to present the problem that needs solving, not the verification of their proof. So perhaps the problem is \"Prove that the maximum likelihood estimator Λ_n is consistent for λ.\"\n\nTherefore, the assumptions would include the setup for the exponential distribution, the MLE formula, the properties of the variables (independence, distribution), and the use of convergence concepts.\n\nLet me structure that:\n\nAssumptions:\n- The random variables X_1, X_2, ..., X_n are independent.\n- Each X_i follows an exponential distribution with parameter λ: X_i ~ Exp(λ).\n- The likelihood function for λ is L(λ; x_1, ..., x_n) = λ^n e^{-λ Σx_i}.\n- The maximum likelihood estimator Λ_n is given by Λ_n = n / (Σ_{i=1}^n X_i).\n- Convergence in probability is equivalent to consistency for the estimator Λ_n.\n- The expected value E[X_i] = 1/λ.\n- The strong law of large numbers applies to the sequence {X_i}.\n\nBut each assumption should be a single line addressing one property. Let's list them:\n\nAssumption:\nEach X_i is an independent random variable,\nEach X_i follows an exponential distribution with parameter λ,\nThe likelihood function is L(λ; x_1, ..., x_n) = λ^n e^{-λ ∑_{i=1}^n x_i},\nThe maximum likelihood estimator Λ_n is defined as Λ_n = n / ∑_{i=1}^n X_i,\nConsistency of Λ_n is equivalent to Λ_n converging in probability to λ as n approaches infinity,\nThe expected value of X_i is E[X_i] = 1/λ,\nThe strong law of large numbers applies to the sequence {X_i}.\n\nProblem: Prove that the maximum likelihood estimator Λ_n is consistent for λ.\n\nThat seems to capture the necessary assumptions and the problem statement. The user's update shows they used SLLN to get almost sure convergence of the sample mean to 1/λ, hence Λ_n converges to λ almost surely, which implies convergence in probability. The problem is to prove consistency, which is the same as proving convergence in probability. However, in the original question, the user is verifying their proof. But according to the format example, we present the problem as it is to be solved, not the user's specific doubts. So the problem is to prove consistency, so the formatted problem should be that.",
    "Original_Problem": "Assumption: \nEach $X_i$ is an independent random variable,\nEach $X_i$ follows an exponential distribution with parameter $\\lambda$,\nThe likelihood function is $\\mathcal{L}(\\lambda; x_1, \\dots, x_n) = \\lambda^n e^{-\\lambda \\sum_{i=1}^n x_i}$,\nThe maximum likelihood estimator $\\Lambda_n$ is defined as $\\Lambda_n = \\frac{n}{\\sum_{i=1}^n X_i}$,\nConsistency of $\\Lambda_n$ is equivalent to $\\Lambda_n$ converging in probability to $\\lambda$ as $n \\to \\infty$,\nThe expected value of $X_i$ is $E[X_i] = \\frac{1}{\\lambda}$,\nThe strong law of large numbers applies to the sequence $\\{X_i\\}$.\nProblem:\nProve that the maximum likelihood estimator $\\Lambda_n$ is consistent for $\\lambda$.",
    "Original_Problem_with_numerical_assumption": "Assumption:\nAssumption 1: Each $X_i$ is an independent random variable,\nAssumption 2: Each $X_i$ follows an exponential distribution with parameter $\\lambda$,\nAssumption 3: The likelihood function is $\\mathcal{L}(\\lambda; x_1, \\dots, x_n) = \\lambda^n e^{-\\lambda \\sum_{i=1}^n x_i}$,\nAssumption 4: The maximum likelihood estimator $\\Lambda_n$ is defined as $\\Lambda_n = \\frac{n}{\\sum_{i=1}^n X_i}$,\nAssumption 5: Consistency of $\\Lambda_n$ is equivalent to $\\Lambda_n$ converging in probability to $\\lambda$ as $n \\to \\infty$,\nAssumption 6: The expected value of $X_i$ is $E[X_i] = \\frac{1}{\\lambda}$,\nAssumption 7: The strong law of large numbers applies to the sequence $\\{X_i\\}$.\nProblem:\nProve that the maximum likelihood estimator $\\Lambda_n$ is consistent for $\\lambda$.",
    "Proof_problem": "Yes",
    "Redundant_assumption": "$$\n\\Lambda_n=\\frac{n}{\\sum\\limits_{k=1}^nX_k}.\n$$",
    "Problem_with_redundant_assumption": "Assumption:\nAssumption 1: Each $X_i$ is an independent random variable,\nAssumption 2: Each $X_i$ follows an exponential distribution with parameter $\\lambda$,\nAssumption 3: The likelihood function is $\\mathcal{L}(\\lambda; x_1, \\dots, x_n) = \\lambda^n e^{-\\lambda \\sum_{i=1}^n x_i}$,\nAssumption 4: The maximum likelihood estimator $\\Lambda_n$ is defined as $\\Lambda_n = \\frac{n}{\\sum_{i=1}^n X_i}$,\nAssumption 5: Consistency of $\\Lambda_n$ is equivalent to $\\Lambda_n$ converging in probability to $\\lambda$ as $n \\to \\infty$,\nAssumption 6: The expected value of $X_i$ is $E[X_i] = \\frac{1}{\\lambda}$,\nAssumption 7: The strong law of large numbers applies to the sequence $\\{X_i\\}$.\nAssumption 8: $$\n\\Lambda_n=\\frac{n}{\\sum\\limits_{k=1}^nX_k}.\n$$\nProblem:\nProve that the maximum likelihood estimator $\\Lambda_n$ is consistent for $\\lambda$."
}