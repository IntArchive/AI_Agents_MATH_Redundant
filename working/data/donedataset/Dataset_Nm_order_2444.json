{
    "Link_API": "https://api.stackexchange.com/2.3/questions/1387071",
    "Title": "Let $\\lambda$ be an eigenvalue of $A$. Prove that $\\lambda^{-1}$ is an eigenvalue of $A^{-1}$.",
    "Score": 14,
    "Category": "Chứng minh",
    "Tags": "linear-algebra",
    "Link": "https://math.stackexchange.com/questions/1387071/let-lambda-be-an-eigenvalue-of-a-prove-that-lambda-1-is-an-eigenvalu",
    "Content": "Let $\\lambda$ be an eigenvalue of $A$. Prove that $\\lambda^{-1}$ is an eigenvalue of $A^{-1}$. My approach: Suppose $\\lambda$ is an eigenvalue of $A$. Then $Ax=\\lambda x$ for some $x\\neq 0$. Since $A$ is invertible, $Ax=\\lambda x \\implies A^{-1}Ax=A^{-1}\\lambda x$. So: $$Ix=A^{-1}\\lambda x \\iff Ix-A^{-1}\\lambda x=0\\iff x(I-A^{-1}\\lambda)=0 $$ Since $x \\neq 0$ and $A$ is invertible, $\\lambda^{-1}=A^{-1}$ (I'm unsure of this equality). I think this somewhat shows it, this might seem elementary but aren't $\\lambda, \\lambda^{-1}$ scalars? So basically, if this proof is true $\\lambda\\cdot\\lambda^{-1}=I$ also? Any other way to show this? Thanks",
    "AcceptedAnswer": "Before I continue, it is important to note that $\\lambda$ is a scalar and not a matrix. So given that $A$ is invertible, $Ax=\\lambda x$, $A$ is invertible, and $\\lambda\\neq 0$, we have $$Ax=\\lambda x\\implies A^{-1}Ax=A^{-1}\\lambda x\\implies x=\\lambda A^{-1}x\\implies \\frac1\\lambda x=A^{-1}x.$$",
    "llm_answer_create_structured_problem": "###BEGIN_OF_FORMAT###\nAssumption: \n$A$ is an invertible matrix,\n$\\lambda$ is an eigenvalue of $A$,\n$x$ is a non-zero eigenvector corresponding to $\\lambda$.\nProblem: \nProve that $\\lambda^{-1}$ is an eigenvalue of $A^{-1}$.\n###",
    "reasoning_create_structured_problem": "Okay, let's tackle this problem step by step. So the question is asking to prove that if λ is an eigenvalue of matrix A, then λ⁻¹ is an eigenvalue of A⁻¹. Hmm, eigenvalues and inverses. Let me recall what I know about eigenvalues and invertible matrices.\n\nFirst, the user mentioned that if λ is an eigenvalue of A, there exists a non-zero vector x such that Ax = λx. That makes sense because eigenvalues are defined by the equation Ax = λx with x ≠ 0. Now, since A is invertible, they tried to manipulate the equation. Let me follow their approach.\n\nStarting from Ax = λx, if A is invertible, we can multiply both sides by A⁻¹. So, A⁻¹Ax = A⁻¹(λx). Simplifying the left side gives Ix, which is just x. On the right side, since λ is a scalar, we can factor it out: λA⁻¹x. So now we have x = λA⁻¹x. The user then rearranged this to x - λA⁻¹x = 0, which factors as x(I - λA⁻¹) = 0. Wait, but x is a vector and I is the identity matrix. Hmm, actually, matrices act on vectors, so maybe the correct way is to write (I - λA⁻¹)x = 0. That makes more sense because the matrix (I - λA⁻¹) multiplied by x gives the zero vector.\n\nSince x is not the zero vector, this implies that the matrix (I - λA⁻¹) is singular. Therefore, its determinant must be zero. So det(I - λA⁻¹) = 0. But we need to relate this to A⁻¹. Let's see. If we want to show that μ is an eigenvalue of A⁻¹, then we need A⁻¹x = μx for some non-zero x. Comparing this to the equation we have from earlier: x = λA⁻¹x, which can be rewritten as A⁻¹x = (1/λ)x. So μ would be 1/λ, which is λ⁻¹. Therefore, this shows that λ⁻¹ is an eigenvalue of A⁻¹.\n\nWait, the user was confused about λ⁻¹ = A⁻¹. That part seems incorrect. Because λ is a scalar, and A⁻¹ is a matrix. So equating them doesn't make sense. The correct approach is to show that A⁻¹x = (1/λ)x, which would mean that 1/λ is the eigenvalue corresponding to the same eigenvector x for the matrix A⁻¹.\n\nAnother way to think about it: If A is invertible, then all its eigenvalues are non-zero because the determinant of A is the product of its eigenvalues, and since A is invertible, det(A) ≠ 0. Therefore, λ ≠ 0, so 1/λ exists. Starting from Ax = λx, multiply both sides by A⁻¹ and divide by λ (since λ ≠ 0), which gives A⁻¹x = (1/λ)x. Hence, 1/λ is an eigenvalue of A⁻¹ with the same eigenvector x.\n\nThe user also mentioned λ·λ⁻¹ = I, which is a bit confusing. Since λ is a scalar, λ·λ⁻¹ = 1, not the identity matrix. The identity matrix I is a matrix with ones on the diagonal, but here we're dealing with scalars. So that part was a mistake in their reasoning.\n\nSo to summarize, the key steps are:\n\n1. Start with Ax = λx, where x ≠ 0 and A is invertible.\n2. Multiply both sides by A⁻¹: A⁻¹Ax = A⁻¹λx → x = λA⁻¹x.\n3. Rearrange to get A⁻¹x = (1/λ)x.\n4. Therefore, (1/λ) is an eigenvalue of A⁻¹ corresponding to eigenvector x.\n\nThis shows that if λ is an eigenvalue of A, then λ⁻¹ is an eigenvalue of A⁻¹, provided A is invertible (which ensures λ ≠ 0).",
    "Original_Problem": "Assumption: \n$A$ is an invertible matrix,\n$\\lambda$ is an eigenvalue of $A$,\n$x$ is a non-zero eigenvector corresponding to $\\lambda$.\nProblem: \nProve that $\\lambda^{-1}$ is an eigenvalue of $A^{-1}$.",
    "Original_Problem_with_numerical_assumption": "Assumption:\nAssumption 1: $A$ is an invertible matrix,\nAssumption 2: $\\lambda$ is an eigenvalue of $A$,\nAssumption 3: $x$ is a non-zero eigenvector corresponding to $\\lambda$.\nProblem: \nProve that $\\lambda^{-1}$ is an eigenvalue of $A^{-1}$.",
    "Proof_problem": "Yes",
    "Redundant_assumption": "$$Ax=\\lambda x\\implies A^{-1}Ax=A^{-1}\\lambda x\\implies x=\\lambda A^{-1}x\\implies \\frac1\\lambda x=A^{-1}x.$$",
    "Problem_with_redundant_assumption": "Assumption:\nAssumption 1: $A$ is an invertible matrix,\nAssumption 2: $\\lambda$ is an eigenvalue of $A$,\nAssumption 3: $x$ is a non-zero eigenvector corresponding to $\\lambda$.\nAssumption 4: $$Ax=\\lambda x\\implies A^{-1}Ax=A^{-1}\\lambda x\\implies x=\\lambda A^{-1}x\\implies \\frac1\\lambda x=A^{-1}x.$$\nProblem: \nProve that $\\lambda^{-1}$ is an eigenvalue of $A^{-1}$."
}