{
    "Link_API": "https://api.stackexchange.com/2.3/questions/4950855",
    "Title": "Inequality involving entropies: $\\left \\|p -\\frac{1}{n} e \\right \\|_2\\ge\\left \\|\\frac{-p\\log p}{H(p)} -\\frac{1}{n} e \\right \\|_2$",
    "Score": 16,
    "Category": "Chứng minh",
    "Tags": "real-analysis, calculus, probability, inequality, entropy",
    "Link": "https://math.stackexchange.com/questions/4950855/inequality-involving-entropies-left-p-frac1n-e-right-2-ge-left",
    "Content": "For a given probability vector $p=(p_1,\\dots,p_n)$ with $p_1,\\dots,p_n > 0, \\sum_{i=1}^n p_i=1$ and with $e:= (1, \\dots, 1)$ , I want to prove the following inequality: $$\\small\\left \\|p -\\frac{1}{n} e \\right \\|^2_2=\\sum_{i=1}^n \\left (p_i -\\frac{1}{n} \\right )^2\\ge\\left \\|-\\frac{1}{H(p)}p \\log p -\\frac{1}{n} e \\right \\|^2_2=\\sum_{i=1}^n \\left (\\frac{-p_i \\log p_i}{-\\sum_{i=1}^n p_i\\log p_i} -\\frac{1}{n} \\right )^2, \\tag{1}$$ where the equality holds only if all the probabilities $p_i,i=1,\\dots,n$ are equal. For $n=2$ , the difference of the left and right sides in (1) is plotted below for $(p_1,p_2)=(p,1-p)$ ( source ): which shows the inequality holds for $n=2$ . I came across with this inequality in my answer to this question . I guess the above inequality holds for any $n\\ge2$ (and any $l_p$ norm) based on my numerical experiments for $n=2$ . For $n=3$ it also holds, which can be seen below where the difference of the left and right sides in (1) is shown for $(p_1,p_2,p_3)=(x,y,1-x-y)$ (you can see the 3D version of the plot here , which can be rotated in any direction): The above inequality indicates that the distance of the probability vector $p\\neq\\frac{1}{n} e$ from the center of the standard simplex, i.e. $\\frac{1}{n} e$ which has the maximum entropy among all distributions on a sample space with $n$ elements, is strictly larger than the distance of the entropy-based probability vector $\\frac{-p \\log p}{H(p)}$ generated based on the entropies of disjoint events occurring with probabilities $p_1,\\dots,p_n$ .",
    "AcceptedAnswer": "I will base the proof on pretty much the same ideas as Amir in his OP, but I'll not try to work in such a high generality and will use the specifics of the setup a bit. The general observation is that if we have a concave positive function $f$ with $f(0)=0$ such that $x+f(x)$ is increasing and $\\sum_i f(p_i)=\\sum_i p_i$ , then $\\sum_i f(p_i)^2\\le \\sum_i p_i^2$ . Indeed, then we can find $P$ such that $f(p)\\ge p, g(p)=f(p)+p\\le g(P)$ for $p\\le P$ and $f(p)< p, g(p)>g(P)$ for $p>P$ . The equality of sums implies that $$\n\\sum_{i:p_i\\le P}(f(p_i)-p_i)=\\sum_{i:p_i> P}(p_i-f(p_i))\\,,\n$$ so $$\n\\sum_{i:p_i\\le P}(f(p_i)^2-p_i^2)=\\sum_{i:p_i\\le P}(f(p_i)-p_i)g(p_i)\\le \\sum_{i:p_i\\le P}(f(p_i)-p_i)g(P)\n\\\\\n=\\sum_{i:p_i> P}(p_i-f(p_i))g(P)\\le \\sum_{i:p_i> P}(p_i-f(p_i))g(p_i)=\\sum_{i:p_i> P}(p_i^2-f(p_i)^2)\n$$ and the claim follows. Note that we do not need here that $f(p)+p$ increases on an interval, just that it increases on the set of the $p_i$ 's used (in that case we take $P$ to be the largest $p_i$ for which $f(p_i)\\ge p_i$ ). Thus, all we need to show is that if we choose $c>0$ so that $c\\sum_i p_i\\log\\frac 1{p_i}=1$ , then for $q$ and $r$ in our set of $p_i$ with $q<r$ , we have $$\ncq\\log\\frac 1q+q\\le cr\\log\\frac 1r+r\\,.\n$$ That inequality is linear in $c\\ge 0$ . It is trivially true for $c=0$ . So, it suffices to check it for the largest possible under the circumstances value of $c$ , which is $[q\\log\\frac 1q+r\\log\\frac 1r]^{-1}$ , i.e., we need to check that $$\nq\\log\\frac 1q+q[q\\log\\frac 1q+r\\log\\frac 1r]\\le r\\log\\frac 1r+r[q\\log\\frac 1q+r\\log\\frac 1r]\\,,\n$$ or $$\n\\frac{\\log\\frac 1q}{\\log\\frac 1r}\\le \\frac rq\\frac{1+r-q}{1+q-r}\\,.\n$$ Now $$\n\\frac{1+r-q}{1+q-r}\\ge \\frac{1-q}{1-r}\\,,\n$$ which, after getting rid of the denominators and cancelling the common terms, is equivalent to $r-q\\ge r^2-q^2=(r-q)(r+q)$ , which holds because $r>q$ and $r+q\\le 1$ . Now, after replacing the complicated fraction on the right with the simple one, the inequality becomes trivial: denoting $\\bar q=1-q>1-r=\\bar r$ , we can rewrite it as $$\n\\frac{\\sum_{k\\ge 1}\\frac 1k\\bar q^k}{\\sum_{k\\ge 1}\\frac 1k\\bar r^k}\\le\n\\frac{\\sum_{k\\ge 1}\\bar q^k}{\\sum_{k\\ge 1}\\bar r^k}\n$$ and we can employ the general principle that if $A_k,B_k>0$ , $A_k/B_k$ is increasing, and $w_k>0$ is a decreasing sequence of weights, then $$\n\\frac{\\sum_{k\\ge 1}w_kA_k}{\\sum_{k\\ge 1}w_kB_k}\\le\n\\frac{\\sum_{k\\ge 1}A_k}{\\sum_{k\\ge 1}B_k}\\,,\n$$ provided that all series converge, etc. The End.",
    "llm_answer_create_structured_problem": "###BEGIN_OF_FORMAT###\nAssumption: \n$p = (p_1, \\dots, p_n)$ is a probability vector,\n$p_i > 0$ for each $i = 1, \\dots, n$,\n$\\sum_{i=1}^n p_i = 1$,\n$e = (1, \\dots, 1) \\in \\mathbb{R}^n$,\n$H(p) = -\\sum_{i=1}^n p_i \\log p_i$.\nProblem:\nProve that  \n$$\\sum_{i=1}^n \\left( p_i - \\frac{1}{n} \\right)^2 \\ge \\sum_{i=1}^n \\left( \\frac{-p_i \\log p_i}{H(p)} - \\frac{1}{n} \\right)^2,$$  \nwhere equality holds only if all $p_i$ are equal.\n###",
    "reasoning_create_structured_problem": "Okay, let's tackle this problem step by step. First, I need to understand what's being asked here. The user wants to prove an inequality involving probability vectors and entropies. Let me parse through the problem statement carefully.\n\nThe problem states that for a given probability vector p = (p₁, ..., pₙ) where each p_i is positive and they sum to 1, we need to show that the squared L² norm of p minus the uniform distribution vector (1/n)e is greater than or equal to the squared L² norm of (-p log p)/H(p) minus the same uniform distribution. Here, H(p) is the entropy, which is the sum of -p_i log p_i over all i. The equality holds only when all p_i are equal, i.e., when p is the uniform distribution.\n\nFirst, I need to list the assumptions. The problem mentions a probability vector p with all positive entries summing to 1. The vector e is defined as a vector of ones. The entropy H(p) is given by the standard formula. The inequality involves the squared L² norms. Also, equality holds only when all p_i are equal. So each of these should be separate assumptions.\n\nLooking at the example provided, the assumptions are listed line by line, each addressing one property. Let me structure that:\n\nAssumptions:\n1. p = (p₁, ..., pₙ) is a probability vector.\n2. Each component p_i > 0 for i = 1, ..., n.\n3. The sum of p_i from i=1 to n is 1 (since it's a probability vector).\n4. e = (1, ..., 1) is a vector of ones with length n.\n5. The entropy H(p) = -Σ_{i=1}^n p_i log p_i.\n6. The inequality involves the squared L² norm: ||x - (1/n)e||²₂ = Σ_{i=1}^n (x_i - 1/n)².\n7. The equality in the given inequality holds only when all p_i are equal (i.e., p = (1/n)e).\n\nWait, but in the original problem statement, the user mentions \"the equality holds only if all the probabilities p_i are equal.\" That's an important condition. So that's another assumption about the equality condition.\n\nNow, the problem is to prove that the squared L² distance from p to the uniform distribution is greater than or equal to the squared L² distance from the entropy-scaled vector (-p log p)/H(p) to the uniform distribution. So the problem statement is as given.\n\nLet me check if I missed any assumptions. The problem involves H(p), which is defined as the entropy, so that's covered. The vectors involved are p and (-p log p)/H(p), which are well-defined because p_i > 0 and H(p) is positive (since p is not uniform, but even if it were, H(p) would be log n). Wait, but H(p) could be zero only if one p_i is 1 and others are 0, but the problem states p_i > 0, so H(p) is positive. Hence, division by H(p) is valid.\n\nAlso, the user mentions that they tested for n=2 and n=3, but the problem is general for n ≥ 2. The original problem statement doesn't specify a particular n, so the assumption is n ≥ 2, but since the user is asking to prove for any n, maybe that's part of the problem's scope, not an assumption. Wait, the assumptions are about the given conditions in the problem. The problem itself is to prove the inequality for any n ≥ 2, given the probability vector. So perhaps the only assumptions are the properties of p and the definitions of e and H(p).\n\nWait, the problem statement says \"for a given probability vector p\", so the assumptions are about p. The user might have intended to include that n ≥ 2, but in the original example, they didn't include n ≥ 2 as an assumption, just that n is an integer. Wait, in the example provided by the user, the original problem had an assumption that n is an integer and n is divisible by 7. But in this current problem, the user mentions n=2 and n=3, but the problem is general. However, the problem statement in the user's question says \"for any n ≥ 2\". Wait, looking back:\n\nThe user wrote: \"I guess the above inequality holds for any n≥2 (and any l_p norm) based on my numerical experiments for n=2.\" So in their problem statement, they are conjecturing it for any n≥2, but in the original problem as stated, they might not have specified n≥2. Wait, let me check the original question again.\n\nOriginal question says: \"For a given probability vector p=(p₁,…,pₙ) with p₁,…,pₙ >0, sum 1…\" So n is the dimension of the vector, which is given. The user is trying to prove the inequality for any such p, hence n is part of the problem's setup. So perhaps the assumption is that n is an integer greater than or equal to 2? But the user hasn't specified that n must be ≥2, but since they tested for n=2 and 3, maybe it's implied. However, in the problem as stated by the user, n is just the dimension of the probability vector, so technically, n could be 1, but in that case, p would be (1), and the inequality would be trivial. But since in the example, the user has n=2 and n=3, maybe the problem is intended for n≥2, but the original problem statement doesn't specify. However, the problem statement in the user's question includes \"for any n≥2\", so perhaps that's an assumption. But in the problem as presented, the user starts with \"For a given probability vector p=(p₁,…,pₙ)\", so n is part of the vector's dimension. So maybe the assumption is that n is an integer greater than or equal to 2. However, the original example given by the user (the divisible by 91 problem) included assumptions on n. Therefore, perhaps in this case, the assumptions should include that n is an integer with n≥2.\n\nBut in the original problem statement as written by the user, the only explicit assumptions are on p: being a probability vector with positive entries summing to 1. The rest are definitions (e is the vector of ones, H(p) is the entropy). The equality condition is part of the problem's conclusion.\n\nWait, the problem says \"where the equality holds only if all the probabilities p_i,i=1,…,n are equal.\" So that's a condition on when equality occurs. So maybe the assumptions are:\n\n1. p is a probability vector (sum p_i =1, each p_i >0).\n2. e is the vector of ones (1,…,1).\n3. H(p) is the entropy, defined as -Σ p_i log p_i.\n\nAnd the problem is to prove the inequality, with the note that equality holds only when all p_i are equal. But in the example provided, the assumption included the condition on n (divisible by 7). Here, perhaps n≥2 is not an assumption but part of the problem's context. However, in the problem statement, the user is trying to prove it for any n≥2. Wait, but in the problem as stated, the user writes \"For a given probability vector p...\", which implies that n is part of the vector's dimension. So maybe the assumption is that n is an integer ≥2. But the original problem didn't specify; the user's conjecture is that it holds for any n≥2. So perhaps the assumption is that n is an integer greater than or equal to 2. However, the problem statement as written by the user does not explicitly state that n≥2; it's part of their conjecture. So maybe it's better to list the explicit assumptions from the problem statement, not the conjecture.\n\nTherefore, the assumptions should be:\n\n1. p = (p₁, ..., pₙ) is a probability vector.\n2. Each p_i > 0 for i = 1, ..., n.\n3. The sum of p_i from i=1 to n is 1.\n4. e is the vector (1, ..., 1) in ℝⁿ.\n5. H(p) is defined as -Σ_{i=1}^n p_i log p_i.\n\nThe problem is to prove that ||p - (1/n)e||₂² ≥ || (-p log p)/H(p) - (1/n)e ||₂², with equality only when all p_i are equal.\n\nWait, but the problem statement in the user's question mentions \"where the equality holds only if all the probabilities p_i are equal\". So that's an additional condition on when equality occurs. So perhaps that is part of the problem statement, not an assumption. The assumptions are the given conditions, and the problem includes proving both the inequality and the equality condition.\n\nSo in the formatted problem, the problem part should include both proving the inequality and stating the equality condition. However, in the example provided, the problem part was \"Prove that [expression] is divisible by 91.\" The equality condition here is part of what needs to be proven. So perhaps in this case, the problem should be written as:\n\n\"Prove that the inequality [...] holds, where equality holds only if all p_i are equal.\"\n\nBut the user's original problem statement in the question already includes that. Let me check:\n\nOriginal problem statement in the user's question:\n\n\"I want to prove the following inequality: [inequality], where the equality holds only if all the probabilities p_i,i=1,…,n are equal.\"\n\nSo the problem is to prove the inequality and the equality condition. Therefore, in the formatted problem, the problem part should include both.\n\nHence, the formatted problem would have:\n\nAssumption:\n- p = (p₁, ..., pₙ) is a probability vector.\n- Each p_i > 0.\n- The sum of p_i from i=1 to n is 1.\n- e = (1, ..., 1) in ℝⁿ.\n- H(p) = -Σ_{i=1}^n p_i log p_i.\n\nProblem:\nProve that \n||p - (1/n)e||₂² ≥ || (-p log p)/H(p) - (1/n)e ||₂²,\nwhere equality holds only if all p_i are equal.\n\nBut wait, the original problem uses the squared L² norm explicitly written as sums. So maybe we need to write the problem using the sum expressions. Let me check the original LaTeX:\n\nThe user wrote:\n\n$$\\small\\left \\|p -\\frac{1}{n} e \\right \\|^2_2=\\sum_{i=1}^n \\left (p_i -\\frac{1}{n} \\right )^2\\ge\\left \\|-\\frac{1}{H(p)}p \\log p -\\frac{1}{n} e \\right \\|^2_2=\\sum_{i=1}^n \\left (\\frac{-p_i \\log p_i}{-\\sum_{i=1}^n p_i\\log p_i} -\\frac{1}{n} \\right )^2, \\tag{1}$$\n\nSo the problem is to prove that the left-hand side (sum of (p_i -1/n)²) is greater than or equal to the right-hand side (sum of ( (-p_i log p_i)/H(p) -1/n )²), with equality only when all p_i are equal.\n\nTherefore, the problem statement in the formatted version should include both the inequality in terms of the sums and the equality condition.\n\nNow, in terms of assumptions, each line should address one property. So:\n\nAssumptions:\n1. $p = (p_1, \\dots, p_n)$ is a probability vector.\n2. $p_i > 0$ for each $i = 1, \\dots, n$.\n3. $\\sum_{i=1}^n p_i = 1$.\n4. $e = (1, \\dots, 1) \\in \\mathbb{R}^n$.\n5. $H(p) = -\\sum_{i=1}^n p_i \\log p_i$.\n6. The norm used is the $L^2$ norm.\n\nWait, but the norm is part of the problem statement. The user defines the norm as the L² norm. So maybe assumption 6 is redundant because the problem uses the L² norm explicitly. But perhaps it's better to include it since the user mentions \"any l_p norm\" in their conjecture, but the problem as stated is specifically for L². However, in the problem statement, the user wrote \"for any n≥2 (and any l_p norm)\", but the actual inequality given is for the L² norm. So the problem as presented is for L², and the user conjectures it might hold for other norms. But the formatted problem should stick to what's given.\n\nSo perhaps the assumptions are:\n\n1. $p$ is a probability vector with $p_i > 0$ for all $i$.\n2. $e$ is the vector of ones in $\\mathbb{R}^n$.\n3. $H(p)$ is the entropy defined as $-\\sum_{i=1}^n p_i \\log p_i$.\n4. The $L^2$ norm is used in the inequality.\n\nBut breaking them into separate lines:\n\nAssumption:\n- $p = (p_1, \\dots, p_n)$ is a probability vector.\n- $p_i > 0$ for each $i = 1, \\dots, n$.\n- $\\sum_{i=1}^n p_i = 1$.\n- $e = (1, \\dots, 1) \\in \\mathbb{R}^n$.\n- $H(p) = -\\sum_{i=1}^n p_i \\log p_i$.\n- The norm $\\| \\cdot \\|_2$ is the Euclidean ($L^2$) norm.\n\nBut maybe some of these can be combined. Wait, the example given by the user had each assumption as a separate line, each addressing one property. For instance, in the example, they had two assumptions: n is an integer, and n is divisible by 7. Similarly, here, each bullet should be one condition. So:\n\nAssumption:\n- $p = (p_1, \\dots, p_n)$ is a probability vector.\n- Each component $p_i > 0$ for $i = 1, \\dots, n$.\n- The sum $\\sum_{i=1}^n p_i = 1$.\n- $e$ is the vector $(1, \\dots, 1)$ in $\\mathbb{R}^n$.\n- $H(p)$ is defined as $-\\sum_{i=1}^n p_i \\log p_i$.\n- The inequality uses the $L^2$ norm squared.\n\nBut the problem statement already includes the definitions of the norms as sums, so maybe the $L^2$ norm is part of the problem's given, not an assumption. However, to be thorough, it's better to list that the norm used is $L^2$.\n\nBut perhaps the user's example didn't list the norm as an assumption, even though the problem involved divisibility by 91. So in this case, the key assumptions are on p and the definitions of e and H(p). The use of the L² norm is part of the problem statement.\n\nTherefore, the formatted assumptions would be:\n\nAssumption:\n- $p = (p_1, \\dots, p_n)$ is a probability vector.\n- Each $p_i > 0$ for $i = 1, \\dots, n$.\n- $\\sum_{i=1}^n p_i = 1$.\n- $e = (1, \\dots, 1) \\in \\mathbb{R}^n$.\n- $H(p) = -\\sum_{i=1}^n p_i \\log p_i$.\n\nProblem:\nProve that\n$$\n\\sum_{i=1}^n \\left( p_i - \\frac{1}{n} \\right)^2 \\ge \\sum_{i=1}^n \\left( \\frac{-p_i \\log p_i}{H(p)} - \\frac{1}{n} \\right)^2,\n$$\nwhere equality holds only if all $p_i$ are equal.\n\nYes, that seems accurate. Each assumption is listed, addressing one property. The problem includes both the inequality and the equality condition.",
    "Original_Problem": "Assumption: \n$p = (p_1, \\dots, p_n)$ is a probability vector,\n$p_i > 0$ for each $i = 1, \\dots, n$,\n$\\sum_{i=1}^n p_i = 1$,\n$e = (1, \\dots, 1) \\in \\mathbb{R}^n$,\n$H(p) = -\\sum_{i=1}^n p_i \\log p_i$.\nProblem:\nProve that  \n$$\\sum_{i=1}^n \\left( p_i - \\frac{1}{n} \\right)^2 \\ge \\sum_{i=1}^n \\left( \\frac{-p_i \\log p_i}{H(p)} - \\frac{1}{n} \\right)^2,$$  \nwhere equality holds only if all $p_i$ are equal.",
    "Original_Problem_with_numerical_assumption": "Assumption:\nAssumption 1: $p = (p_1, \\dots, p_n)$ is a probability vector,\nAssumption 2: $p_i > 0$ for each $i = 1, \\dots, n$,\nAssumption 3: $\\sum_{i=1}^n p_i = 1$,\nAssumption 4: $e = (1, \\dots, 1) \\in \\mathbb{R}^n$,\nAssumption 5: $H(p) = -\\sum_{i=1}^n p_i \\log p_i$.\nProblem:\nProve that  \n$$\\sum_{i=1}^n \\left( p_i - \\frac{1}{n} \\right)^2 \\ge \\sum_{i=1}^n \\left( \\frac{-p_i \\log p_i}{H(p)} - \\frac{1}{n} \\right)^2,$$  \nwhere equality holds only if all $p_i$ are equal.",
    "Proof_problem": "Yes",
    "Redundant_assumption": "$$\n\\frac{1+r-q}{1+q-r}\\ge \\frac{1-q}{1-r}\\,,\n$$",
    "Problem_with_redundant_assumption": "Assumption:\nAssumption 1: $p = (p_1, \\dots, p_n)$ is a probability vector,\nAssumption 2: $p_i > 0$ for each $i = 1, \\dots, n$,\nAssumption 3: $\\sum_{i=1}^n p_i = 1$,\nAssumption 4: $e = (1, \\dots, 1) \\in \\mathbb{R}^n$,\nAssumption 5: $H(p) = -\\sum_{i=1}^n p_i \\log p_i$.\nAssumption 6: $$\n\\frac{1+r-q}{1+q-r}\\ge \\frac{1-q}{1-r}\\,,\n$$\nProblem:\nProve that  \n$$\\sum_{i=1}^n \\left( p_i - \\frac{1}{n} \\right)^2 \\ge \\sum_{i=1}^n \\left( \\frac{-p_i \\log p_i}{H(p)} - \\frac{1}{n} \\right)^2,$$  \nwhere equality holds only if all $p_i$ are equal."
}