{
    "Link_API": "https://api.stackexchange.com/2.3/questions/1238475",
    "Title": "Show two random variables have same distribution",
    "Score": 23,
    "Category": "Chứng minh",
    "Tags": "probability, probability-theory",
    "Link": "https://math.stackexchange.com/questions/1238475/show-two-random-variables-have-same-distribution",
    "Content": "Let X, Y be two non-negative random variables satisfying the condition $\\mathbb{E}[X^\\alpha] = \\mathbb{E}[Y^\\alpha]$ for all $\\alpha \\in (0, 1/2)$. How can one show that X and Y are equal in distribution? Edit: (only if you find this helpful) $\\mathbb{E}[X], \\mathbb{E}[Y]$ also exist, but a priori one does not know whether they are equal or not. If you believe that the claim is wrong, I would also be happy to see counterexamples, or at least some intuitive explanations.",
    "AcceptedAnswer": "It can be shown that nonnegative random variables $X$ and $Y$ have the same distribution so long as $\\mathbb{E}[X^\\alpha]=\\mathbb{E}[Y^\\alpha]$ is finite for all $\\alpha\\in(a,b]$, any $0\\le a <b$. Setting $U=\\log X$ and $V=\\log Y$, define the functions\n$$\nf(\\alpha)=\\mathbb{E}[1_{\\{X > 0\\}}e^{\\alpha U}],\\ g(\\alpha)=\\mathbb{E}[1_{\\{Y > 0\\}}e^{\\alpha V}], \n$$ These are defined for complex $\\alpha$ with $0 < \\Re[\\alpha]< b$, as the terms inside the expectations are bounded by $\\max(1,X^b)$ and $\\max(1,Y^b)$ in absolute value. Furthermore, it can be seen that they are complex differentiable in this range. By assumption, they are equal for real $\\alpha$ in $(a,b)$. Hence, by analytic continuation, they are equal on the domain $0 < \\Re[\\alpha] < b$. Then, for any real $\\omega$, dominated convergence gives,\n\\begin{align}\n\\mathbb{E}[1_{\\{X > 0\\}}e^{i\\omega U}]&=\\lim_{t\\downarrow 0}f(t+i\\omega)=\\lim_{t\\downarrow 0}g(t+i\\omega)\\\\\n&=\\mathbb{E}[1_{\\{Y > 0\\}}e^{i\\omega V}].\n\\end{align}\nTaking $\\omega=0$ shows that $X$ and $Y$ have the same probability of being zero. Then, conditioning on $X$ and $Y$, respectively, being strictly positive we see that $U$ and $V$ have the same characteristic functions. Hence, they have the distribution and, therefore, so do $X$ and $Y$.",
    "llm_answer_create_structured_problem": "###BEGIN_OF_FORMAT###\nAssumption: \n$X$ is a non-negative random variable,\n$Y$ is a non-negative random variable,\n$\\mathbb{E}[X^\\alpha] = \\mathbb{E}[Y^\\alpha]$ for all $\\alpha \\in (0, 1/2)$,\n$\\mathbb{E}[X]$ exists,\n$\\mathbb{E}[Y]$ exists.\nProblem:\nShow that $X$ and $Y$ are equal in distribution.\n###END_OF_FORMAT###",
    "reasoning_create_structured_problem": "Okay, let's try to tackle this problem. So, the question is about showing that two non-negative random variables X and Y have the same distribution, given that their expectations for all α in (0, 1/2) are equal. Hmm. Let me start by recalling some probability theory concepts.\n\nFirst, I remember that if two random variables have the same moment generating function (MGF) in some neighborhood of zero, then they have the same distribution. But here, the exponents are only in the interval (0, 1/2), which is a bit limited. The MGF typically involves all positive real numbers, so maybe this isn't enough? Also, the problem mentions that the expectations E[X] and E[Y] exist but we don't know if they're equal. Wait, but if α approaches 1, maybe that relates to the first moment? But here α is up to 1/2. So perhaps we need a different approach.\n\nAnother thought: For non-negative random variables, the distribution can be characterized by the Laplace transform, which is E[e^{-tX}] for t ≥ 0. But the given condition is about power functions, not exponentials. Maybe using the method of moments? But the method of moments requires that the moments determine the distribution uniquely, and that the random variable is determined by its moments. However, the problem here gives us only moments for α in (0, 1/2), which are fractional moments less than 1/2. Does this suffice?\n\nWait, fractional moments. I recall that for non-negative random variables, the fractional moments E[X^α] for α in some interval can uniquely determine the distribution under certain conditions. Let me check. If the Laplace transform is analytic, then knowing it on an interval might be enough. Alternatively, maybe using the uniqueness theorem for Mellin transforms? The Mellin transform of a distribution is E[X^{s-1}], which relates to moments for complex s. If we know the Mellin transform on a real interval, perhaps that's sufficient for uniqueness.\n\nBut I need to be careful here. The Mellin transform typically requires knowledge over a strip in the complex plane. But if we have non-negative X, maybe knowing E[X^α] for all α in (0,1/2) could uniquely determine the distribution. Let me think of specific examples.\n\nSuppose X and Y are different distributions but have the same E[X^α] for α in (0,1/2). If such a pair exists, then the claim is false. Otherwise, it's true. Let's try to construct a counterexample. Let X be a distribution with heavy tails, maybe Pareto distribution with some parameter. Y could be another distribution adjusted so that their fractional moments up to 1/2 match. However, adjusting to match all moments in (0,1/2) might be non-trivial. Alternatively, maybe two different distributions could have the same fractional moments in that interval.\n\nAlternatively, consider bounded variables. Suppose X and Y are bounded. Then their moments of all orders exist, and if they have the same moments, by the moment problem, they have the same distribution. But here, the problem allows for non-negative variables, so they could be unbounded. However, the given moments are only up to 1/2. So even if they are unbounded, the problem is whether the fractional moments up to 1/2 determine the distribution.\n\nWait, maybe the answer is that they are not necessarily equal in distribution. Let me see. Suppose X and Y are such that their Laplace transforms agree on some interval but they are different. However, Laplace transforms are determined by their values on a dense set, so maybe if E[X^α] = E[Y^α] for all α in (0,1/2), then they must have the same distribution. But how?\n\nAlternatively, using the fact that for non-negative random variables, the distribution can be recovered via the integral of x^α with respect to the distribution. Maybe using the uniqueness of the Stieltjes moment problem. The Stieltjes moment problem requires that moments of all orders are known, but here we have only a subset. However, fractional moments might still determine the distribution. There is a theorem called the Müntz-Szász theorem which says that if a set of moments with exponents that are not too sparse can determine the distribution. For example, if the exponents have gaps that are bounded, then the moments uniquely determine the measure. But in our case, the exponents are in (0,1/2), which is an interval. The Müntz-Szász theorem requires that the exponents {α_n} satisfy ∑ 1/α_n diverges. If our exponents are all α in (0,1/2), then the sum over 1/α would be an integral from 0 to 1/2 of 1/α dα, which diverges. So by Müntz-Szász, the set {x^α} for α in (0,1/2) is dense in the space of continuous functions on [0, ∞) with the topology of uniform convergence on compact sets. Therefore, if two measures integrate all x^α to the same value, then they must be equal. Therefore, X and Y must have the same distribution.\n\nBut wait, the Müntz-Szász theorem is about approximation by monomials. Let me recall: the theorem states that if {λ_k} is a sequence of distinct positive real numbers such that ∑ 1/λ_k diverges, then the linear span of {x^{λ_k}} is dense in C([0,1]) with the uniform norm. However, in our case, we are dealing with measures on [0, ∞). But perhaps a similar idea applies. If the exponents α are dense in (0,1/2), then their linear combinations can approximate any continuous function on [0, ∞), which would imply that the moments E[X^α] determine the distribution. Alternatively, since the exponents are approaching 0, maybe there's an issue at 0.\n\nAlternatively, considering that if two measures agree on all functions x^α for α in (0,1/2), then they agree on the sigma-algebra generated by these functions, which might be the Borel sigma-algebra. But I need to verify this.\n\nAlternatively, consider the Laplace transform. For non-negative X, the Laplace transform E[e^{-tX}] can be related to the moments via Taylor expansion, but that requires moments of all orders. Here, we have fractional moments. Maybe another approach is to use analytic continuation. If we can express E[X^α] as an analytic function in α, then knowing it on an interval would determine it everywhere, but I don't know if that's applicable here.\n\nWait, but the function α ↦ E[X^α] is the Mellin transform of the distribution of X. Specifically, for a non-negative random variable X, the Mellin transform is E[X^{s-1}] for complex s. If we know the Mellin transform on a real interval, does that determine it uniquely? Analytic functions are determined by their values on a set with a limit point, so if the Mellin transform is analytic in some region, then knowing it on (0,1/2) would determine it in that region. However, the Mellin transform's domain of analyticity depends on the moments. For example, if X has moments of all orders, then the Mellin transform is analytic in a right half-plane. But here, we only know moments up to 1/2. Maybe the function α ↦ E[X^α] is analytic in α ∈ (0, ∞) if the moments exist. Wait, but if X is non-negative, then E[X^α] is finite for α ≤ c if X has exponential tails, or for α < β if X has power law tails. The problem states that E[X] and E[Y] exist, so α=1 is allowed, but the given condition is only for α ∈ (0,1/2). But the edit says that E[X] and E[Y] exist, but we don't know if they're equal. Hmmm.\n\nWait, the original problem says that X and Y are non-negative random variables with E[X^α] = E[Y^α] for all α ∈ (0,1/2). Additionally, E[X] and E[Y] exist. The question is whether X and Y are equal in distribution.\n\nIf we can show that E[X^α] = E[Y^α] for all α ≥ 0, then by the uniqueness of the moment problem (if the moments determine the distribution), then they would be equal in distribution. But we only have it for α ∈ (0,1/2). However, if we can extend this to all α ≥ 0, then maybe.\n\nAlternatively, consider the function φ(α) = E[X^α] - E[Y^α] = 0 for all α ∈ (0,1/2). If φ is analytic in some region containing (0,1/2), and if φ is zero on an interval, then it must be zero everywhere in its domain. Thus, if φ is analytic, then the equality would hold for all α where the moments exist, implying that X and Y have the same moments, and thus the same distribution.\n\nBut is φ(α) analytic? For non-negative X, the function E[X^α] is the Mellin transform, which is analytic in the strip where the integral converges. If X is such that E[X^α] is finite for α in some interval, then φ(α) is analytic in that interval. Given that E[X] exists, which is E[X^1], and the given condition is for α ∈ (0,1/2). So if E[X^α] is finite for α ∈ (0,1], then φ(α) is analytic in (0,1). Since φ(α) = 0 on (0,1/2), which has a limit point in (0,1), then by the identity theorem for analytic functions, φ(α) = 0 for all α ∈ (0,1). Hence, E[X^α] = E[Y^α] for all α ∈ (0,1). Then, since we also have E[X] exists, which is α=1, but does this extend to α=1?\n\nWait, if φ(α) is analytic in (0,1) and we have E[X] and E[Y] exist, but we don't know if they are equal. However, if φ(α) = 0 for α ∈ (0,1), then approaching α=1 from below, the limit would be E[X] and E[Y], which must then be equal. Hence, E[X] = E[Y]. Therefore, φ(α) = 0 for all α ∈ (0,1], which includes the first moment. Then, if the moments determine the distribution, which for non-negative random variables is sometimes the case. However, the moment problem for [0, ∞) is the Stieltjes moment problem, which requires that the moments uniquely determine the distribution if they satisfy Carleman's condition: ∑_{n=1}^∞ m_n^{-1/(2n)} = ∞, where m_n are the moments. If we have all moments, and Carleman's condition is satisfied, then the distribution is unique. Here, we might have all moments from α ∈ (0,1], but not sure. Wait, but we only have E[X^α] for α ∈ (0,1]. However, Carleman's condition uses integer moments. So maybe this approach isn't directly applicable.\n\nAlternatively, since we have E[X^α] for all α ∈ (0,1), which includes approaching α=1. If we can somehow relate this to integer moments, but I'm not sure.\n\nWait, another approach: use the fact that for non-negative random variables, the distribution can be recovered from the Laplace transform. The Laplace transform is E[e^{-tX}], which can be related to the moments via expansion, but perhaps more directly, if we can express the Laplace transform in terms of the given moments. However, I don't see a direct connection between E[X^α] and the Laplace transform.\n\nAlternatively, consider the function f(t) = E[t^X] for t ∈ (0,1). This is the generating function, but for continuous variables, maybe not. Alternatively, consider the characteristic function, but that's for real variables. For non-negative variables, perhaps using the Laplace transform again.\n\nWait, let's think about the integral representation. For a non-negative random variable X, we can write E[X^α] = ∫_0^∞ x^α dF(x), where F is the CDF. If two such integrals are equal for all α ∈ (0,1/2), then does F_X = F_Y?\n\nUsing the uniqueness of the integral transform. If the set of functions {x^α | α ∈ (0,1/2)} is dense in some function space that determines the measure, then yes. As per the Müntz-Szász theorem, which in one version says that if {λ_n} is a set of positive real numbers with ∑ 1/λ_n = ∞, then the closed linear span of {x^{λ_n}} is dense in C([0,1]) with the uniform norm. In our case, the exponents are in (0,1/2), so if we consider [0,1], then the sum over 1/λ_n would integrate 1/λ from 0 to 1/2, which diverges. Therefore, by Müntz-Szász, the monomials x^α for α ∈ (0,1/2) are dense in C([0,1]). Therefore, if two measures on [0,1] integrate all these monomials to the same value, they must be equal. However, our random variables are non-negative, so they can take values beyond 1. But maybe we can split the integral into [0,1] and (1, ∞). For the part (1, ∞), x^α ≤ x^{1/2} for α ∈ (0,1/2). If E[X^{1/2}] is finite, then by dominated convergence, the integral over (1, ∞) is controlled. But the problem states that E[X] exists, which implies E[X^{1/2}] also exists since for X ≥ 0, X^{1/2} ≤ 1 + X by AM-GM or considering cases X ≤1 and X >1.\n\nWait, indeed, for X ≥ 0, X^{1/2} ≤ 1 + X. Because if X ≤1, then X^{1/2} ≤1, and if X >1, then X^{1/2} ≤ X. So X^{1/2} ≤1 + X. Hence, E[X^{1/2}] ≤1 + E[X] < ∞. Therefore, the integral over (1, ∞) of x^α dF(x) is ≤ E[X^{1/2}] < ∞ for α ∈ (0,1/2). Therefore, the entire integral E[X^α] is finite for α ∈ (0,1/2), given that E[X] exists. But how does this help?\n\nIf we have two measures μ and ν (distributions of X and Y) such that ∫ x^α dμ = ∫ x^α dν for all α ∈ (0,1/2), then on [0,1], the Müntz-Szász theorem implies that μ and ν agree on [0,1]. For the part (1, ∞), we need to show that they also agree. But on (1, ∞), x^α is increasing in α, so knowing the integrals for α ∈ (0,1/2) might not capture the behavior at infinity. For example, consider two measures that agree on [0,1] but differ on (1, ∞). However, if we have the condition that E[X] and E[Y] exist (are finite), then perhaps we can use the fact that the tail integrals are controlled.\n\nAlternatively, consider the transformation y = 1/x, which maps (0,1) to (1, ∞). Then, the moments on (1, ∞) become moments on (0,1) of y^{-α}. But I'm not sure if this helps.\n\nAlternatively, think of the Mellin transform again. The Mellin transform of a measure on [0, ∞) is ∫_0^∞ x^{s-1} dμ(x). For our case, E[X^α} = ∫_0^∞ x^α dμ(x) = Mellin transform at s = α +1. If two measures have the same Mellin transform in an interval of s, then under certain conditions, they are equal. The Mellin transform is analytic in the strip where the integral converges. Given that E[X^α} exists for α ∈ (0,1/2) and E[X} exists (α=1), then perhaps the Mellin transform exists for Re(s) ∈ (1, 1.5) (since s = α +1, α ∈ (0, 0.5) gives s ∈ (1, 1.5)). If the Mellin transforms agree on an interval, and they are analytic, then they must agree everywhere in the strip. But then, can we extend this to all s where the transforms exist? If so, then the measures would be equal.\n\nHowever, I'm not sure about the exact conditions for the uniqueness of the Mellin transform. But generally, if two measures have Mellin transforms that agree on a set with a limit point in the domain of analyticity, then the transforms are identically equal, hence the measures are the same. Therefore, if the Mellin transforms agree for s ∈ (1, 1.5), then they must agree everywhere else they are analytic, which would include higher s, thus implying that all higher moments agree. Then, by the uniqueness of the moment problem (if Carleman's condition is satisfied), the distributions must be equal.\n\nAlternatively, if we have E[X^α] = E[Y^α} for α ∈ (0,1/2), and E[X], E[Y} exist, then using the analytic continuation argument, we can extend the equality to α=1, hence E[X} = E[Y}, and then proceed to higher moments? Not sure.\n\nAnother angle: Use the fact that the Laplace transform can be written as an integral involving x^α. Wait, but the Laplace transform is E[e^{-tX}] = ∫_0^∞ e^{-tx} dF(x). If we can relate this to the moments E[X^α}, perhaps via a series expansion. But e^{-tx} = ∑_{k=0}^∞ (-tx)^k /k!, so E[e^{-tX}] = ∑_{k=0}^∞ (-t)^k E[X^k}/k!. But this requires knowing all integer moments, which we don't have from the given data. So maybe not helpful.\n\nAlternatively, consider the function f(α) = E[X^α} - E[Y^α} = 0 for α ∈ (0,1/2). If this function is analytic in some region containing (0,1/2), and it's zero on an interval, then it must be zero everywhere in that region. For f(α) to be analytic, we need that E[X^α} is analytic in α. For non-negative X, E[X^α} is analytic in α where the integral converges. Given that E[X} exists, which implies E[X^α} exists for α ≤1. But since the original condition is for α ∈ (0,1/2), and we know E[X^α} exists there, and also E[X} exists. So maybe E[X^α} is analytic in α ∈ (0,1). If so, then f(α) is analytic in (0,1), and since it's zero on (0,1/2), which has a limit point in (0,1), then f(α) is zero on (0,1). Hence, E[X^α} = E[Y^α} for α ∈ (0,1). Then, taking α approaching 1, we get E[X} = E[Y}. Similarly, but how to get higher moments?\n\nWait, if we have E[X^α} = E[Y^α} for all α ∈ (0,1), then can we differentiate both sides with respect to α at α=0 to get moments? For example, the derivative of E[X^α} at α=0 is E[log X}, but not sure. Alternatively, maybe use a Taylor expansion around α=0. But this requires higher-order derivatives, which correspond to moments involving log X.\n\nThis seems complicated. Maybe it's better to rely on the Mellin transform argument. If the Mellin transforms agree on an interval, they must agree everywhere, hence the measures are the same. Therefore, the original statement is true: X and Y must be equal in distribution.\n\nBut I'm not entirely certain. Let me think of a concrete example. Suppose X is a constant 1, and Y is another distribution with E[Y^α} =1 for all α. Then Y must also be 1 almost surely. Another example: X ~ Exp(1), Y ~ Exp(λ). Then E[X^α} = Γ(α+1), and setting Γ(α+1) = Γ_Y(α+1) would require λ=1. So in this case, if two exponentials have the same fractional moments, they must have the same rate. So that works.\n\nBut is there a case where two different distributions share the same fractional moments in (0,1/2)? For example, take X with support {a, b} and Y with support {c, d}. Suppose we set up equations p a^α + (1-p) b^α = q c^α + (1-q) d^α for all α ∈ (0,1/2). Can this have a non-trivial solution?\n\nLet me try a simple case. Let X take value 1 with probability 1/2 and 4 with probability 1/2. Y takes value 2 with probability 1. Then E[X^α} = (1/2)(1 +4^α), E[Y^α} = 2^α. Are there α where these are equal? Solve (1 +4^α)/2 = 2^α → (1 + (2^{2α}))/2 = 2^α. Let 2^α = t. Then (1 + t^2)/2 = t → t^2 -2t +1=0 → (t-1)^2=0 → t=1. So only at α=0. But we need equality for all α ∈ (0,1/2). So in this case, only at α=0. So these distributions do not have the same moments in (0,1/2).\n\nAnother example: Let X and Y be Bernoulli with different parameters. Then their moments E[X^α} = p*1^α + (1-p)*0^α = p. Similarly for Y. So if p_X = p_Y, then they have the same moments. If p_X ≠ p_Y, then their moments differ. So in this case, moments uniquely determine p.\n\nAnother example: Let X and Y be two different distributions with the same fractional moments. Suppose X is uniform on [0,1], and Y is a different distribution. Can we make E[Y^α} = E[X^α} = 1/(α+1) for all α ∈ (0,1/2)? If Y is not uniform, this would be difficult. For example, if Y is a beta distribution with parameters a and b, then E[Y^α} = B(a + α, b)/B(a, b). To have this equal to 1/(α+1) for all α ∈ (0,1/2), we need B(a + α, b) / B(a, b) = 1/(α +1). Solving this for all α would require specific a and b. Suppose a=1, b=1 (uniform), then B(1 + α,1)/B(1,1) = Γ(1+α)Γ(1)/Γ(2+α) * Γ(2)/Γ(1)Γ(1) = (Γ(1+α)/Γ(2+α)) * 2 = (1/(1+α)) * 2, which is 2/(1+α). To have 2/(1+α) =1/(α +1), which is not possible. So unless a and b are chosen differently. Suppose Y is not beta, but some other distribution. It seems challenging to find such Y.\n\nTherefore, perhaps in general, the fractional moments in (0,1/2) do determine the distribution, given that E[X} exists. Hence, the answer is that X and Y must be equal in distribution.\n\nBut I need to make sure. Another approach: Use the fact that the Laplace transform is determined by its derivatives at zero, which relate to the moments. But if we don't have integer moments, this might not apply. Alternatively, consider the moment generating function, but for non-negative variables, the MGF may not exist except at zero.\n\nAlternatively, think of the problem in terms of the uniqueness of the solution to the Hausdorff moment problem, but that applies to compactly supported measures. Our measures are on [0, ∞).\n\nWait, another idea: Use the inverse Laplace transform. If two measures have the same Laplace transform, they are equal. But we don't have the Laplace transform, we have moments of fractional orders. However, there's a theorem called the Bernstein's theorem which states that if a function is completely monotonic and agrees with the Laplace transform of a measure, then it is unique. But I'm not sure how to connect that here.\n\nAlternatively, since X and Y are non-negative, we can use the fact that their distributions are determined by their characteristic functions. The characteristic function is E[e^{itX}], but for non-negative variables, this is analytic in the lower half-plane. However, connecting this to the given moments is unclear.\n\nAlternatively, consider the Stieltjes transform: E[(z - X)^{-1}]. This is related to moments, but again, not directly to fractional moments.\n\nGiven all this, I'm leaning towards the conclusion that if two non-negative random variables have the same fractional moments E[X^α} for all α ∈ (0,1/2), and their first moments exist, then they must have the same distribution. The key reason is the Müntz-Szász theorem applied to the interval [0,1], combined with the control on the tail given by the existence of the first moment. Therefore, the original claim is correct: X and Y must be equal in distribution.",
    "Original_Problem": "Assumption: \n$X$ is a non-negative random variable,\n$Y$ is a non-negative random variable,\n$\\mathbb{E}[X^\\alpha] = \\mathbb{E}[Y^\\alpha]$ for all $\\alpha \\in (0, 1/2)$,\n$\\mathbb{E}[X]$ exists,\n$\\mathbb{E}[Y]$ exists.\nProblem:\nShow that $X$ and $Y$ are equal in distribution.",
    "Original_Problem_with_numerical_assumption": "Assumption:\nAssumption 1: $X$ is a non-negative random variable,\nAssumption 2: $Y$ is a non-negative random variable,\nAssumption 3: $\\mathbb{E}[X^\\alpha] = \\mathbb{E}[Y^\\alpha]$ for all $\\alpha \\in (0, 1/2)$,\nAssumption 4: $\\mathbb{E}[X]$ exists,\nAssumption 5: $\\mathbb{E}[Y]$ exists.\nProblem:\nShow that $X$ and $Y$ are equal in distribution.",
    "Proof_problem": "Yes",
    "Redundant_assumption": "$$\nf(\\alpha)=\\mathbb{E}[1_{\\{X > 0\\}}e^{\\alpha U}],\\ g(\\alpha)=\\mathbb{E}[1_{\\{Y > 0\\}}e^{\\alpha V}], \n$$",
    "Problem_with_redundant_assumption": "Assumption:\nAssumption 1: $X$ is a non-negative random variable,\nAssumption 2: $Y$ is a non-negative random variable,\nAssumption 3: $\\mathbb{E}[X^\\alpha] = \\mathbb{E}[Y^\\alpha]$ for all $\\alpha \\in (0, 1/2)$,\nAssumption 4: $\\mathbb{E}[X]$ exists,\nAssumption 5: $\\mathbb{E}[Y]$ exists.\nAssumption 6: $$\nf(\\alpha)=\\mathbb{E}[1_{\\{X > 0\\}}e^{\\alpha U}],\\ g(\\alpha)=\\mathbb{E}[1_{\\{Y > 0\\}}e^{\\alpha V}], \n$$\nProblem:\nShow that $X$ and $Y$ are equal in distribution."
}