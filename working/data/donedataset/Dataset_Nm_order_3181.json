{
    "Link_API": "https://api.stackexchange.com/2.3/questions/497806",
    "Title": "Matrices $B$ that commute with every matrix commuting with $A$",
    "Score": 12,
    "Category": "Chứng minh",
    "Tags": "linear-algebra, matrices, noncommutative-algebra",
    "Link": "https://math.stackexchange.com/questions/497806/matrices-b-that-commute-with-every-matrix-commuting-with-a",
    "Content": "There have been many questions in the vein of this one, but I can't find one that answers it specifically. Suppose $A,B\\in M_n(\\mathbb C)$ are two matrices such that, for any other matrix $C\\in M_n(\\mathbb C)$ , $$AC=CA\\implies BC=CB.$$ Prove that $B=p(A)$ for a polynomial $p\\in\\mathbb C[t]$ . Nothing is assumed about $A$ being diagonalisable or having distinct eigenvalues or being invertible. Edit: after further work, this might not actually be true. If it's not, a counterexample would be great.",
    "AcceptedAnswer": "Reference: Lagerstrom, Paco, A Proof of a Theorem on Commutative Matrices , Bulletin of AMS, Volume 51 (1945), 535-536. This holds for any field $\\mathbb{F}$. Let $M^A$ denote the $\\mathbb{F}[x]$-module structure on $\\mathbb{F}^n$. \nWe use rational canonical form-invariant factor form. Then we have\n    $$\n    M^A \\simeq \\mathbb{F}[x]/P_1 \\oplus\\cdots \\oplus \\mathbb{F}[x]/P_r,\n    $$\n    where $P_i=(p_i)$, $p_i|p_{i+1}$.\n    This gives invariant subspace decomposition,\n    $$\n    M^A =\\bigoplus_{i=1}^r M_i,\n    $$\n    where $M_i\\simeq \\mathbb{F}[x]/P_i$. Let $\\pi_i:M^A\\longrightarrow M_i$ be the projection, and $\\pi_{ij}:M_i\\longrightarrow M_j$ be the natural projection for $i>j$. Extend $\\pi_{ij}$ linearly to $M^A$ by assigning 0 on all $M_k(k\\neq i)$. Then all $\\pi_i$ and $\\pi_{ij}$ commute with $A$, thus commute with $B$. Therefore, each $M_i$ is $A$-invariant, thus it is also $B$-invariant. \nLet $e_i\\in M_i$ be the element corresponding to $1+P_i\\in\\mathbb{F}[x]/P_i$.\n    We see that there is  $p(x)\\in\\mathbb{F}[x]$ such that $Be_r=p(A)e_r$. We claim that $Be_i=p(A)e_i$ for all $i<r$, and hence $B=p(A)$.\n    $$\n    Be_i=B\\pi_{ri}e_r=\\pi_{ri}Be_r=\\pi_{ri}p(A)e_r=p(A)\\pi_{ri}e_r=p(A)e_i.\n    $$\n    This completes the proof.",
    "llm_answer_create_structured_problem": "###BEGIN_OF_FORMAT###\nAssumption: \n$A$ is a matrix in $M_n(\\mathbb{C})$,\n$B$ is a matrix in $M_n(\\mathbb{C})$,\nFor every matrix $C \\in M_n(\\mathbb{C})$, if $AC = CA$, then $BC = CB$,\nNo assumption is made about $A$ being diagonalizable,\nNo assumption is made about $A$ having distinct eigenvalues,\nNo assumption is made about $A$ being invertible.\nProblem: \nProve that $B = p(A)$ for some polynomial $p \\in \\mathbb{C}[t]$.\n###END_OF_FORMAT###",
    "reasoning_create_structured_problem": "Okay, let me try to work through this problem step by step. So, the problem is about matrices A and B in the set of n×n complex matrices, M_n(ℂ). The condition given is that for any matrix C in M_n(ℂ), if C commutes with A (i.e., AC = CA), then C must also commute with B (BC = CB). We need to prove that B is a polynomial in A, that is, B = p(A) for some polynomial p with complex coefficients. Also, there's an edit note suggesting that the original claim might not hold, but let's first try to see if it's true and then check for possible counterexamples.\n\nFirst, let's recall some linear algebra concepts. If two matrices commute with each other, they share some properties, especially if they are diagonalizable. However, in this problem, A is not assumed to be diagonalizable or invertible, so we can't rely on that. The key here seems to be the set of matrices that commute with A. Let's denote the set of all matrices commuting with A as the centralizer of A, written as C(A) = { C ∈ M_n(ℂ) | AC = CA }. The problem states that any matrix in C(A) must also commute with B. So, B must commute with every element of C(A). Therefore, B is in the centralizer of C(A), which would be the double centralizer. \n\nNow, there's a theorem in linear algebra called the double centralizer theorem. It states that for a finite-dimensional vector space over a field, the double centralizer of a subset S (under some conditions) is equal to the algebra generated by S. In our case, if we consider the algebra generated by A, which consists of all polynomials in A, then the centralizer of C(A) should be this algebra. Therefore, B must be in this algebra, meaning B is a polynomial in A. That seems to be the direction to go.\n\nWait, but let me verify the double centralizer theorem. For a single operator A on a finite-dimensional vector space over ℂ, the centralizer of the centralizer of A (i.e., C(C(A))) is equal to the algebra generated by A and the identity matrix. That's the statement, right? So if B commutes with every element of C(A), then B is in C(C(A))), which by the theorem should be the algebra generated by A. Since the field is ℂ, which is algebraically closed, the algebra generated by A is indeed the set of all polynomials in A. Therefore, B must be a polynomial in A. So, this would prove the statement.\n\nBut wait, the edit in the original question says that after further work, it might not be true. Maybe the user found a counterexample? Let me think. The key here is whether the double centralizer theorem applies here. For a single matrix A, in the case where the centralizer C(A) is as large as possible, maybe if A is a scalar matrix, then C(A) is all of M_n(ℂ), so the centralizer of that would be the center, which is scalar matrices. But scalar matrices are polynomials of A (if A is scalar). Wait, if A is scalar, say A = λI, then any polynomial in A is also a scalar matrix. So in that case, B would have to be a scalar matrix. But in this case, the condition is that if C commutes with A (which is always true, since A is scalar), then C must commute with B. So B must commute with every matrix C, which implies B is scalar. So that's consistent with the theorem, since scalar matrices are polynomials of A.\n\nAnother case: suppose A is a Jordan block. Then the centralizer C(A) consists of upper triangular Toeplitz matrices (matrices that are polynomials in A). Then the centralizer of C(A) would again be polynomials in A. So in that case, B would have to be a polynomial in A. Hmm.\n\nWait, but maybe there's a case where A is not cyclic, i.e., its Jordan form has more than one block. Suppose A is diagonal with two distinct eigenvalues. Then C(A) consists of all block diagonal matrices with blocks corresponding to the eigenvalues. Then the centralizer of C(A) would be the set of matrices that are block diagonal in the same partition. But if A is diagonal with two distinct eigenvalues, say λ and μ, then the algebra generated by A would consist of diagonal matrices where the entries are f(λ) and f(μ) for some polynomial f. However, the centralizer of C(A) in this case would be the set of matrices that are block diagonal, but each block can be arbitrary within their blocks. Wait, no. If A is diagonal with two blocks of λ and μ, then C(A) consists of all block diagonal matrices (with blocks of appropriate sizes). Then the centralizer of C(A) would be matrices that commute with all such block diagonal matrices. Such matrices must be scalar within each block. Wait, but if you have a block diagonal matrix where each block is arbitrary, then commuting with all such would require that the matrix is scalar in each block. But if A itself is diagonal with two different eigenvalues, then the algebra generated by A would be diagonal matrices with entries constant on each block. Wait, but if you take polynomials in A, since A is diagonal, they would be diagonal matrices where each diagonal entry is a polynomial evaluated at the corresponding eigenvalue. If A has two distinct eigenvalues, say λ and μ, then polynomials in A would give you diagonal matrices where entries are of the form p(λ) and p(μ) for some polynomial p. However, the centralizer of C(A) would be matrices that are scalar on each block (since commuting with all block diagonal matrices implies that they must be scalar on each block). Therefore, the centralizer of C(A) is larger than the algebra generated by A if there are repeated eigenvalues. Wait, for example, if A is diagonal with entries λ, λ, μ, μ (two blocks of size 2 each), then C(A) consists of block diagonal matrices with two 2×2 blocks. The centralizer of C(A) would be matrices that are block diagonal, with each block being scalar matrices. So, in this case, the centralizer of C(A) is the set of matrices of the form [a I_2 0; 0 b I_2], where a and b are scalars. However, the algebra generated by A would be diagonal matrices where the first two entries are p(λ) and the last two are p(μ). Therefore, unless λ = μ, the algebra generated by A is a 2-dimensional algebra, while the centralizer of C(A) is a 2-dimensional algebra as well (since a and b can be arbitrary). Wait, but in this case, if we take p(A) such that p(λ) = a and p(μ) = b, then since we can choose a polynomial p that interpolates these values (since λ ≠ μ), then any matrix in the centralizer of C(A) can be expressed as p(A) for some polynomial p. Therefore, in this case, the centralizer of C(A) is equal to the algebra generated by A. So perhaps the theorem holds here.\n\nBut wait, suppose A is a diagonal matrix with three distinct eigenvalues. Then similar logic applies. The centralizer of C(A) would be diagonal matrices where entries corresponding to the same eigenvalue are equal. But polynomials in A can achieve any such diagonal matrix by choosing a polynomial that takes the desired values at each eigenvalue. Since with three distinct eigenvalues, we can construct a polynomial p such that p(λ_i) = a_i for each eigenvalue λ_i. Therefore, the algebra generated by A is equal to the centralizer of C(A). \n\nSo maybe the theorem does hold. However, the user mentioned an edit suggesting a possible counterexample. Let me think of a case where A is not cyclic, meaning that its minimal polynomial has degree less than n, but I'm not sure. Wait, another example: suppose A is the zero matrix. Then C(A) is all of M_n(ℂ). Then the centralizer of C(A) is the center of M_n(ℂ), which is scalar matrices. But polynomials in A (the zero matrix) are scalar multiples of the zero matrix, i.e., all scalar matrices? Wait, no. If A is the zero matrix, then p(A) is p(0)I. So polynomials in A would be scalar matrices where the scalar is p(0) for some polynomial p. However, the centralizer of C(A) (which is the center) is all scalar matrices. Therefore, unless the field is algebraically closed and we can take p(0) to be any scalar, but in ℂ, any scalar can be expressed as p(0) for some polynomial p. Wait, no. If you take p(t) = c, a constant polynomial, then p(A) = cI. So in fact, polynomials in A (the zero matrix) are exactly the scalar matrices. Therefore, in this case, the centralizer of C(A) is equal to the algebra generated by A. So again, the theorem holds.\n\nHmm. Maybe the user's doubt was misplaced, and the theorem does hold. Alternatively, perhaps there's a case where A is not cyclic, and the centralizer of C(A) is larger than the algebra generated by A. Wait, let's suppose A is a Jordan block of size n. Then C(A) is the set of upper triangular Toeplitz matrices. The centralizer of C(A) would be the set of matrices that commute with all upper triangular Toeplitz matrices. What's that set? It's known that the centralizer of the set of upper triangular Toeplitz matrices is the set of upper triangular Toeplitz matrices themselves. Wait, but in this case, the centralizer of C(A) would be C(A), and since A generates the algebra of upper triangular Toeplitz matrices (which is the same as polynomials in A), then again, the centralizer of C(A) is the algebra generated by A. So B must be a polynomial in A.\n\nAlternatively, perhaps if A is a direct sum of two Jordan blocks with the same eigenvalue. Let's say A = J_2(λ) ⊕ J_2(λ), so a 4×4 matrix with two Jordan blocks of size 2. Then C(A) consists of matrices of the form [P Q; R S], where P, Q, R, S are 2×2 upper triangular Toeplitz matrices. Wait, no. If A is a block diagonal matrix with two Jordan blocks, then matrices that commute with A must be block diagonal, with each block commuting with the corresponding Jordan block. So C(A) would consist of block diagonal matrices where each block is an upper triangular Toeplitz matrix. Then the centralizer of C(A) would consist of matrices that commute with all such block diagonal matrices. Such matrices must be block diagonal, and within each block, they must commute with all upper triangular Toeplitz matrices. The centralizer of the upper triangular Toeplitz matrices (in 2×2 matrices) is the set of upper triangular Toeplitz matrices themselves. Therefore, the centralizer of C(A) would be block diagonal matrices where each block is upper triangular Toeplitz. But these are exactly the polynomials in A. Because A is the direct sum of two Jordan blocks, any polynomial in A would be the direct sum of the same polynomial in each block. Since each block is a Jordan block, polynomials in each block are upper triangular Toeplitz, so the entire matrix would be block diagonal with upper triangular Toeplitz blocks. Hence, the centralizer of C(A) is equal to the algebra generated by A. Therefore, B must be a polynomial in A.\n\nSo in all these cases, the theorem seems to hold. Maybe the user was confused, but according to the double centralizer theorem, this should be true. Let me check a reference. In the book \"A Course in Ring Theory\" by P. Lam, or maybe in \"Algebra\" by Lang. Alternatively, in \"Linear Algebra\" by Hoffman and Kunze. In Hoffman and Kunze, Chapter 6, Section 4, there's a theorem: Let V be a finite-dimensional vector space over F, and let A be a linear operator on V. Then the centralizer of the centralizer of A (i.e., C(C(A))) is the set of all operators which are polynomials in A with coefficients from F. \n\nYes! That's exactly the theorem we need. So in our case, since we're working over ℂ, which is a field, and V is ℂ^n, the theorem applies. Therefore, if B commutes with every operator that commutes with A, then B is a polynomial in A. Therefore, the original statement is true, and the user's doubt might have been due to a misunderstanding. Therefore, the answer should confirm the statement and cite the double centralizer theorem.\n\nHowever, the user mentioned \"after further work, this might not actually be true. If it's not, a counterexample would be great.\" But based on the theorem from Hoffman and Kunze, it should be true. Therefore, unless there's a mistake in the theorem's conditions. Let me check the exact statement.\n\nThe theorem in Hoffman and Kunze states: Let V be a finite-dimensional vector space over the field F, and let A be a linear operator on V. Let C(A) be the set of all linear operators on V which commute with A. Then the centralizer of C(A) is the set of all operators which are polynomials in A. \n\nTherefore, the conclusion is that B must be a polynomial in A. Hence, the original problem's statement is correct, and there's no counterexample. The user's edit might be incorrect. \n\nTherefore, the formatted problem should list the assumptions and the problem as per the user's request. The key assumptions here are that A and B are n×n complex matrices, and that for every matrix C, if AC=CA then BC=CB. Nothing is assumed about A's diagonalizability, eigenvalues, or invertibility. Therefore, the assumptions should capture these points.",
    "Original_Problem": "Assumption: \n$A$ is a matrix in $M_n(\\mathbb{C})$,\n$B$ is a matrix in $M_n(\\mathbb{C})$,\nFor every matrix $C \\in M_n(\\mathbb{C})$, if $AC = CA$, then $BC = CB$,\nNo assumption is made about $A$ being diagonalizable,\nNo assumption is made about $A$ having distinct eigenvalues,\nNo assumption is made about $A$ being invertible.\nProblem: \nProve that $B = p(A)$ for some polynomial $p \\in \\mathbb{C}[t]$.",
    "Original_Problem_with_numerical_assumption": "Assumption:\nAssumption 1: $A$ is a matrix in $M_n(\\mathbb{C})$,\nAssumption 2: $B$ is a matrix in $M_n(\\mathbb{C})$,\nAssumption 3: For every matrix $C \\in M_n(\\mathbb{C})$, if $AC = CA$, then $BC = CB$,\nAssumption 4: No assumption is made about $A$ being diagonalizable,\nAssumption 5: No assumption is made about $A$ having distinct eigenvalues,\nAssumption 6: No assumption is made about $A$ being invertible.\nProblem: \nProve that $B = p(A)$ for some polynomial $p \\in \\mathbb{C}[t]$.",
    "Proof_problem": "Yes",
    "Redundant_assumption": "$$\n    Be_i=B\\pi_{ri}e_r=\\pi_{ri}Be_r=\\pi_{ri}p(A)e_r=p(A)\\pi_{ri}e_r=p(A)e_i.\n    $$",
    "Problem_with_redundant_assumption": "Assumption:\nAssumption 1: $A$ is a matrix in $M_n(\\mathbb{C})$,\nAssumption 2: $B$ is a matrix in $M_n(\\mathbb{C})$,\nAssumption 3: For every matrix $C \\in M_n(\\mathbb{C})$, if $AC = CA$, then $BC = CB$,\nAssumption 4: No assumption is made about $A$ being diagonalizable,\nAssumption 5: No assumption is made about $A$ having distinct eigenvalues,\nAssumption 6: No assumption is made about $A$ being invertible.\nAssumption 7: $$\n    Be_i=B\\pi_{ri}e_r=\\pi_{ri}Be_r=\\pi_{ri}p(A)e_r=p(A)\\pi_{ri}e_r=p(A)e_i.\n    $$\nProblem: \nProve that $B = p(A)$ for some polynomial $p \\in \\mathbb{C}[t]$."
}