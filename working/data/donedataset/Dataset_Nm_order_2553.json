{
    "Link_API": "https://api.stackexchange.com/2.3/questions/2822190",
    "Title": "Proof that Markov Matrix has Eigenvalue of 1",
    "Score": 13,
    "Category": "Chứng minh",
    "Tags": "linear-algebra, matrices, eigenvalues-eigenvectors",
    "Link": "https://math.stackexchange.com/questions/2822190/proof-that-markov-matrix-has-eigenvalue-of-1",
    "Content": "Taken from “Introduction to Linear Algebra” by Gilbert Strang: I can verify this property of Markov matrices through computation on example matrices, but can someone please provide clarity on the final statement that 1 is an Eigenvalue?",
    "AcceptedAnswer": "Since each column of $A$ sums to $1$, each column of $A-I$ sums to $0.$  This means that the sum of the rows (linear combination with coefficients all equal to $1$), is the $0$ vector. If there is a linear combination of row vectors with not all zero coefficients, then the rows are linearly dependent, and any matrix with linearly dependent rows (or columns) must have determinant $0.$  Thus, $\\det(A-I) = 0,$  so by definition, $\\lambda_1 = 1$ is an eigenvalue. Edit: Recall that $\\lambda$ is an eigenvalue of $A$ if and only if $Av = \\lambda v$ for some nonzero vector $v.$  Rearranging, we can see that this statement is equivalent to $(A-\\lambda I)v = 0.$  If $A-\\lambda I$ is invertible, then $v = (A-\\lambda I)^{-1} \\cdot 0 = 0,$  which is a contradiction. So, we must have that $A-\\lambda I$ is not invertible, i.e. $\\det(A-\\lambda I) = 0.$  Thus, an alternate definition is $\\lambda$ is an eigenvalue of $A$ if and only if $\\det(A-\\lambda I) = 0.$  Since we have that $\\det(A-1\\cdot I) = 0,$ $\\lambda = 1$ is an eigenvalue.",
    "llm_answer_create_structured_problem": "###BEGIN_OF_FORMAT###\nAssumption: \nThe matrix is a square matrix,\nAll entries in the matrix are non-negative,\nEach column of the matrix sums to 1.\nProblem:\nProve that the Markov matrix has an eigenvalue of 1.\n###",
    "reasoning_create_structured_problem": "Okay, let me try to work through this problem step by step. So, the task is to prove that a Markov matrix has an eigenvalue of 1. First, I need to recall what a Markov matrix is. From what I remember, a Markov matrix, also known as a stochastic matrix, is a square matrix where each entry is non-negative and the sum of the entries in each column is equal to 1. Wait, or is it each row? Hmm, I should double-check that. I think it's actually the columns that sum to 1. Let me confirm: yes, a right stochastic matrix (which is often what's referred to as a Markov matrix) has non-negative entries and each column sums to 1. So that's a key point.\n\nNow, eigenvalues. The problem states that 1 is an eigenvalue of such a matrix. To prove that, I need to show that there exists a non-zero vector v such that Av = v, where A is the Markov matrix. That equation can be rewritten as (A - I)v = 0, which means that the matrix (A - I) must be singular. A matrix is singular if its determinant is zero, so another approach could be to show that det(A - I) = 0. But calculating determinants for large matrices can be cumbersome. Maybe there's a more straightforward way.\n\nLet me think about the properties of the matrix. Since each column of A sums to 1, if I sum the rows of A, each column contributes 1 to the total sum. Wait, actually, the sum of each column is 1. So if I consider the vector that's all ones, say e = [1, 1, ..., 1]^T, then multiplying A^T by e would give a vector where each entry is the sum of the corresponding column of A^T, which are the rows of A. Wait, no. Let me clarify. If A is a column stochastic matrix (columns sum to 1), then A^T is row stochastic (rows sum to 1). But the eigenvalue we're interested in is for A, not A^T. However, eigenvalues of A and A^T are the same. But maybe the eigenvector corresponding to eigenvalue 1 for A is different from that of A^T.\n\nAlternatively, since the columns of A sum to 1, if I multiply A by a vector whose entries are all 1s, what happens? Let's see. Let e be the vector with all entries 1. Then (Ae)_i = sum_{j} A_{ij} e_j = sum_{j} A_{ij} * 1 = sum of the ith row of A. Wait, but if the columns sum to 1, that doesn't necessarily mean the rows sum to 1. So Ae would be a vector where each entry is the sum of the rows of A. But if columns sum to 1, rows can sum to anything. Hmm, maybe that's not the right approach.\n\nWait, no. If A is column stochastic, then each column sums to 1. So if I take the vector e = [1, 1, ..., 1]^T, then e^T A would be a row vector where each entry is the sum of the columns of A, which is 1 for each column. So e^T A = e^T. That means that e^T is a left eigenvector of A corresponding to eigenvalue 1. Therefore, 1 is an eigenvalue of A^T, and since eigenvalues of A and A^T are the same, 1 is also an eigenvalue of A. But does that mean that A has a right eigenvector corresponding to eigenvalue 1? Not necessarily directly, but since the eigenvalues are the same, there must exist some non-zero vector v such that Av = v. Wait, but maybe there's a more direct way to see this.\n\nAlternatively, let's consider the matrix A - I. If I can show that this matrix is singular, then 1 is an eigenvalue. To show that A - I is singular, we can show that its determinant is zero. But how? The sum of the columns of A is each 1, so if we sum the columns of A - I, each column would be (A's column) minus the standard basis vector. So the sum of the columns of A - I would be (sum of columns of A) - (sum of standard basis vectors). Since each column of A sums to 1, the sum of columns of A is a vector where each entry is 1. Then subtracting the sum of standard basis vectors, which is also a vector of ones, gives a zero vector. Therefore, the columns of A - I are linearly dependent, which implies that the determinant is zero. Hence, 1 is an eigenvalue.\n\nWait, that seems like a solid argument. Let me break it down. If we take the matrix A - I, then the sum of each column of A is 1. So when we look at A - I, each column is the original column of A minus the standard basis vector (which has a 1 in the corresponding position). If we add up all the columns of A - I, we get (sum of columns of A) minus (sum of standard basis vectors). The sum of columns of A is a vector where each entry is 1 (since each column sums to 1). The sum of standard basis vectors is a vector of all ones as well. So 1 - 1 = 0. Therefore, the sum of the columns of A - I is the zero vector, which means the columns are linearly dependent. Therefore, the matrix A - I is singular, so its determinant is zero. Therefore, 1 is an eigenvalue of A.\n\nSo that's the proof. To recap: since the columns of A sum to 1, subtracting the identity matrix makes the columns of A - I sum to zero, implying linear dependence among the columns, hence the determinant is zero, so 1 is an eigenvalue.\n\nAnother way to see it is by considering the transpose. Since A^T is a row stochastic matrix (rows sum to 1), then A^T has an eigenvalue of 1 with the eigenvector being the vector of all ones. Therefore, A must have 1 as an eigenvalue because A and A^T share the same eigenvalues.\n\nBut maybe the first approach is more straightforward. The key point is that the columns summing to 1 lead to the matrix A - I having linearly dependent columns, hence determinant zero, hence 1 is an eigenvalue.\n\nSo the problem is to prove that 1 is an eigenvalue of a Markov matrix, given that the matrix is a square matrix with non-negative entries and each column sums to 1. The assumptions would then include the properties that define a Markov matrix.",
    "Original_Problem": "Assumption: \nThe matrix is a square matrix,\nAll entries in the matrix are non-negative,\nEach column of the matrix sums to 1.\nProblem:\nProve that the Markov matrix has an eigenvalue of 1.",
    "Original_Problem_with_numerical_assumption": "Assumption:\nAssumption 1: The matrix is a square matrix,\nAssumption 2: All entries in the matrix are non-negative,\nAssumption 3: Each column of the matrix sums to 1.\nProblem:\nProve that the Markov matrix has an eigenvalue of 1.",
    "Proof_problem": "Yes",
    "Redundant_assumption": "$\\det(A-I) = 0,$",
    "Problem_with_redundant_assumption": "Assumption:\nAssumption 1: The matrix is a square matrix,\nAssumption 2: All entries in the matrix are non-negative,\nAssumption 3: Each column of the matrix sums to 1.\nAssumption 4: $\\det(A-I) = 0,$\nProblem:\nProve that the Markov matrix has an eigenvalue of 1."
}