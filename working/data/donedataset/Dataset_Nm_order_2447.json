{
    "Link_API": "https://api.stackexchange.com/2.3/questions/2167643",
    "Title": "Conditional expectation of a joint normal distribution",
    "Score": 14,
    "Category": "Ch·ª©ng minh",
    "Tags": "probability, statistics, probability-distributions, normal-distribution, conditional-expectation",
    "Link": "https://math.stackexchange.com/questions/2167643/conditional-expectation-of-a-joint-normal-distribution",
    "Content": "Let $X_1, X_2$ be jointly normal $N(\\mu, \\Sigma)$. I know that in general, $\\mathbb{E}[X_2|X_1]$ can be computed by integrating the conditional density, but in the case of jointly normal variables, it suffices to do a linear projection: $\\mathbb{E}[X_2 | \\sigma(X_1)] = \\mathbb{E}[X_2|\\mathrm{span}(\\mathbf{1}, X_1)] = \\mu_2 + \\frac{\\mathrm{cov}(X_2, X_1)}{\\mathrm{var}(X_1)} (X_1 - \\mu_1) $ Is there a neat proof of this fact (one doesn't require doing any integrals)? Looking for references too.",
    "AcceptedAnswer": "I've found an answer that I'm happy with: $$ Y - \\frac{\\mathrm{cov}(X, Y)}{\\mathrm{var}(X)} X $$ is jointly normal with $X$ and uncorrelated, hence independent. Therefore $$\\mathbb{E}[Y|X] = \\mathbb{E}[Y - \\frac{\\mathrm{cov}(X, Y)}{\\mathrm{var}(X)} X + \\frac{\\mathrm{cov}(X, Y)}{\\mathrm{var}(X)} X| X] \\\\\n= \\mathbb{E}[Y - \\frac{\\mathrm{cov}(X, Y)}{\\mathrm{var}(X)} X] +\\frac{\\mathrm{cov}(X, Y)}{\\mathrm{var}(X)}X \\\\\n=  \\mathbb{E} [Y] + \\frac{\\mathrm{cov}(X, Y)}{\\mathrm{var}(X)} (X - \\mathbb{E}[X])$$",
    "llm_answer_create_structured_problem": "###BEGIN_OF_FORMAT###\nAssumption: \n$X_1$ and $X_2$ are jointly normally distributed,\nThe joint distribution of $X_1$ and $X_2$ has mean vector $\\mu$,\nThe joint distribution of $X_1$ and $X_2$ has covariance matrix $\\Sigma$.\nProblem:\nProve that $$\\mathbb{E}[X_2 | \\sigma(X_1)] = \\mu_2 + \\frac{\\mathrm{cov}(X_2, X_1)}{\\mathrm{var}(X_1)} (X_1 - \\mu_1)$$ without integrating the conditional density.\n###",
    "reasoning_create_structured_problem": "Okay, so I need to structure the problem from the Math Stack Exchange question about the conditional expectation of jointly normal variables. Let me start by reading the original question carefully.\n\nThe user mentions that X1 and X2 are jointly normal with mean Œº and covariance matrix Œ£. They want to know if there's a neat proof, without integrals, that the conditional expectation E[X2|X1] is a linear projection given by Œº2 + (cov(X2,X1)/var(X1))(X1 - Œº1). They also ask for references.\n\nFirst, I need to extract the assumptions. The problem states that X1 and X2 are jointly normal. So the first assumption is that X1 and X2 have a joint normal distribution. Then, the parameters are given as N(Œº, Œ£), so the mean vector is Œº and covariance matrix is Œ£. Those are separate assumptions. \n\nNext, the problem is about finding the conditional expectation E[X2|X1] using a linear projection method without integrating. The user mentions that generally, you might integrate the conditional density, but for joint normals, a linear projection suffices. The formula given includes Œº2, the covariance between X2 and X1, and the variance of X1. \n\nSo the assumptions should list each property one by one. Let me check: \n\n1. X1 and X2 are jointly normally distributed. \n2. The joint distribution has mean vector Œº. \n3. The joint distribution has covariance matrix Œ£. \n\nAre there more assumptions? The problem is asking about the form of the conditional expectation, so maybe there's an assumption about the projection? Wait, no. The user is stating that in the case of jointly normal variables, the linear projection suffices, so that's part of the problem statement, not an assumption. The assumptions are just the properties of the distribution.\n\nSo the problem is to prove that the conditional expectation formula holds without integrating, using linear projection. The user is looking for a neat proof and references. But in the required format, the \"problem\" part should just state the actual mathematical problem, not the request for references or the method. So the problem is to show that E[X2 | X1] equals that linear expression.\n\nWait, in the example given, the problem was to prove the divisibility, and the assumptions included the conditions on n. Here, the assumptions are the joint normality, the mean vector, and the covariance matrix. The problem is to prove that the conditional expectation has that specific linear form. The user's question includes that they know the general method (integration) but want a proof using linear projection. So maybe the problem statement should include that it's using the linear projection approach?\n\nBut according to the format, the problem should be exactly what's being asked. So the problem is to prove that E[X2 | œÉ(X1)] equals that linear expression. The assumptions are the joint normality, mean, and covariance. The user's question includes that they know the integration method but want a linear projection proof. However, in the example provided, the assumption included that n is an integer and divisible by 7. So in that case, the problem was structured with those assumptions. Here, the assumptions are about the distribution of X1 and X2. \n\nSo putting it all together:\n\nAssumptions:\n- X1 and X2 are jointly normally distributed.\n- The joint distribution of X1 and X2 has mean vector Œº.\n- The joint distribution of X1 and X2 has covariance matrix Œ£.\n\nProblem: Prove that ùîº[X‚ÇÇ | œÉ(X‚ÇÅ)] = Œº‚ÇÇ + (cov(X‚ÇÇ, X‚ÇÅ)/var(X‚ÇÅ))(X‚ÇÅ - Œº‚ÇÅ) without integrating the conditional density.\n\nWait, the original problem states \"Is there a neat proof of this fact (one that doesn't require doing any integrals)? Looking for references too.\" But according to the format, the problem should be phrased as the mathematical statement to prove, not the request for a proof method or references. So the problem is to prove that the conditional expectation is equal to that linear expression, under the given assumptions.\n\nSo the final formatted problem should have the three assumptions listed, and the problem is to prove that the conditional expectation equals that formula.\n\nI need to make sure each assumption is on a separate line and applies to one object. The first assumption is about X1 and X2 being jointly normal. The second is about their mean vector Œº, and the third about the covariance matrix Œ£. Each is a separate condition applied to the joint distribution. That seems correct.",
    "Original_Problem": "Assumption: \n$X_1$ and $X_2$ are jointly normally distributed,\nThe joint distribution of $X_1$ and $X_2$ has mean vector $\\mu$,\nThe joint distribution of $X_1$ and $X_2$ has covariance matrix $\\Sigma$.\nProblem:\nProve that $$\\mathbb{E}[X_2 | \\sigma(X_1)] = \\mu_2 + \\frac{\\mathrm{cov}(X_2, X_1)}{\\mathrm{var}(X_1)} (X_1 - \\mu_1)$$ without integrating the conditional density.",
    "Original_Problem_with_numerical_assumption": "Assumption:\nAssumption 1: $X_1$ and $X_2$ are jointly normally distributed,\nAssumption 2: The joint distribution of $X_1$ and $X_2$ has mean vector $\\mu$,\nAssumption 3: The joint distribution of $X_1$ and $X_2$ has covariance matrix $\\Sigma$.\nProblem:\nProve that $$\\mathbb{E}[X_2 | \\sigma(X_1)] = \\mu_2 + \\frac{\\mathrm{cov}(X_2, X_1)}{\\mathrm{var}(X_1)} (X_1 - \\mu_1)$$ without integrating the conditional density.",
    "Proof_problem": "Yes",
    "Redundant_assumption": "$$\\mathbb{E}[Y|X] = \\mathbb{E}[Y - \\frac{\\mathrm{cov}(X, Y)}{\\mathrm{var}(X)} X + \\frac{\\mathrm{cov}(X, Y)}{\\mathrm{var}(X)} X| X] \\\\\n= \\mathbb{E}[Y - \\frac{\\mathrm{cov}(X, Y)}{\\mathrm{var}(X)} X] +\\frac{\\mathrm{cov}(X, Y)}{\\mathrm{var}(X)}X \\\\\n=  \\mathbb{E} [Y] + \\frac{\\mathrm{cov}(X, Y)}{\\mathrm{var}(X)} (X - \\mathbb{E}[X])$$",
    "Problem_with_redundant_assumption": "Assumption:\nAssumption 1: $X_1$ and $X_2$ are jointly normally distributed,\nAssumption 2: The joint distribution of $X_1$ and $X_2$ has mean vector $\\mu$,\nAssumption 3: The joint distribution of $X_1$ and $X_2$ has covariance matrix $\\Sigma$.\nAssumption 4: $$\\mathbb{E}[Y|X] = \\mathbb{E}[Y - \\frac{\\mathrm{cov}(X, Y)}{\\mathrm{var}(X)} X + \\frac{\\mathrm{cov}(X, Y)}{\\mathrm{var}(X)} X| X] \\\\\n= \\mathbb{E}[Y - \\frac{\\mathrm{cov}(X, Y)}{\\mathrm{var}(X)} X] +\\frac{\\mathrm{cov}(X, Y)}{\\mathrm{var}(X)}X \\\\\n=  \\mathbb{E} [Y] + \\frac{\\mathrm{cov}(X, Y)}{\\mathrm{var}(X)} (X - \\mathbb{E}[X])$$\nProblem:\nProve that $$\\mathbb{E}[X_2 | \\sigma(X_1)] = \\mu_2 + \\frac{\\mathrm{cov}(X_2, X_1)}{\\mathrm{var}(X_1)} (X_1 - \\mu_1)$$ without integrating the conditional density."
}