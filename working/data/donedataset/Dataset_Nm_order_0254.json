{
    "Link_API": "https://api.stackexchange.com/2.3/questions/114072",
    "Title": "What is the proof that covariance matrices are always semi-definite?",
    "Score": 44,
    "Category": "Chứng minh",
    "Tags": "probability, matrices, vector-spaces, proof-writing, positive-semidefinite",
    "Link": "https://math.stackexchange.com/questions/114072/what-is-the-proof-that-covariance-matrices-are-always-semi-definite",
    "Content": "Suppose that we have two different discreet signal vectors of $N^\\text{th}$ dimension, namely $\\mathbf{x}[i]$ and $\\mathbf{y}[i]$, each one having a total of $M$ set of samples/vectors. $\\mathbf{x}[m] = [x_{m,1} \\,\\,\\,\\,\\, x_{m,2} \\,\\,\\,\\,\\, x_{m,3} \\,\\,\\,\\,\\, ... \\,\\,\\,\\,\\, x_{m,N}]^\\text{T}; \\,\\,\\,\\,\\,\\,\\, 1 \\leq m \\leq M$ $\\mathbf{y}[m] = [y_{m,1} \\,\\,\\,\\,\\, y_{m,2} \\,\\,\\,\\,\\, y_{m,3} \\,\\,\\,\\,\\, ... \\,\\,\\,\\,\\, y_{m,N}]^\\text{T}; \\,\\,\\,\\,\\,\\,\\,\\,\\, 1 \\leq m \\leq M$ And, I build up a covariance matrix in-between these signals. $\\{C\\}_{ij} = E\\left\\{(\\mathbf{x}[i] - \\bar{\\mathbf{x}}[i])^\\text{T}(\\mathbf{y}[j] - \\bar{\\mathbf{y}}[j])\\right\\}; \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, 1 \\leq i,j \\leq M $ Where, $E\\{\\}$ is the \"expected value\" operator. What is the proof that, for all arbitrary values of $\\mathbf{x}$ and $\\mathbf{y}$ vector sets, the covariance matrix $C$ is always semi-definite ($C \\succeq0$) (i.e.; not negative definte; all of its eigenvalues are non-negative)?",
    "AcceptedAnswer": "A symmetric matrix $C$ of size $n\\times n$ is semi-definite if and only if $u^tCu\\geqslant0$ for every $n\\times1$ (column) vector $u$, where $u^t$ is the $1\\times n$ transposed (line) vector. If $C$ is a covariance matrix in the sense that $C=\\mathrm E(XX^t)$ for some $n\\times 1$ random vector $X$, then the linearity of the expectation yields that $u^tCu=\\mathrm E(Z_u^2)$, where $Z_u=u^tX$ is a real valued random variable, in particular $u^tCu\\geqslant0$ for every $u$. If $C=\\mathrm E(XY^t)$ for two centered random vectors $X$ and $Y$, then $u^tCu=\\mathrm E(Z_uT_u)$ where $Z_u=u^tX$ and $T_u=u^tY$ are two real valued centered random variables. Thus, there is no reason to expect that $u^tCu\\geqslant0$ for every $u$ (and, indeed, $Y=-X$ provides a counterexample).",
    "llm_answer_create_structured_problem": "###BEGIN_OF_FORMAT###\nAssumption: \n$\\mathbf{x}$ is a discrete signal vector of $N^\\text{th}$ dimension,\n$\\mathbf{y}$ is a discrete signal vector of $N^\\text{th}$ dimension,\nEach signal has $M$ samples,\nFor each $1 \\leq m \\leq M$, $\\mathbf{x}[m]$ is an $N$-dimensional vector,\nFor each $1 \\leq m \\leq M$, $\\mathbf{y}[m]$ is an $N$-dimensional vector,\nThe covariance matrix $C$ is an $M \\times M$ matrix,\nEach entry $\\{C\\}_{ij} = E\\left\\{(\\mathbf{x}[i] - \\bar{\\mathbf{x}}[i])^\\text{T}(\\mathbf{y}[j] - \\bar{\\mathbf{y}}[j])\\right\\}$,\nThe expectation operator $E$ is applied,\nThe vectors $\\mathbf{x}$ and $\\mathbf{y}$ are arbitrary.\nProblem: \nProve that the covariance matrix $C$ is always semi-definite ($C \\succeq 0$) for arbitrary $\\mathbf{x}$ and $\\mathbf{y}$ vector sets.\n###",
    "reasoning_create_structured_problem": "Okay, let me try to figure out how to structure this problem properly. The user wants me to act as a math professor and format the given Math Stack Exchange question into their specified format. The example they provided shows breaking down the assumptions into individual points and then stating the problem clearly.\n\nFirst, I need to parse the original question. The title is asking for a proof that covariance matrices are always semi-definite. The question body goes into more detail: they have two discrete signal vectors x and y, each of Nth dimension with M samples. They define the covariance matrix C with entries C_ij as the expected value of (x[i] - mean(x[i]))^T (y[j] - mean(y[j])). Then they ask for the proof that C is always positive semi-definite for any x and y.\n\nNow, I need to extract the assumptions. Let's list each assumption line by line. Each line should address one property or condition of a single object. \n\nAssumptions:\n1. x and y are discrete signal vectors of Nth dimension.\n2. Each signal vector has M samples (so M vectors each for x and y).\n3. The covariance matrix C is defined as C_ij = E[(x[i] - mean(x[i]))^T (y[j] - mean(y[j]))].\n4. The expected value operator E is used.\n5. The problem applies to arbitrary x and y vectors.\n\nWait, but in the covariance definition, usually covariance is between variables, but here the user might be referring to cross-covariance between two vectors. Also, the standard covariance matrix for a single vector is always PSD, but here they're considering cross-covariance between x and y. Wait, but actually, in the standard case, a covariance matrix (auto-covariance) is PSD because it's E[(X - μ)(X - μ)^T], which is PSD. But here, the user is talking about cross-covariance between x and y. So C_ij here is the covariance between x[i] and y[j], which might not be symmetric. Wait, but if that's the case, the matrix C might not even be symmetric, so it's not necessarily PSD. However, maybe the user is conflating cross-covariance with the standard covariance matrix. Alternatively, maybe there's a misunderstanding in their setup.\n\nWait, the problem states \"covariance matrix in-between these signals\" with C_ij defined as E[(x[i] - mean(x[i]))^T (y[j] - mean(y[j]))]. But if x and y are vectors, then (x[i] - mean(x[i])) is a vector, and the transpose times (y[j] - mean(y[j])) would be a scalar, so each entry C_ij is a scalar. Wait, but that would make C an MxM matrix where each entry is the covariance between the i-th sample of x and j-th sample of y. Hmm, but that seems a bit off. Normally, covariance matrices are NxN if each vector is of size N. But here, M is the number of samples, each sample is a vector of size N. So the standard covariance matrix would be NxN, computed across the M samples. But the user is defining C as MxM, where each entry is the covariance between the i-th and j-th samples? That seems non-standard. Maybe there's confusion in the setup here.\n\nBut regardless, the user is asking for the proof that C is PSD. However, if C is defined as cross-covariance between different signals, and if C is not symmetric, then it might not be PSD. So perhaps the user made a mistake in their setup. But given the problem as stated, I have to structure the assumptions as given.\n\nWait, the problem says \"covariance matrix in-between these signals\", and defines C_ij as E[(x[i] - mean(x[i]))^T (y[j] - mean(y[j]))]. Let me parse this again. Each x[i] and y[j] are vectors. So (x[i] - mean(x[i])) is a vector, and (y[j] - mean(y[j])) is another vector. Then the transpose of the first times the second is their inner product, which is a scalar. Then the expectation of that scalar is C_ij. So C is an MxM matrix where each entry is the covariance between the i-th vector of x and the j-th vector of y. But in standard terms, covariance matrices are between variables, not between samples. So if you have M samples of N-dimensional vectors, the covariance matrix is NxN, computed as the expectation of (x - μ)(x - μ)^T. But here, the user is arranging it as MxM, which is unusual. Therefore, perhaps the user has a different setup, possibly mixing up the dimensions.\n\nBut regardless, the problem as posed is to prove that this matrix C is PSD. However, given that C_ij is defined as the covariance between x[i] and y[j], if x and y are different signals, then C may not be symmetric, hence not necessarily PSD. Therefore, there might be a mistake in the problem's formulation, but since we have to take the user's question as given, we proceed.\n\nBut maybe the user intended to have a single random vector and compute its covariance matrix, which is standard. Alternatively, perhaps x and y are the same, but that's not stated. Alternatively, maybe the cross-covariance between x and y can be represented as a PSD matrix, but I'm not sure. However, given the user's question, we need to structure their assumptions as per their problem statement.\n\nSo, extracting assumptions:\n\n1. x[i] and y[i] are discrete signal vectors of Nth dimension.\n2. Each has M samples (so M vectors for x and M vectors for y).\n3. The covariance matrix C is defined with entries C_ij = E[(x[i] - mean(x[i]))^T (y[j] - mean(y[j]))].\n4. The expected value operator is used.\n5. The problem states that this holds for arbitrary x and y vectors.\n\nBut wait, the user's question says \"for all arbitrary values of x and y vector sets\", so C is built from these x and y, and they want to prove C is always PSD.\n\nHowever, given that in the standard case, a covariance matrix (auto-covariance) is PSD, but cross-covariance between two different vectors might not be. So perhaps the user is mistaken in their setup. However, perhaps they intended to use the same vector, i.e., x = y, but the question states \"two different discrete signal vectors\". Alternatively, maybe they are considering the covariance matrix of a single random vector, but structured in a different way.\n\nBut regardless, the task is to format their question into the specified structure. So, the assumptions should list each condition they mention. Let's break them down:\n\nAssumptions:\n- x and y are discrete signal vectors of Nth dimension.\n- Each has M samples (M vectors each).\n- The covariance matrix C is defined by C_ij = E[(x[i] - mean(x[i]))^T (y[j] - mean(y[j]))].\n- The expectation operator E is used.\n- The vectors x and y are arbitrary.\n\nWait, but in their problem statement, they might have meant that x and y are random vectors, and each sample m (from 1 to M) is an observation. So, perhaps the setup is that for each m, x[m] and y[m] are observations, and the covariance is computed across these samples. But the way they defined C_ij is a bit unclear. Normally, covariance between variables i and j would be across the samples, but here, the indices i and j are up to M, which is the number of samples. This is confusing because usually, the number of variables is N and the samples are M. So maybe the user has mixed indices. But given that, the problem as stated is to prove that this MxM matrix C is PSD for any x and y.\n\nAlternatively, maybe they meant to define the covariance matrix between the variables, not between the samples. For example, if you have N variables with M samples each, then the covariance matrix is NxN. But the way they wrote C_ij as E[(x[i] - mean(x[i]))^T (y[j] - mean(y[j]))] is a bit confusing. If x and y are vectors of variables, then x[i] would be the i-th variable's vector across samples. Wait, but in their definition, x[m] is a vector of size N for each m. So x is a collection of M samples, each sample being an N-dimensional vector. Then, to compute the covariance matrix, you would typically have an NxN matrix where each entry (i,j) is the covariance between the i-th and j-th components across the M samples. But the user is instead creating an MxM matrix where each entry (i,j) is the covariance between the i-th sample of x and the j-th sample of y. This is non-standard and might not necessarily be PSD.\n\nHowever, the user's question is as stated, so we need to extract assumptions based on their description.\n\nBreaking down the assumptions line by line:\n\n1. x and y are discrete signal vectors of Nth dimension. (Each signal vector is N-dimensional)\n2. Each signal has M samples. (So M instances of x and y vectors)\n3. The covariance matrix C is MxM with entries C_ij = E[(x[i] - mean(x[i]))^T (y[j] - mean(y[j]))].\n4. The expectation operator E is used.\n5. The vectors x and y are arbitrary.\n\nWait, but the problem says \"for all arbitrary values of x and y vector sets\", meaning that x and y can be any vectors, not necessarily random variables. However, covariance typically involves centered random variables. If x and y are deterministic vectors, then the covariance would be zero unless there's some probabilistic structure. This might indicate confusion in the problem statement. But again, we take it as given.\n\nAlternatively, maybe the user is considering the sample covariance matrix. If x and y are centered (mean subtracted), then the covariance matrix would be (1/(M-1)) X^T Y, where X and Y are data matrices. But if X and Y are N x M matrices (each column is a sample), then X^T Y would be M x M, and this matrix can be either PSD or not depending on X and Y. However, if X = Y, then X^T X is PSD. But if X and Y are different, X^T Y is not necessarily PSD.\n\nBut the problem states \"covariance matrices are always semi-definite\", which is generally true for auto-covariance matrices (when Y = X), but not necessarily for cross-covariance. Therefore, there might be a mistake in the problem's premise. But the user is asking for the proof regardless, so we proceed.\n\nAssuming that the user intended the auto-covariance case (i.e., y = x), but they mentioned two different signals. Alternatively, perhaps the cross-covariance as defined is not necessarily PSD, but the user thinks it is. However, the problem as posed requires structuring based on their description.\n\nTherefore, the assumptions would be:\n\nAssumption:\n- x and y are discrete signal vectors of Nth dimension.\n- Each signal has M samples.\n- The covariance matrix C is defined by C_ij = E[(x[i] - mean(x[i]))^T (y[j] - mean(y[j]))].\n- The expectation operator E is used.\n- The problem considers arbitrary x and y vector sets.\n\nWait, but each of these needs to be one line per assumption, each addressing one property. Let's list them:\n\nAssumption:\nEach signal vector x and y has N dimensions.\nEach signal has M samples (vectors).\nFor each 1 ≤ m ≤ M, x[m] is an N-dimensional vector.\nFor each 1 ≤ m ≤ M, y[m] is an N-dimensional vector.\nThe covariance matrix C is an M×M matrix.\nEach entry C_ij is defined as the expected value of (x[i] - mean(x[i]))^T (y[j] - mean(y[j])).\nThe expectation operator E is applied.\nThe vectors x and y are arbitrary.\n\nBut perhaps some of these can be combined or split. Let's see:\n\nOriginal question says:\n\n\"Suppose that we have two different discreet signal vectors of N^th dimension, namely x[i] and y[i], each one having a total of M set of samples/vectors. x[m] = [x_{m,1} ... x_{m,N}]^T; 1 ≤ m ≤ M; y[m] = [y_{m,1} ... y_{m,N}]^T; 1 ≤ m ≤ M.\"\n\nSo assumptions:\n\n1. x and y are two different discrete signal vectors.\n2. Each signal vector has N dimensions.\n3. Each signal has M samples (vectors).\n4. For each 1 ≤ m ≤ M, x[m] is an N-dimensional vector: [x_{m,1}, ..., x_{m,N}]^T.\n5. Similarly for y[m].\n6. The covariance matrix C is M×M.\n7. Each entry C_ij = E{(x[i] - mean(x[i]))^T (y[j] - mean(y[j]))}.\n8. The expectation operator E is used.\n9. The problem applies to arbitrary x and y vectors.\n\nBut the key points here are the definitions of x, y, the covariance matrix C's entries, and the arbitrariness of x and y.\n\nHowever, the problem as stated might have a mistake because if x and y are different, C might not be PSD. But the user is asking for the proof that it is always PSD. Therefore, the formatted problem should capture the user's exact question, even if there's a potential mistake in the setup.\n\nSo structuring the assumptions:\n\nAssumption:\nx and y are discrete signal vectors of Nth dimension.\nEach signal has M samples.\nx[m] is an N-dimensional vector for each 1 ≤ m ≤ M.\ny[m] is an N-dimensional vector for each 1 ≤ m ≤ M.\nThe covariance matrix C is M×M.\nEach entry C_ij = E[(x[i] - E[x[i]])^T (y[j] - E[y[j]])}.\nThe expectation operator E is used.\nThe vectors x and y are arbitrary.\n\nBut need to check if each assumption is a single property. For example, \"x and y are discrete signal vectors of Nth dimension\" combines two objects (x and y) into one line. According to the example given, each assumption should address only one object. So perhaps split into:\n\nAssumption:\nx is a discrete signal vector of Nth dimension.\ny is a discrete signal vector of Nth dimension.\nEach signal has M samples.\nFor each 1 ≤ m ≤ M, x[m] is an N-dimensional vector.\nFor each 1 ≤ m ≤ M, y[m] is an N-dimensional vector.\nThe covariance matrix C is an M×M matrix.\nEach entry C_ij is defined as E[(x[i] - E[x[i]])^T (y[j] - E[y[j]])}.\nThe expectation operator E is applied.\nThe vectors x and y are arbitrary.\n\nYes, this breaks down each assumption into individual lines, each addressing one property of a single object.\n\nNow, the problem statement is: What is the proof that covariance matrices are always semi-definite?\n\nBut according to the user's specific setup, the problem is to prove that for all arbitrary x and y, the covariance matrix C as defined is PSD.\n\nThus, the formatted problem should be:\n\nProblem: Prove that the covariance matrix $C$ defined by $\\{C\\}_{ij} = E\\left\\{(\\mathbf{x}[i] - \\bar{\\mathbf{x}}[i])^\\text{T}(\\mathbf{y}[j] - \\bar{\\mathbf{y}}[j])\\right\\}$ is always semi-definite ($C \\succeq 0$) for arbitrary $\\mathbf{x}$ and $\\mathbf{y}$ vector sets.\n\nBut wait, in the user's original question, they mention \"covariance matrices\" in the title, but their specific matrix C is defined between two different signals. However, typically, a covariance matrix is for a single signal, and cross-covariance between two signals can be defined but isn't necessarily PSD. Therefore, the problem as posed might have an error, but we have to structure it as per their description.\n\nTherefore, the final answer should list all the assumptions line by line and then the problem statement.",
    "Original_Problem": "Assumption: \n$\\mathbf{x}$ is a discrete signal vector of $N^\\text{th}$ dimension,\n$\\mathbf{y}$ is a discrete signal vector of $N^\\text{th}$ dimension,\nEach signal has $M$ samples,\nFor each $1 \\leq m \\leq M$, $\\mathbf{x}[m]$ is an $N$-dimensional vector,\nFor each $1 \\leq m \\leq M$, $\\mathbf{y}[m]$ is an $N$-dimensional vector,\nThe covariance matrix $C$ is an $M \\times M$ matrix,\nEach entry $\\{C\\}_{ij} = E\\left\\{(\\mathbf{x}[i] - \\bar{\\mathbf{x}}[i])^\\text{T}(\\mathbf{y}[j] - \\bar{\\mathbf{y}}[j])\\right\\}$,\nThe expectation operator $E$ is applied,\nThe vectors $\\mathbf{x}$ and $\\mathbf{y}$ are arbitrary.\nProblem: \nProve that the covariance matrix $C$ is always semi-definite ($C \\succeq 0$) for arbitrary $\\mathbf{x}$ and $\\mathbf{y}$ vector sets.",
    "Original_Problem_with_numerical_assumption": "Assumption:\nAssumption 1: $\\mathbf{x}$ is a discrete signal vector of $N^\\text{th}$ dimension,\nAssumption 2: $\\mathbf{y}$ is a discrete signal vector of $N^\\text{th}$ dimension,\nAssumption 3: Each signal has $M$ samples,\nAssumption 4: For each $1 \\leq m \\leq M$, $\\mathbf{x}[m]$ is an $N$-dimensional vector,\nAssumption 5: For each $1 \\leq m \\leq M$, $\\mathbf{y}[m]$ is an $N$-dimensional vector,\nAssumption 6: The covariance matrix $C$ is an $M \\times M$ matrix,\nAssumption 7: Each entry $\\{C\\}_{ij} = E\\left\\{(\\mathbf{x}[i] - \\bar{\\mathbf{x}}[i])^\\text{T}(\\mathbf{y}[j] - \\bar{\\mathbf{y}}[j])\\right\\}$,\nAssumption 8: The expectation operator $E$ is applied,\nAssumption 9: The vectors $\\mathbf{x}$ and $\\mathbf{y}$ are arbitrary.\nProblem: \nProve that the covariance matrix $C$ is always semi-definite ($C \\succeq 0$) for arbitrary $\\mathbf{x}$ and $\\mathbf{y}$ vector sets.",
    "Proof_problem": "Yes",
    "Redundant_assumption": "$C=\\mathrm E(XX^t)$",
    "Problem_with_redundant_assumption": "Assumption:\nAssumption 1: $\\mathbf{x}$ is a discrete signal vector of $N^\\text{th}$ dimension,\nAssumption 2: $\\mathbf{y}$ is a discrete signal vector of $N^\\text{th}$ dimension,\nAssumption 3: Each signal has $M$ samples,\nAssumption 4: For each $1 \\leq m \\leq M$, $\\mathbf{x}[m]$ is an $N$-dimensional vector,\nAssumption 5: For each $1 \\leq m \\leq M$, $\\mathbf{y}[m]$ is an $N$-dimensional vector,\nAssumption 6: The covariance matrix $C$ is an $M \\times M$ matrix,\nAssumption 7: Each entry $\\{C\\}_{ij} = E\\left\\{(\\mathbf{x}[i] - \\bar{\\mathbf{x}}[i])^\\text{T}(\\mathbf{y}[j] - \\bar{\\mathbf{y}}[j])\\right\\}$,\nAssumption 8: The expectation operator $E$ is applied,\nAssumption 9: The vectors $\\mathbf{x}$ and $\\mathbf{y}$ are arbitrary.\nAssumption 10: $C=\\mathrm E(XX^t)$\nProblem: \nProve that the covariance matrix $C$ is always semi-definite ($C \\succeq 0$) for arbitrary $\\mathbf{x}$ and $\\mathbf{y}$ vector sets."
}