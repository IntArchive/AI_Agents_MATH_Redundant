{
    "Link_API": "https://api.stackexchange.com/2.3/questions/1747559",
    "Title": "image of adjoint equals orthogonal complement of kernel",
    "Score": 23,
    "Category": "Chứng minh",
    "Tags": "linear-algebra, adjoint-operators",
    "Link": "https://math.stackexchange.com/questions/1747559/image-of-adjoint-equals-orthogonal-complement-of-kernel",
    "Content": "Let $T:V\\to W$ be a linear map of finite-dimensional spaces.  Then\n$${\\rm im}(T^{\\textstyle*})=({\\rm ker}\\,T)^\\perp\\ .\\tag{$*$}$$\nI can prove this as follows:\n$${\\rm ker}(T^{\\textstyle*})=({\\rm im}\\,T)^\\perp\\tag{$**$}$$\nis quite easy, and we know $T^{\\textstyle*}{}^{\\textstyle*}=T$ and $W^{\\perp\\perp}=W$, so\n$${\\rm im}(T^{\\textstyle*})=({\\rm im}\\,T^{\\textstyle*})^{\\perp\\perp}=({\\rm ker}\\,T^{\\textstyle*}{}^{\\textstyle*})^\\perp=({\\rm ker}\\,T)^\\perp\\ .$$\nHowever, I would be interested in a \"direct\" proof of $(*)$.  It's fairly easy to show ${\\rm LHS}\\subseteq{\\rm RHS}$.  For the converse I have tried obvious things but seem to be going round in circles. Also, any insights as to why $(**)$ is harder than $(*)$ - if in fact it is :) Edit .  To clarify, I am considering the adjoint defined in terms of an inner product,\n$$\\langle\\,T({\\bf v})\\mid{\\bf w}\\,\\rangle\n  =\\langle\\,{\\bf v}\\mid T^{\\textstyle*}({\\bf w})\\,\\rangle\\ .$$",
    "AcceptedAnswer": "Let ${\\bf v}\\in{\\rm im}(T^\\ast)$, then ${\\bf v}=T^\\ast({\\bf w})$ for some ${\\bf w}\\in W$. Now, given ${\\bf u}\\in\\ker T$, we see that $T({\\bf u})={\\bf 0}$ and therefore\n$$\\langle {\\bf u}\\mid{\\bf v}\\rangle\n=\\langle {\\bf u}\\mid T^\\ast({\\bf w})\\rangle\n=\\langle T({\\bf u})\\mid {\\bf w}\\rangle\n=\\langle {\\bf 0}\\mid{\\bf w}\\rangle\n=0.$$\nThat is, ${\\bf v}\\in(\\ker T)^\\perp$. Conversely, if ${\\bf v}\\notin{\\rm im}(T^\\ast)$, then there exists an ${\\bf v}'\\in{\\rm im}(T^\\ast)^\\perp$ such that $\\langle {\\bf v}\\mid{\\bf v}'\\rangle\\ne 0$. In fact, we have ${\\bf v}'\\in\\ker T$ because $T^\\ast T({\\bf v}')\\in{\\rm im}(T^\\ast)$, which implies\n$$\\langle T({\\bf v}')\\mid T({\\bf v}')\\rangle\n=\\langle {\\bf v}'\\mid T^\\ast T({\\bf v}')\\rangle=0\\quad\\Longrightarrow\\quad\nT({\\bf v}')={\\bf 0}.$$\nTherefore ${\\bf v}\\notin(\\ker T)^\\perp$, which completes the proof.",
    "llm_answer_create_structured_problem": "###BEGIN_OF_FORMAT###\nAssumption: \n$T$ is a linear map from $V$ to $W$,\n$V$ is a finite-dimensional vector space,\n$W$ is a finite-dimensional vector space,\nThe adjoint operator $T^*$ satisfies $\\langle T(\\mathbf{v}), \\mathbf{w} \\rangle = \\langle \\mathbf{v}, T^*(\\mathbf{w}) \\rangle$ for all $\\mathbf{v} \\in V$ and $\\mathbf{w} \\in W$.\nProblem:  \nProve that $\\operatorname{im}(T^*) = (\\ker T)^\\perp$.\n###",
    "reasoning_create_structured_problem": "Okay, let me try to parse this Math Stack Exchange question. The user wants to prove that the image of the adjoint operator equals the orthogonal complement of the kernel of the original operator. The problem is about linear maps between finite-dimensional spaces. \n\nFirst, I need to extract the assumptions. The question mentions that T is a linear map between finite-dimensional vector spaces V and W. The adjoint T* is defined using an inner product, so that the inner product of T(v) and w equals the inner product of v and T*(w). \n\nThe user already knows a proof using the double dual and the fact that (**) is easier, which is ker(T*) = (im T)⊥. Then they use duality to get the result. But they want a direct proof for im(T*) = (ker T)⊥. \n\nSo the key assumptions here are: T is a linear map between finite-dimensional spaces, the adjoint is defined via the given inner product relation, and the spaces are finite-dimensional (which is important because in infinite dimensions things can get trickier with closures and such). \n\nI need to list each assumption line by line. Let me check:\n\n1. T is a linear map from V to W.\n2. V and W are finite-dimensional vector spaces.\n3. The adjoint T* is defined by ⟨T(v), w⟩ = ⟨v, T*(w)⟩ for all v in V, w in W.\n4. The problem is to show that image of T* is equal to the orthogonal complement of kernel of T.\n\nWait, the problem mentions \"image of adjoint equals orthogonal complement of kernel\". So the problem is to prove im(T*) = (ker T)⊥. \n\nSo in the assumptions, the user has already considered the adjoint defined via inner product. Also, finite-dimensionality is crucial here because in infinite dimensions, the orthogonal complement of the kernel might not coincide with the image of the adjoint without closure. But since the problem specifies finite-dimensional spaces, that's an assumption to list.\n\nSo each assumption per line:\n\n- T: V → W is a linear map,\n- V and W are finite-dimensional vector spaces,\n- The adjoint operator T* is defined by ⟨T(v), w⟩ = ⟨v, T*(w)⟩ for all v ∈ V and w ∈ W.\n\nThen the problem is to prove that the image of T* is equal to the orthogonal complement of the kernel of T, i.e., im(T*) = (ker T)⊥.\n\nI need to make sure each assumption is a single property or condition applied to one object. Let's see:\n\nFirst line: T is a linear map from V to W. That's one property applied to T.\n\nSecond line: V and W are finite-dimensional. That's a condition on both V and W. Wait, the format says \"applied to one object\". Maybe split into two? Or is it okay as a single line because they are both spaces in the problem? The example given in the user's example mentions \"n is an integer, n is divisible by 7\" as two separate lines. So perhaps \"V is a finite-dimensional vector space\" and \"W is a finite-dimensional vector space\" should be two lines. But the original problem says \"finite-dimensional spaces\" in the problem statement, so maybe combining them is okay. Hmm. Let me check the example again. In the example, the user had \"n is an integer\" and \"n is divisible by 7\" as separate assumptions, even though both apply to n. So here, since the finite-dimensionality applies to both V and W, maybe each should be a separate assumption. However, the original problem statement says \"finite-dimensional spaces\", so perhaps the user intended that both V and W are finite-dimensional. So perhaps two separate lines:\n\n- V is a finite-dimensional vector space,\n- W is a finite-dimensional vector space.\n\nAlternatively, maybe the problem considers both V and W as finite-dimensional, so we can state that as one assumption. But according to the example, each assumption should address only one property or condition applied to one object. So perhaps split into:\n\n- V is a finite-dimensional vector space,\n- W is a finite-dimensional vector space.\n\nThird assumption: The adjoint operator T* is defined by the given inner product relation. That's another condition on T*.\n\nSo putting it all together:\n\nAssumption:\n- T is a linear map from V to W,\n- V is a finite-dimensional vector space,\n- W is a finite-dimensional vector space,\n- The adjoint operator T* satisfies ⟨T(v), w⟩ = ⟨v, T*(w)⟩ for all v ∈ V and w ∈ W.\n\nProblem: Prove that im(T*) = (ker T)⊥.\n\nWait, but in the problem statement, the user wrote \"image of adjoint equals orthogonal complement of kernel\". So \"im(T*) = (ker T)⊥\". That's the problem.\n\nNow, check if there's anything else. The user mentioned that they can prove LHS subset RHS easily, but the converse is harder. But for the purpose of structuring the problem, that's part of their attempt, not part of the problem assumptions. So we don't include that.\n\nTherefore, the assumptions are as above, and the problem is to prove the equality.",
    "Original_Problem": "Assumption: \n$T$ is a linear map from $V$ to $W$,\n$V$ is a finite-dimensional vector space,\n$W$ is a finite-dimensional vector space,\nThe adjoint operator $T^*$ satisfies $\\langle T(\\mathbf{v}), \\mathbf{w} \\rangle = \\langle \\mathbf{v}, T^*(\\mathbf{w}) \\rangle$ for all $\\mathbf{v} \\in V$ and $\\mathbf{w} \\in W$.\nProblem:  \nProve that $\\operatorname{im}(T^*) = (\\ker T)^\\perp$.",
    "Original_Problem_with_numerical_assumption": "Assumption:\nAssumption 1: $T$ is a linear map from $V$ to $W$,\nAssumption 2: $V$ is a finite-dimensional vector space,\nAssumption 3: $W$ is a finite-dimensional vector space,\nAssumption 4: The adjoint operator $T^*$ satisfies $\\langle T(\\mathbf{v}), \\mathbf{w} \\rangle = \\langle \\mathbf{v}, T^*(\\mathbf{w}) \\rangle$ for all $\\mathbf{v} \\in V$ and $\\mathbf{w} \\in W$.\nProblem:  \nProve that $\\operatorname{im}(T^*) = (\\ker T)^\\perp$.",
    "Proof_problem": "Yes",
    "Redundant_assumption": "$$\\langle {\\bf u}\\mid{\\bf v}\\rangle\n=\\langle {\\bf u}\\mid T^\\ast({\\bf w})\\rangle\n=\\langle T({\\bf u})\\mid {\\bf w}\\rangle\n=\\langle {\\bf 0}\\mid{\\bf w}\\rangle\n=0.$$",
    "Problem_with_redundant_assumption": "Assumption:\nAssumption 1: $T$ is a linear map from $V$ to $W$,\nAssumption 2: $V$ is a finite-dimensional vector space,\nAssumption 3: $W$ is a finite-dimensional vector space,\nAssumption 4: The adjoint operator $T^*$ satisfies $\\langle T(\\mathbf{v}), \\mathbf{w} \\rangle = \\langle \\mathbf{v}, T^*(\\mathbf{w}) \\rangle$ for all $\\mathbf{v} \\in V$ and $\\mathbf{w} \\in W$.\nAssumption 5: $$\\langle {\\bf u}\\mid{\\bf v}\\rangle\n=\\langle {\\bf u}\\mid T^\\ast({\\bf w})\\rangle\n=\\langle T({\\bf u})\\mid {\\bf w}\\rangle\n=\\langle {\\bf 0}\\mid{\\bf w}\\rangle\n=0.$$\nProblem:  \nProve that $\\operatorname{im}(T^*) = (\\ker T)^\\perp$."
}