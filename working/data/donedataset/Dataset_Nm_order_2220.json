{
    "Link_API": "https://api.stackexchange.com/2.3/questions/2766934",
    "Title": "Simple proof that if $A^n=I$ then $\\mathrm{tr}(A^{-1})=\\overline{\\mathrm{tr}(A)}$",
    "Score": 15,
    "Category": "Chứng minh",
    "Tags": "linear-algebra, matrices, representation-theory, linear-transformations",
    "Link": "https://math.stackexchange.com/questions/2766934/simple-proof-that-if-an-i-then-mathrmtra-1-overline-mathrmtra",
    "Content": "Let $A$ be a linear map from a finite dimensional complex vector space to itself. If $A$ has finite order then the trace of its inverse is the conjugate of its trace. I know two proofs of this fact, but they both require linear algebra facts whose proofs are themselves quite involved. Since $A^n=I$, the eigenvalues of $A$ are roots of unity. Hence they have unit norm, and so their reciprocals are their conjugates. Then the result follows from following facts: (a) The eigenvalues of $A^{-1}$ are the reciprocals of the eigenvalues of $A$, (b) the dimensions of the eigenspaces of $A^{-1}$ are equal to the dimensions of the corresponding eigenspaces of $A$, (c) the trace is equal to the sum of the (generalised) eigenvalues. The proof of (a) is relatively easy, but (b) and (c) seem to require the existence of Jordan Normal Form, which requires a lot of work. By Weyl's Unitary Trick , there's a inner product for which $A$ is unitary (this proof is itself a fair amount of work). So in an orthonormal basis (which we must construct with the Gram-Schmidt procedure) the inverse of $A$ is given by its conjugate transpose (one must also prove this). So the trace of the inverse is the conjugate of the trace. Since  the condition $A^n=I$ and the consequence $\\mathrm{tr}(A^{-1})=\\overline{\\mathrm{tr}(A)}$ are both elementary statements, I'm wondering if there's a short proof from first principles (ideally without quoting any big linear algebra Theorems). Can anyone find one?",
    "AcceptedAnswer": "Since the question attracted quickly four votes, i'll try to use minimal known linear algebra to get the result. (One more comment. Since the complex conjugation is involved, there is no purely algebraic proof, e.g. one that uses polynomial/functional calculus in $A$.) We start with $A$ a matrix, an endomorphism of a vector space $V$ of finite dimension $\\ge 1$ over $\\Bbb C$, such that for a suitable natural $n$ we have $$A^n=I\\ .$$ Let $v\\ne 0$ be a vector in $V$. The sequence $v, Av, A^2 v,\\dots A^nv=v, \\dots$ is periodic. Let $d$ be its period, $d$ is a divisor of $n$. If $d=1$ we record this $v$, set $w=v$. Else, let $\\xi$ be a primitive $d$-root of unity in $\\Bbb C$, e.g. $\\xi=\\exp\\frac {2\\pi\\, i}d$ if we want to fix the ideas (and leave algebra). Consider the following vectors in $V$:\n$$\n\\begin{aligned}\nw_0 &=v +Av+\\dots+A^{d-1}v\\ ,\\\\\nw_1 &=v +\\xi Av+\\dots+(\\xi A)^{d-1}v\\ ,\\\\\n\\ \\vdots\\ \\vdots\\  &\\qquad \\vdots\\qquad\\vdots\\qquad\\vdots\\qquad\\vdots\\qquad\\vdots\\qquad\\vdots\\qquad\\vdots\\qquad\\\\\nw_k &=v +(\\xi^k A)v+\\dots+(\\xi^k A)^{d-1}v\\ ,\\\\\n\\ \\vdots\\ \\vdots\\  &\\qquad \\vdots\\qquad\\vdots\\qquad\\vdots\\qquad\\vdots\\qquad\\vdots\\qquad\\vdots\\qquad\\vdots\\qquad\\\\\nw_{d-1} &=v +(\\xi^{d-1} A)v+\\dots+(\\xi^{d-1} A)^{d-1}v\\ .\n\\end{aligned}\n$$\nIf at least one of these vectors is $\\ne 0$, then we record it, and set $w$ to be one choice among them. Else?! Else we have the situation, which is formally described by the following relation:\n$$\n\\underbrace{\n\\begin{bmatrix}\n1 & 1 & 1 & \\dots & 1\\\\\n1 & \\xi & \\xi^2 &\\dots & \\xi^{d-1}\\\\\n1 & \\xi^2 & \\xi^4 &\\dots  & \\xi^{2(d-1)}\\\\\n\\vdots &\\vdots &\\vdots &\\ddots &\\vdots\\\\\n1 & \\xi^{d-1} & \\xi^{2(d-1)} &\\dots  & \\xi^{(d-1)(d-1)}\n\\end{bmatrix}\n}_{\\text{Vandermonde}(1,\\xi,\\dots,\\xi^{d-1})}\n%\n\\begin{bmatrix}\nv \\\\ Av\\\\ A^2 v\\\\\\vdots\\\\A^{d-1}v\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\ 0\\\\ 0\\\\\\vdots\\\\0\n\\end{bmatrix}\n\\ .\n$$\nThe Vandermonde matrix is invertible, so we formally multiply from left with its inverse. To be exact, this is reflected then in building linear combinations in the given formulas for $w_0,w_1,\\dots,w_{d-1}$ to isolate $0=v=Av=\\dots$ , which gives a contradiction. Doing this we have constructed a $w$ such that $Aw=\\xi^? w$ for a root of unit $\\xi^?$. We consider $V'$, the quotient space or some subspace of $V$ generated by \"the other\" vectors that extend the linear independent system $\\{w\\}$ to a basis, and consider the same problem with the induced / restricted $A$ on $V'$. Inductively we get a basis of $V$ on which $A$ acts diagonally (or upper triangular if taking quotients), and the elements on the diagonal are $\\xi_1,\\xi_2,\\dots$ all of them roots of unit. Then the inverse matrix has the same shape with diagonal $\\xi_1^{-1},\\xi_2^{-1},\\dots$ and the equality involving traces can be equivalently traced back to:\n$$\n\\frac 1{\\xi_1}+\n\\frac 1{\\xi_2}+\n\\dots \n=\n\\overline{\\xi_1}+\n\\overline{\\xi_2}+\n\\dots \n$$\nwhich is true.",
    "llm_answer_create_structured_problem": "###BEGIN_OF_FORMAT###\nAssumption: \n$A$ is a linear map from a finite-dimensional complex vector space to itself,\n$A^n = I$ for some integer $n$,\nThe trace of a matrix is the sum of its eigenvalues (counted with algebraic multiplicity),\nThe eigenvalues of $A^{-1}$ are the reciprocals of the eigenvalues of $A$,\nAll eigenvalues of $A$ are roots of unity (since $A^n = I$),\nFor any root of unity $\\lambda$, $\\lambda^{-1} = \\overline{\\lambda}$.\nProblem: \nProve that $\\mathrm{tr}(A^{-1}) = \\overline{\\mathrm{tr}(A)}$.\n###END_OF_FORMAT###",
    "reasoning_create_structured_problem": "Okay, let me try to tackle this problem. So, the user wants a simple proof that if A^n equals the identity matrix, then the trace of A inverse is the conjugate of the trace of A. They mentioned that existing proofs use some heavy linear algebra like Jordan Normal Form or Weyl's unitary trick, but they want something more elementary. Hmm, let's think about the given conditions.\n\nFirst, A is a linear map on a finite-dimensional complex vector space, and it has finite order, meaning A^n = I for some integer n. The eigenvalues of A must be roots of unity since their nth power is 1. That means each eigenvalue λ satisfies |λ| = 1, so their inverses are their conjugates, right? Because λ^{-1} = \\overline{λ} if |λ|=1.\n\nSo, if we can show that the trace of A inverse is the sum of the inverses of the eigenvalues of A, which would then be the sum of the conjugates of the eigenvalues. But the trace of A is the sum of its eigenvalues. Therefore, tr(A^{-1}) should be the conjugate of tr(A). But wait, the problem states that the trace of A inverse is the conjugate of the trace of A. So, that seems to follow if the eigenvalues of A inverse are the conjugates of the eigenvalues of A. But why would that be the case?\n\nWait, eigenvalues of A inverse are the reciprocals of the eigenvalues of A. But since the eigenvalues are roots of unity, reciprocals are the same as conjugates. So, λ^{-1} = \\overline{λ} because λ is on the unit circle. Therefore, the trace of A inverse is the sum of λ^{-1}, which is the sum of \\overline{λ}, which is the conjugate of the sum of λ, which is the conjugate of tr(A). So, tr(A^{-1}) = \\overline{tr(A)}. \n\nBut the user says that existing proofs require Jordan form or unitary tricks. The user wants a proof without that. So, maybe we can avoid Jordan form by using the fact that A is diagonalizable? Wait, if A^n = I, then the minimal polynomial of A divides x^n - 1, which has distinct roots (since the derivative is nx^{n-1}, and the roots are the nth roots of unity, which are distinct in complex numbers). Therefore, A is diagonalizable. So, maybe we don't need Jordan form here because A is diagonalizable. If A is diagonalizable, then we can write A as a diagonal matrix with eigenvalues λ_1, ..., λ_n on the diagonal. Then A inverse is diagonal with entries λ_1^{-1}, ..., λ_n^{-1}, so the trace of A inverse is sum λ_i^{-1} = sum \\overline{λ_i} because |λ_i|=1. Then, the trace of A is sum λ_i, so its conjugate is sum \\overline{λ_i}, which equals tr(A^{-1}).\n\nTherefore, maybe the key is that A is diagonalizable, so we don't need Jordan form. So, the steps would be:\n\n1. Since A^n = I, the minimal polynomial of A divides x^n - 1, which has distinct roots, hence A is diagonalizable.\n\n2. Therefore, A is diagonalizable, and its eigenvalues are nth roots of unity, hence on the unit circle.\n\n3. The trace of A is the sum of its eigenvalues, and the trace of A inverse is the sum of their inverses, which are their conjugates.\n\n4. Hence, tr(A^{-1}) = conjugate of tr(A).\n\nBut the user says that existing proofs use the Jordan form. But here, we can avoid Jordan form by using the fact that A is diagonalizable because its minimal polynomial has distinct roots. So, maybe we can present this argument without invoking Jordan form.\n\nAlternatively, maybe we can use the fact that in characteristic 0, if a matrix satisfies a polynomial equation with distinct roots, it is diagonalizable. So, in the complex numbers, x^n - 1 splits into linear factors with distinct roots (as long as n is not zero, which it isn't here). So, that gives diagonalizability.\n\nSo, perhaps this is a simpler approach. Let me structure this.\n\nFirst, show that A is diagonalizable because its minimal polynomial divides x^n - 1, which has distinct roots. Therefore, A is diagonalizable. Then, since all eigenvalues λ satisfy λ^n = 1, they lie on the unit circle, so λ^{-1} = \\overline{λ}. Then, trace of A^{-1} is sum λ^{-1} = sum \\overline{λ} = \\overline{sum λ} = \\overline{tr(A)}. Therefore, done.\n\nBut the user says that existing proofs require Jordan form, but here we don't need Jordan form because the minimal polynomial having distinct roots gives diagonalizability directly. Maybe that's the key point. So, the proof can be simplified by noting diagonalizability, which follows from the minimal polynomial having distinct roots. Then, the rest follows.\n\nBut does the minimal polynomial necessarily divide x^n - 1? Yes, because A^n = I, so the minimal polynomial must divide x^n - 1. Therefore, if x^n - 1 factors into distinct linear factors over the complex numbers, then the minimal polynomial is separable, hence A is diagonalizable. So, in this case, since we're working over the complex numbers, and x^n - 1 has distinct roots (the nth roots of unity), the minimal polynomial will have distinct roots, hence A is diagonalizable. Therefore, we can avoid Jordan form by using this fact.\n\nTherefore, the proof can proceed as follows:\n\n1. A^n = I implies that the minimal polynomial of A divides x^n - 1. Since the roots of x^n - 1 are distinct (as the derivative n x^{n-1} has no roots in common with x^n - 1 except possibly at 0, but x^n -1 has no root at 0), the minimal polynomial has distinct roots, hence A is diagonalizable.\n\n2. Therefore, A is diagonalizable with eigenvalues λ_1, ..., λ_m (counted with multiplicity) where each λ_i^n = 1, so |λ_i| = 1, hence λ_i^{-1} = \\overline{λ_i}.\n\n3. The trace of A is the sum of the λ_i, and the trace of A^{-1} is the sum of λ_i^{-1} = sum \\overline{λ_i} = \\overline{sum λ_i} = \\overline{tr(A)}.\n\nHence, tr(A^{-1}) = \\overline{tr(A)}.\n\nThis avoids Jordan form and uses the diagonalizability from the minimal polynomial. Now, does the user consider the fact that the minimal polynomial has distinct roots leading to diagonalizability as a big theorem? Maybe, but perhaps it's more elementary than Jordan form. Alternatively, maybe there's a way to avoid even that.\n\nAlternatively, consider the eigenvalues directly. Since A^n = I, every eigenvalue λ satisfies λ^n = 1. Thus, each eigenvalue is a root of unity, hence |λ|=1. Then, for each eigenvalue λ of A, 1/λ is an eigenvalue of A^{-1}. But since |λ|=1, 1/λ = \\overline{λ}. So, the eigenvalues of A^{-1} are the conjugates of the eigenvalues of A. However, the trace of A is the sum of eigenvalues, and the trace of A^{-1} is the sum of the eigenvalues of A^{-1}, which are the conjugates. Thus, tr(A^{-1}) = \\overline{tr(A)}.\n\nBut wait, this seems too quick. But why can we say that the trace of A^{-1} is the sum of the eigenvalues of A^{-1}? Isn't that dependent on whether A is diagonalizable? Because if A is not diagonalizable, the trace is still the sum of the eigenvalues (with algebraic multiplicity), but the eigenvalues of A^{-1} would be the reciprocals of the eigenvalues of A (with the same multiplicities). However, even if A is not diagonalizable, the trace is the sum of the eigenvalues (with multiplicity). But since A^n = I, all eigenvalues are roots of unity, hence semi-simple (i.e., diagonalizable), because if there were a Jordan block, then A^n would have superdiagonal entries, which contradicts A^n = I. Therefore, A must be diagonalizable. So, maybe we can argue that A is diagonalizable because if it had a Jordan block, then A^n would not be I. Let's see.\n\nSuppose A has a Jordan block J of size k >1 for eigenvalue λ. Then J^n is upper triangular with λ^n on the diagonal and entries above the diagonal involving n λ^{n-1} times the superdiagonal entries of J. Since A^n = I, each Jordan block must satisfy J^n = I. But for k >1, J^n would have 1's on the diagonal but non-zero entries above, which cannot be the identity matrix. Therefore, all Jordan blocks must be 1x1, meaning A is diagonalizable. Hence, A is diagonalizable, so we can proceed as before.\n\nThis argument avoids using the minimal polynomial and instead uses the structure of Jordan blocks. But does the user consider this as part of \"big theorems\"? Because it's essentially using the Jordan form theorem. So maybe this is not better.\n\nAlternatively, perhaps we can use the fact that if a matrix satisfies p(A) = 0 where p has distinct roots, then it's diagonalizable. That's a theorem, but maybe it's considered a fundamental result in linear algebra. However, if we can use that theorem, then the proof is straightforward. But if the user wants to avoid quoting that theorem, perhaps we need another approach.\n\nAlternatively, use the fact that A is diagonalizable over the complex numbers because it's of finite order. This is a standard result: any finite order matrix over the complex numbers is diagonalizable. The proof is because, as above, its minimal polynomial divides x^n -1, which has distinct roots. So, this is essentially the same argument.\n\nThe user is asking for a proof from first principles without quoting big theorems. So maybe we need to directly show that A is diagonalizable. Here's an approach: since A^n = I, we can consider that (A - λI) is zero for each eigenvalue λ, but that's not helpful. Wait, another way: suppose v is a generalized eigenvector for eigenvalue λ. Then (A - λI)^k v =0 for some k. Then, since A^n = I, we have (A^n - I)v =0. But maybe combining these equations would lead us to conclude that k=1, hence all eigenvectors are actual eigenvectors, hence A is diagonalizable. But I'm not sure about the details here.\n\nAlternatively, consider that if A^n = I, then A is invertible, and A^{-1} = A^{n-1}. So, A^{-1} is a polynomial in A. Therefore, A and A^{-1} commute, and they can be simultaneously triangularized. But triangularizing A, we get that the eigenvalues are on the diagonal. Since A^n = I, each diagonal entry λ satisfies λ^n =1, so |λ|=1. Then, the trace of A is the sum of λ's, and the trace of A^{-1} is the sum of λ^{-1}'s. But since A is triangularizable, even if it's not diagonal, the trace is still the sum of the eigenvalues. But in this case, even if A is not diagonal, since the eigenvalues are on the unit circle, λ^{-1} = \\overline{λ}, so the sum of λ^{-1}'s is the conjugate of the sum of λ's. Therefore, tr(A^{-1}) = \\overline{tr(A)}.\n\nWait, this seems to work without needing diagonalization. Because even if A is upper triangular, the trace is the sum of the diagonals, which are the eigenvalues. So, even if A isn't diagonalizable, the trace of A^{-1} would still be the sum of the reciprocals of the eigenvalues, which for roots of unity are their conjugates. But then, the sum of the conjugates is the conjugate of the sum. Therefore, regardless of whether A is diagonalizable or not, tr(A^{-1}) = \\overline{tr(A)}. But wait, is that true?\n\nWait, the trace of a matrix is the sum of its eigenvalues (with algebraic multiplicity). If A is not diagonalizable, then the eigenvalues of A^{-1} are the reciprocals of the eigenvalues of A (with the same algebraic multiplicities). Therefore, even if A is not diagonalizable, tr(A^{-1}) = sum_{λ} (1/λ) * multiplicity. But since each λ is a root of unity, 1/λ = \\overline{λ}. Therefore, sum (1/λ) * multiplicity = sum \\overline{λ} * multiplicity = \\overline{sum λ * multiplicity} = \\overline{tr(A)}. Therefore, even if A is not diagonalizable, the result holds. Therefore, maybe we don't need to use diagonalization at all. We can just use the properties of eigenvalues and trace.\n\nBut wait, does this hold? Let me check. Suppose A has eigenvalues λ_1, ..., λ_m with algebraic multiplicities. Then, A^{-1} has eigenvalues 1/λ_1, ..., 1/λ_m with the same multiplicities. Since each λ_i^n =1, so |λ_i|=1, hence 1/λ_i = \\overline{λ_i}. Therefore, tr(A^{-1}) = sum 1/λ_i = sum \\overline{λ_i} = \\overline{sum λ_i} = \\overline{tr(A)}. Therefore, this is true regardless of whether A is diagonalizable or not, because trace is the sum of eigenvalues with multiplicities, and eigenvalues of A^{-1} are reciprocals, which are conjugates here. Therefore, the result holds without needing to assume diagonalization. Therefore, maybe the user's original proofs are complicating things by trying to use Jordan form or unitary matrices, but in reality, the result follows directly from eigenvalue properties.\n\nBut wait, how do we know that the eigenvalues of A^{-1} are the reciprocals of the eigenvalues of A, with the same multiplicities? Is that always true? Let's think. For any invertible matrix A, the eigenvalues of A^{-1} are the reciprocals of the eigenvalues of A. This is because if Av = λv, then A^{-1}v = (1/λ)v. So, if A is diagonalizable, then it's clear. But if A is not diagonalizable, then in the Jordan form, the eigenvalues of A^{-1} are still the reciprocals of the eigenvalues of A. For example, consider a Jordan block J with eigenvalue λ. Then J^{-1} is upper triangular with 1/λ on the diagonal and entries above the diagonal involving powers of 1/λ and the superdiagonal entries. So, even if there are Jordan blocks, the eigenvalues of J^{-1} are still 1/λ. Therefore, the algebraic multiplicities are preserved. Therefore, regardless of Jordan form, the trace of A^{-1} is the sum of the reciprocals of the eigenvalues, counted with multiplicity.\n\nTherefore, in this case, since all eigenvalues satisfy λ^n=1, hence |λ|=1, so 1/λ = \\overline{λ}. Therefore, sum (1/λ_i) = sum \\overline{λ_i} = \\overline{sum λ_i}, hence tr(A^{-1}) = \\overline{tr(A)}. Therefore, the proof doesn't require anything beyond knowing that eigenvalues of A^{-1} are reciprocals and that trace is sum of eigenvalues. So, maybe this is the elementary proof the user is looking for.\n\nBut wait, does this require knowing that trace is the sum of eigenvalues? Yes, but that's a basic fact. The trace is equal to the sum of eigenvalues counted with algebraic multiplicity, which is true over any algebraically closed field. Since we're working over complex numbers, that's fine. Therefore, the proof can be as simple as:\n\nSince A^n = I, every eigenvalue λ of A satisfies λ^n =1, hence |λ|=1 and thus 1/λ = \\overline{λ}. The eigenvalues of A^{-1} are the reciprocals of the eigenvalues of A, which are their conjugates. The trace of A is the sum of its eigenvalues, and the trace of A^{-1} is the sum of the conjugates of these eigenvalues. Therefore, tr(A^{-1}) = \\overline{tr(A)}. \n\nThis seems to be a very straightforward proof, using only basic properties of eigenvalues and trace. It doesn't require Jordan form, minimal polynomials, or unitary matrices. Just the fact that trace is the sum of eigenvalues and that eigenvalues of the inverse are reciprocals. The key step is recognizing that for roots of unity, reciprocals are conjugates. Therefore, maybe the user's original proofs were overcomplicating things, and this is the elementary proof they wanted.\n\nBut wait, the user says they know two proofs: one using Jordan form facts (which this avoids) and one using unitary matrices. The latter would involve showing A is similar to a unitary matrix, hence its inverse is conjugate transpose, so trace of inverse is conjugate of trace. But the proof I outlined above seems much simpler and doesn't need any of that. So, why did the user think those facts were necessary? Maybe because they were considering the multiplicities or the generalized eigenvalues? But in reality, trace is always the sum of eigenvalues with algebraic multiplicities, regardless of Jordan form. So, even if the matrix isn't diagonalizable, the trace of A^{-1} is still the sum of reciprocals of eigenvalues with multiplicities. Therefore, the proof is straightforward.\n\nTherefore, the answer is that there is a simple proof: since A^n = I, all eigenvalues λ satisfy λ^n =1, hence their reciprocals are conjugates. The trace of A is the sum of λ's, and trace of A^{-1} is sum of 1/λ's = sum of conjugates = conjugate of sum of λ's. Hence, done.",
    "Original_Problem": "Assumption: \n$A$ is a linear map from a finite-dimensional complex vector space to itself,\n$A^n = I$ for some integer $n$,\nThe trace of a matrix is the sum of its eigenvalues (counted with algebraic multiplicity),\nThe eigenvalues of $A^{-1}$ are the reciprocals of the eigenvalues of $A$,\nAll eigenvalues of $A$ are roots of unity (since $A^n = I$),\nFor any root of unity $\\lambda$, $\\lambda^{-1} = \\overline{\\lambda}$.\nProblem: \nProve that $\\mathrm{tr}(A^{-1}) = \\overline{\\mathrm{tr}(A)}$.",
    "Original_Problem_with_numerical_assumption": "Assumption:\nAssumption 1: $A$ is a linear map from a finite-dimensional complex vector space to itself,\nAssumption 2: $A^n = I$ for some integer $n$,\nAssumption 3: The trace of a matrix is the sum of its eigenvalues (counted with algebraic multiplicity),\nAssumption 4: The eigenvalues of $A^{-1}$ are the reciprocals of the eigenvalues of $A$,\nAssumption 5: All eigenvalues of $A$ are roots of unity (since $A^n = I$),\nAssumption 6: For any root of unity $\\lambda$, $\\lambda^{-1} = \\overline{\\lambda}$.\nProblem: \nProve that $\\mathrm{tr}(A^{-1}) = \\overline{\\mathrm{tr}(A)}$.",
    "Proof_problem": "Yes",
    "Redundant_assumption": "$$A^n=I\\ .$$",
    "Problem_with_redundant_assumption": "Assumption:\nAssumption 1: $A$ is a linear map from a finite-dimensional complex vector space to itself,\nAssumption 2: $A^n = I$ for some integer $n$,\nAssumption 3: The trace of a matrix is the sum of its eigenvalues (counted with algebraic multiplicity),\nAssumption 4: The eigenvalues of $A^{-1}$ are the reciprocals of the eigenvalues of $A$,\nAssumption 5: All eigenvalues of $A$ are roots of unity (since $A^n = I$),\nAssumption 6: For any root of unity $\\lambda$, $\\lambda^{-1} = \\overline{\\lambda}$.\nAssumption 7: $$A^n=I\\ .$$\nProblem: \nProve that $\\mathrm{tr}(A^{-1}) = \\overline{\\mathrm{tr}(A)}$."
}