{
    "Link_API": "https://api.stackexchange.com/2.3/questions/709419",
    "Title": "Prove $SST=SSE+SSR$",
    "Score": 18,
    "Category": "Chứng minh",
    "Tags": "statistics, regression",
    "Link": "https://math.stackexchange.com/questions/709419/prove-sst-ssessr",
    "Content": "Prove $$SST=SSE+SSR$$ I start with $$SST= \\Sigma (y_i-\\bar{y})^2=...=SSE+SSR+ \\Sigma 2( y_i-y_i^*)(y_i^*-\\bar{y} )$$ and I don't know how to prove that  $\\Sigma 2( y_i-y_i^*)(y_i^*-\\bar{y} )=0$ a note on notation: the residuals $e_i$ is $e_i=y_i-y_i^*$. A more common notation is $\\hat{y}$.",
    "AcceptedAnswer": "The principle underlying least squares regression is that the sum of the squares of the  errors is minimized. We can use calculus to find equations for the parameters $\\beta_0$ and $\\beta_1$ that minimize the sum of the squared errors. Let $S = \\displaystyle\\sum\\limits_{i=1}^n \\left(e_i \\right)^2= \\sum \\left(y_i - \\hat{y_i} \\right)^2= \\sum \\left(y_i - \\beta_0 - \\beta_1x_i\\right)^2$ We want to find $\\beta_0$ and $\\beta_1$ that minimize the sum, $S$ . We start by taking the partial derivative of $S$ with respect to $\\beta_0$ and setting it to zero. $$\\frac{\\partial{S}}{\\partial{\\beta_0}} = \\sum 2\\left(y_i - \\beta_0 - \\beta_1x_i\\right)^1 (-1) = 0$$ notice that this says, $$\\begin{align}\\sum \\left(y_i - \\beta_0 - \\beta_1x_i\\right) &= 0 \\\\\n       \\sum \\left(y_i - \\hat{y_i} \\right) &= 0 \\qquad (eqn. 1)\\end{align}$$ Hence, the sum of the residuals is zero (as expected). Rearranging and solving for $\\beta_0$ we arrive at, $$\\begin{aligned}\\sum \\beta_0 &= \\sum y_i -\\beta_1 \\sum x_i\\\\n\\beta_0 &= \\sum y_i -\\beta_1 \\sum x_i\\\\ \\beta_0 &= \\frac{1}{n}\\sum y_i -\\beta_1 \\frac{1}{n}\\sum x_i \\end{aligned}$$ now taking the partial of $S$ with respect to $\\beta_1$ and setting it to zero we have, $$\\frac{\\partial{S}}{\\partial{\\beta_1}} = \\sum 2\\left(y_i - \\beta_0 - \\beta_1x_i\\right)^1 (-x_i) = 0$$ and dividing through by $-2$ and rearranging we have, $$\\sum x_i \\left(y_i - \\beta_0 - \\beta_1x_i\\right) = 0$$ $$\\sum x_i \\left(y_i - \\hat{y_i} \\right) = 0$$ but, again we know that $\\hat{y_i} = \\beta_0 + \\beta_1x_i$ . Thus, $x_i = \\frac{1}{\\beta_1}\\left( \\hat{y_i} - \\beta_0 \\right) = \\frac1{\\beta_1}\\hat{y_i} -\\frac{\\beta_0}{\\beta_1}$ . Substituting this into the equation above gives the desired result. $$\\begin{aligned}\\sum x_i \\left(y_i - \\hat{y_i} \\right) &= 0\\\\\\sum \\left(\\frac1{\\beta_1}\\hat{y_i} - \\frac{\\beta_0}{\\beta_1}\\right) \\left(y_i - \\hat{y_i} \\right) &= 0\\\\\\frac1{\\beta_1}\\sum \\hat{y_i} \\left(y_i - \\hat{y_i} \\right) - \\frac{\\beta_0}{\\beta_1} \\sum \\left(y_i - \\hat{y_i} \\right)&= 0\\end{aligned}$$ Now, the second term is zero (by eqn. 1) and so, we arrive immediately at the desired result: $$\\sum \\hat{y_i} \\left(y_i - \\hat{y_i} \\right) = 0  \\qquad (eqn. 2)$$ Now, let's use eqn. 1 and eqn. 2 to show that $\\sum \\left(\\hat{y_i} - \\bar{y} \\right) \\left( y_i - \\hat{y_i} \\right) = 0$ - which was your original question. $$\\sum \\left(\\hat{y_i} - \\bar{y} \\right) \\left( y_i - \\hat{y_i} \\right) = \\sum \\hat{y_i} \\left( y_i - \\hat{y_i} \\right)  - \\bar{y} \\sum \\left( y_i - \\hat{y_i} \\right) = 0$$",
    "llm_answer_create_structured_problem": "###BEGIN_OF_FORMAT###\nAssumption: \nThe regression model is linear in parameters,\nThe residuals $e_i$ have a mean of zero (i.e., $\\sum e_i = 0$),\nThe residuals $e_i$ are uncorrelated with the predicted values $\\hat{y}_i$ (i.e., $\\sum e_i \\hat{y}_i = 0$).\nProblem:\nProve $$SST = SSE + SSR$$, where  \n- $SST = \\sum (y_i - \\bar{y})^2$ (total sum of squares),  \n- $SSE = \\sum (y_i - \\hat{y}_i)^2$ (sum of squared errors),  \n- $SSR = \\sum (\\hat{y}_i - \\bar{y})^2$ (sum of squares due to regression).  \n\n###END_OF_FORMAT###",
    "reasoning_create_structured_problem": "Okay, let's tackle this problem step by step. The user wants to prove that SST equals SSE plus SSR in the context of linear regression. The original question mentions expanding the SST into SSE, SSR, and a cross product term, which they need to show is zero. \n\nFirst, I need to recall the definitions of these terms. SST is the total sum of squares, which measures the total variation in the observed data (y_i). SSE is the sum of squared errors (residuals), which captures the variation not explained by the model. SSR is the sum of squares due to regression, explaining the variation accounted for by the model.\n\nThe equation given in the question expands SST into SSE + SSR plus the cross term Σ2(y_i - y_i*)(y_i* - ȳ). The goal is to prove that this cross term equals zero. \n\nLet me write down the expressions formally. Let’s denote the predicted values as ŷ_i (instead of y_i* for consistency with common notation). The residuals e_i = y_i - ŷ_i. \n\nSo, the cross term is Σ2e_i(ŷ_i - ȳ). To show this sum is zero, we need to show Σe_i(ŷ_i - ȳ) = 0. Expanding this, it becomes Σe_iŷ_i - ȳΣe_i. \n\nNow, in ordinary least squares (OLS) regression, certain properties hold. Specifically, the sum of residuals Σe_i = 0, and the residuals are orthogonal to the predictors. Additionally, the residuals are orthogonal to the predicted values ŷ_i. That is, Σe_iŷ_i = 0. \n\nIf these properties hold, then the cross term becomes 0 - ȳ*0 = 0. But wait, why is Σe_iŷ_i zero? Because in OLS, the design matrix X is orthogonal to the residuals e. Since ŷ = Xβ̂, the inner product of ŷ and e is β̂'X'e, and X'e = 0 by the normal equations. Hence, Σe_iŷ_i = 0. \n\nTherefore, both terms in the cross product sum are zero, making the entire cross term zero. Thus, SST = SSE + SSR. \n\nBut wait, the problem didn't specify the context of linear regression explicitly. However, given the notation (residuals, predicted values), it's implied that this is within OLS. So the key assumptions here are related to the OLS properties. \n\nTherefore, the assumptions should list the necessary conditions for those OLS properties to hold. Each assumption should be a single condition on one object. Let me list them:\n\n1. The model is linear in parameters.\n2. The residuals have mean zero (Σe_i = 0).\n3. The residuals are uncorrelated with the predicted values (Σe_iŷ_i = 0).\n\nThese are standard results from OLS estimation under the Gauss-Markov assumptions. The problem assumes these properties hold, allowing the cross term to vanish. \n\nSo, structuring the problem as per the required format, the assumptions would include these three points. The problem then is to prove SST = SSE + SSR given these conditions.",
    "Original_Problem": "Assumption: \nThe regression model is linear in parameters,\nThe residuals $e_i$ have a mean of zero (i.e., $\\sum e_i = 0$),\nThe residuals $e_i$ are uncorrelated with the predicted values $\\hat{y}_i$ (i.e., $\\sum e_i \\hat{y}_i = 0$).\nProblem:\nProve $$SST = SSE + SSR$$, where  \n- $SST = \\sum (y_i - \\bar{y})^2$ (total sum of squares),  \n- $SSE = \\sum (y_i - \\hat{y}_i)^2$ (sum of squared errors),  \n- $SSR = \\sum (\\hat{y}_i - \\bar{y})^2$ (sum of squares due to regression).",
    "Original_Problem_with_numerical_assumption": "Assumption:\nAssumption 1: The regression model is linear in parameters,\nAssumption 2: The residuals $e_i$ have a mean of zero (i.e., $\\sum e_i = 0$),\nAssumption 3: The residuals $e_i$ are uncorrelated with the predicted values $\\hat{y}_i$ (i.e., $\\sum e_i \\hat{y}_i = 0$).\nProblem:\nProve $$SST = SSE + SSR$$, where  \n- $SST = \\sum (y_i - \\bar{y})^2$ (total sum of squares),  \n- $SSE = \\sum (y_i - \\hat{y}_i)^2$ (sum of squared errors),  \n- $SSR = \\sum (\\hat{y}_i - \\bar{y})^2$ (sum of squares due to regression).",
    "Proof_problem": "Yes",
    "Redundant_assumption": "$$\\begin{align}\\sum \\left(y_i - \\beta_0 - \\beta_1x_i\\right) &= 0 \\\\\n       \\sum \\left(y_i - \\hat{y_i} \\right) &= 0 \\qquad (eqn. 1)\\end{align}$$",
    "Problem_with_redundant_assumption": "Assumption:\nAssumption 1: The regression model is linear in parameters,\nAssumption 2: The residuals $e_i$ have a mean of zero (i.e., $\\sum e_i = 0$),\nAssumption 3: The residuals $e_i$ are uncorrelated with the predicted values $\\hat{y}_i$ (i.e., $\\sum e_i \\hat{y}_i = 0$).\nAssumption 4: $$\\begin{align}\\sum \\left(y_i - \\beta_0 - \\beta_1x_i\\right) &= 0 \\\\\n       \\sum \\left(y_i - \\hat{y_i} \\right) &= 0 \\qquad (eqn. 1)\\end{align}$$\nProblem:\nProve $$SST = SSE + SSR$$, where  \n- $SST = \\sum (y_i - \\bar{y})^2$ (total sum of squares),  \n- $SSE = \\sum (y_i - \\hat{y}_i)^2$ (sum of squared errors),  \n- $SSR = \\sum (\\hat{y}_i - \\bar{y})^2$ (sum of squares due to regression)."
}