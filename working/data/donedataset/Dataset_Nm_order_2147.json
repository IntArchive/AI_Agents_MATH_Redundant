{
    "Link_API": "https://api.stackexchange.com/2.3/questions/503308",
    "Title": "How to prove Brownian motion is Gaussian Process?",
    "Score": 16,
    "Category": "Chứng minh",
    "Tags": "stochastic-processes, brownian-motion",
    "Link": "https://math.stackexchange.com/questions/503308/how-to-prove-brownian-motion-is-gaussian-process",
    "Content": "I'm reading Bernt Oksendal's \"Stochastic Differential Equations\" and this is one of the  proof that I'm totally lost. This is from Ch2.2, page 12-13 (sixth edition). First, Brownian motion is defined as $$P^x(B_{t_1}\\in F_1, \\cdots, B_{t_k}\\in F_k) := \\\\ \\int\\limits_{F_1 \\times \\cdots \\times F_k}p(t_1, x, x_1)\\cdots p(t_k-t_{k-1}, x_{k-1}, x_k)dx_1 \\ldots dx_k, \\tag{2.2.2}$$ where $$p(t,x,y) := (2\\pi t)^{-n/2}\\cdot \\exp(-\\frac{|x-y|^2}{2t})$$ Then, it says, Brownian motion $B_t$ is Gaussian Process,  i.e. for all $0 \\leq t1 \\leq \\cdots \\leq t_k$ the random variable $Z = (B_{t_1}, \\ldots, B_{t_k} ) \\in \\mathbb{R}^{nk}$ has a (multi)normal distribution. This means that there exists a vector $M \\in \\mathbb{R}^{nk}$ and a non-negative definite matrix $C = [c_{jm}] \\in \\mathbb{R}^{nk\\times nk}$ such that $$E^x\\left[\\exp\\left(i\\sum_{j=1}^{nk}u_jZ_j\\right)\\right] = \\exp\\left(-\\frac{1}{2}\\sum_{j,m}u_jc_{jm}u_m+i\\sum_j u_j M_j\\right) \\tag{2.2.3},$$ for all $u = (u_1, \\ldots , u_{nk}) \\in \\mathbb{R}^{nk}$ , where $i =\\sqrt{-1}$ is the imaginary unit, and $E^x$ denotes expectation with respect to $P^x$ . Moreover, if (2.2.3) holds then $M = E^x[Z]$ is the mean value of $Z$ (2.2.4),\nand $c_{jm} = E^x[(Z_j - M_j)(Z_m -M_m)]$ is the covariance matrix of $Z$ (2.2.5). Then it goes to the proof: To see that (2.2.3) holds for $Z = (B_{t_1}, \\ldots, B_{t_k} ) $ we calculate its left hand\nside explicitly by using (2.2.2) (see Appendix A -- something about multi-normal distribution) and obtain (2.2.3) with $$M=E^x[Z]=(x, x, \\cdots, x)\\in \\mathbb{R}^{nk} \\tag{2.2.6}$$ and $$C=\\begin{pmatrix} t_1 I_n & t_1 I_n & \\cdots & t_1 I_n \\\\ t_1 I_n & t_2 I_n & \\cdots & t_2 I_n\\\\ \\vdots & \\vdots & & \\vdots \\\\ t_1 I_n & t_2 I_n & \\cdots & t_k I_n \\end{pmatrix} \\tag{2.2.7} $$ Hence $$E^x[B_t] = x \\text{,     for all} t\\geq 0 \\tag{2.2.8}$$ and $$E^x[(B_t-x)^2]=nt, E^x[(B_t-x)(B_s-x)]=n \\min(s,t) \\tag{2.2.9}$$ I'm totally lost on how it just jump to the conclusion of (2.2.6) - (2.2.9). (2.2.2) seems quite far away from (2.2.3), how could it claim (2.2.3) stands because of (2.2.2) and could give the result for $M$ and $C$ ? Also I don't understand how come (2.2.7) looks so weird, the foot note is not normal (i,j) format, I could not figure out how this conclusion was reached....",
    "AcceptedAnswer": "A possible solution to avoid painful computations, using standard results, is the following. For the sake of simplicity, let us consider the 1-dimensional case and $x=0$ (the idea for those cases is the same, but the computations are longer). Define $T(x_1,\\ldots,x_k):=\\left(\\sum_{i=1}^j x_i\\right)_{j=1}^k=(x_1,x_1+x_2,\\ldots, x_1+x_2+\\ldots+x_k)$ . Rewrite the p.d.f. of (2.2.2) as: $$\n\\dfrac{1}{\\prod_{j=1}^k \\left(2\\pi(t_j-t_{j-1})\\right)^{1/2}}\\text{exp} \\left(\\sum_{j=1}^k\\dfrac{(z_j-z_{j-1})}{2(t_j-t_{j-1})} \\right)\n$$ Use the change of variables theorem (it is a straightforward formula), noting that $\\det{T}=1$ , to show that $X:=T^{-1}(Z)$ has a density of the form: $$\n\\prod_{j=1}^k\\rho^\\mathcal{N}_{t_j-t_{j-1}}(x_j),\n$$ where $\\rho^\\mathcal{N}_{\\sigma^2}$ is the p.d.f. of a normal variable with mean zero and variance $\\sigma^2$ . It is standard now, as the density is the one of a Gaussian vector with independent components, that the characteristic function is: $$\nE^0\\left[\\exp\\left(i\\sum_{j=1}^{k}u_jX_j\\right)\\right] = \\exp\\left(-\\frac{1}{2}\\sum_{j,m}u_j\\tilde{c}_{jm}u_m\\right) \\tag{2.2.3'}\n$$ where $$\n\\tilde{C}=(\\tilde{c}_{ij})_{i,j}=(\\delta_{ij}(t_j-t_{j-1}))_{i,j}\n$$ with $t_0:=0$ . To finally obtain the characteristic function of $Z$ , we know that it is going to be of the form of (2.2.3) with $$\nC=M_T \\tilde{C} M_T^t\n$$ where $M_T$ is the associated matrix of $T$ , i.e., $(M_T)_{ij}\\equiv M_{ij}=\\sum_{l=1}^{i}\\delta_{lj}$ , $$\nM_T= \\begin{bmatrix}\n1 & 0 & 0 & 0 & 0 & \\ldots & 0  \\\\\n1 & 1 & 0 & 0 & 0 & \\ldots & 0  \\\\\n1 & 1 & 1 & 0 & 0 & \\ldots & 0  \\\\\n&\\ldots  &\\ldots &\\ldots &\\ldots&\\ldots \\\\\n1& 1& 1& 1& 1& 1&1\n\\end{bmatrix} .\n$$ Thus, $$\nc_{ij}=\\sum_{l,m}\\left(\\sum_{r=1}^{i}\\delta_{rl}\\delta_{lm}(t_l-t_{l-1})\\sum_{s=1}^{j}\\delta_{sm}\\right)= \\sum_{r=1}^{i}\\sum_{s=1}^{j}\\delta_{rs}(t_r-t_{r-1})=\\sum_{r=1}^{m}(t_r-t_{r-1})=t_m \n$$ where $m:=\\min\\{i,j\\}$ . So we get (2.2.3) and (2.2.7).",
    "llm_answer_create_structured_problem": "###BEGIN_OF_FORMAT###\nAssumption: \n$B_t$ is a Brownian motion defined by the probability measure $P^x$ in equation (2.2.2),\nThe transition density $p(t,x,y)$ is given by $(2\\pi t)^{-n/2} \\exp\\left(-\\frac{|x-y|^2}{2t}\\right)$,\nTime points satisfy $0 \\leq t_1 \\leq t_2 \\leq \\cdots \\leq t_k$,\nThe expectation $E^x$ is computed with respect to $P^x$.\nProblem: \nProve that $Z = (B_{t_1}, B_{t_2}, \\ldots, B_{t_k})$ is a multivariate normal random variable with mean vector $M = (x, x, \\ldots, x) \\in \\mathbb{R}^{nk}$ and covariance matrix $C$ as defined in equation (2.2.7), leading to the conclusions in equations (2.2.8) and (2.2.9).\n###",
    "reasoning_create_structured_problem": "Okay, let's tackle this problem step by step. The user wants to prove that Brownian motion is a Gaussian process by showing that the random variable Z = (B_{t₁}, ..., B_{t_k}) has a multivariate normal distribution. The key here is to connect the definition of Brownian motion given via the probability measure P^x and the transition density p(t, x, y) to the characteristic function in (2.2.3). \n\nFirst, the assumptions. The problem states that Brownian motion is defined using the probability measure P^x with the joint distribution given by the integral involving the heat kernel p(t, x, y). The user also mentions that a Gaussian process requires the random vector Z to satisfy the characteristic function condition with mean M and covariance matrix C. \n\nLooking at the example provided earlier, the assumptions are listed per line, each addressing one property. For this problem, the main assumptions are:\n\n1. The process B_t is a Brownian motion as defined by the given probability measure P^x.\n2. The transition density p(t, x, y) is the standard Gaussian kernel.\n3. The time points t₁ ≤ ... ≤ t_k are fixed.\n4. The expectation E^x is taken with respect to P^x.\n\nThe problem then is to show that Z satisfies the characteristic function of a multivariate normal distribution, leading to the specific forms of M and C. The user is confused about how (2.2.2) leads to (2.2.3) and the resulting expressions for M and C. \n\nBut according to the instructions, I need to structure the problem by listing each assumption line by line, then the problem statement. So the assumptions should be individual conditions. For instance, the definition of Brownian motion using P^x, the form of p(t, x, y), the time ordering, etc. Then the problem is to prove that B_t is Gaussian with the given M and C.\n\nChecking the original question again, the user is asking how the conclusion (2.2.6) to (2.2.9) is reached from (2.2.2). So the core problem is to derive M and C using the definition provided. Therefore, the formatted problem should list the key assumptions (definition of B_t, form of p(t,x,y), etc.) and then state the problem as proving that Z is multivariate normal with those M and C.\n\nI need to ensure that each assumption is a single property applied to one object. So breaking down the definitions given:\n\n- B_t is defined with the probability measure P^x as in (2.2.2).\n- The transition density p(t,x,y) is given by the Gaussian kernel.\n- The time points are ordered 0 ≤ t₁ ≤ ... ≤ t_k.\n- The expectation E^x is under P^x.\n\nThus, each of these can be separate assumptions. The problem is to show that Z is multivariate normal, i.e., (2.2.3) holds with specific M and C, leading to (2.2.6)-(2.2.9).",
    "Original_Problem": "Assumption: \n$B_t$ is a Brownian motion defined by the probability measure $P^x$ in equation (2.2.2),\nThe transition density $p(t,x,y)$ is given by $(2\\pi t)^{-n/2} \\exp\\left(-\\frac{|x-y|^2}{2t}\\right)$,\nTime points satisfy $0 \\leq t_1 \\leq t_2 \\leq \\cdots \\leq t_k$,\nThe expectation $E^x$ is computed with respect to $P^x$.\nProblem: \nProve that $Z = (B_{t_1}, B_{t_2}, \\ldots, B_{t_k})$ is a multivariate normal random variable with mean vector $M = (x, x, \\ldots, x) \\in \\mathbb{R}^{nk}$ and covariance matrix $C$ as defined in equation (2.2.7), leading to the conclusions in equations (2.2.8) and (2.2.9).",
    "Original_Problem_with_numerical_assumption": "Assumption:\nAssumption 1: $B_t$ is a Brownian motion defined by the probability measure $P^x$ in equation (2.2.2),\nAssumption 2: The transition density $p(t,x,y)$ is given by $(2\\pi t)^{-n/2} \\exp\\left(-\\frac{|x-y|^2}{2t}\\right)$,\nAssumption 3: Time points satisfy $0 \\leq t_1 \\leq t_2 \\leq \\cdots \\leq t_k$,\nAssumption 4: The expectation $E^x$ is computed with respect to $P^x$.\nProblem: \nProve that $Z = (B_{t_1}, B_{t_2}, \\ldots, B_{t_k})$ is a multivariate normal random variable with mean vector $M = (x, x, \\ldots, x) \\in \\mathbb{R}^{nk}$ and covariance matrix $C$ as defined in equation (2.2.7), leading to the conclusions in equations (2.2.8) and (2.2.9).",
    "Proof_problem": "Yes",
    "Redundant_assumption": "$$\nE^0\\left[\\exp\\left(i\\sum_{j=1}^{k}u_jX_j\\right)\\right] = \\exp\\left(-\\frac{1}{2}\\sum_{j,m}u_j\\tilde{c}_{jm}u_m\\right) \\tag{2.2.3'}\n$$",
    "Problem_with_redundant_assumption": "Assumption:\nAssumption 1: $B_t$ is a Brownian motion defined by the probability measure $P^x$ in equation (2.2.2),\nAssumption 2: The transition density $p(t,x,y)$ is given by $(2\\pi t)^{-n/2} \\exp\\left(-\\frac{|x-y|^2}{2t}\\right)$,\nAssumption 3: Time points satisfy $0 \\leq t_1 \\leq t_2 \\leq \\cdots \\leq t_k$,\nAssumption 4: The expectation $E^x$ is computed with respect to $P^x$.\nAssumption 5: $$\nE^0\\left[\\exp\\left(i\\sum_{j=1}^{k}u_jX_j\\right)\\right] = \\exp\\left(-\\frac{1}{2}\\sum_{j,m}u_j\\tilde{c}_{jm}u_m\\right) \\tag{2.2.3'}\n$$\nProblem: \nProve that $Z = (B_{t_1}, B_{t_2}, \\ldots, B_{t_k})$ is a multivariate normal random variable with mean vector $M = (x, x, \\ldots, x) \\in \\mathbb{R}^{nk}$ and covariance matrix $C$ as defined in equation (2.2.7), leading to the conclusions in equations (2.2.8) and (2.2.9)."
}