{
    "Link_API": "https://api.stackexchange.com/2.3/questions/1617751",
    "Title": "Proof that the cross product is not associative without using components",
    "Score": 24,
    "Category": "Chứng minh",
    "Tags": "vectors",
    "Link": "https://math.stackexchange.com/questions/1617751/proof-that-the-cross-product-is-not-associative-without-using-components",
    "Content": "I need to show that the cross product is not associative without using components.\nI understand how to do it with components, which leads to an immediate counterexample, but other than that I am not sure how to do it.",
    "AcceptedAnswer": "Consider two non-zero perpendicular vectors $\\def\\v#1{{\\bf#1}}\\v a$ and $\\v b$.  We have\n$$(\\v a \\times\\v a)\\times\\v b=\\v0\\times\\v b=\\v0\\ .$$\nHowever $\\v a\\times\\v b$ is perpendicular to $\\v a$, and is not the zero vector, so\n$$\\v a\\times(\\v a\\times \\v b)\\ne\\v 0\\ .$$\nTherefore\n$$(\\v a \\times\\v a)\\times\\v b\\ne\\v a\\times(\\v a\\times \\v b)\\ .$$",
    "llm_answer_create_structured_problem": "###BEGIN_OF_FORMAT###\nAssumption: \nThe cross product operation is defined in three-dimensional space,\nThe cross product of two vectors results in a vector orthogonal to both,\nThe problem must be solved without using component-wise computation.\nProblem: \nShow that the cross product is not associative without using components.\n###END_OF_FORMAT###",
    "reasoning_create_structured_problem": "Okay, let's tackle this problem. The user wants to show that the cross product isn't associative without using components. Hmm, usually when I think about the cross product, I remember that it's defined in three dimensions and results in a vector perpendicular to the original two. Associativity would mean that (a × b) × c equals a × (b × c) for all vectors a, b, c. But we know that's not true.\n\nWait, the challenge here is to prove it without using components. So, how do I approach this without breaking into coordinate systems or i, j, k components? Maybe using properties like anticommutativity, distributivity, or the Jacobi identity? Let me recall. The cross product is anticommutative (a × b = -b × a), distributive over addition, and satisfies the Jacobi identity: a × (b × c) + b × (c × a) + c × (a × b) = 0.\n\nIf associativity held, then (a × b) × c would equal a × (b × c). But if that's the case, substituting into the Jacobi identity would give a different result. Wait, but the Jacobi identity already shows that the cross product isn't associative because it includes a sum of three terms. If it were associative, maybe the identity would collapse differently? Not sure if that's the right path.\n\nAlternatively, think about the magnitude. The magnitude of a × b is |a||b|sinθ. If associativity held, then |(a × b) × c| would equal |a × (b × c)|. But maybe using geometric interpretations. For example, (a × b) × c is in the plane of a × b and c, which is the plane perpendicular to a and b, then combined with c. Whereas a × (b × c) is in the plane of a and (b × c), which is perpendicular to both b and c. So unless a, b, c are in specific orientations, these results would point in different directions, hence can't be equal.\n\nWait, but how to formalize that without components? Maybe choose specific vectors where the associativity fails. For example, take standard basis vectors. But the user said without using components. Wait, even if I use standard basis vectors, I might implicitly be using components. Maybe use orthogonal vectors with certain properties. Let's see. Suppose a, b, c are non-coplanar vectors. Let me try with a = i, b = j, c = k. Then (i × j) × k = k × k = 0. On the other hand, i × (j × k) = i × i = 0. Hmm, both zero. Not helpful.\n\nAnother example: Let a = i, b = i, c = j. Then (i × i) × j = 0 × j = 0. i × (i × j) = i × k = -j. So 0 vs -j. That's a counterexample. But wait, here I used specific basis vectors, which might be considered using components. The user wants to avoid that. So how else can we find a contradiction?\n\nMaybe using the property that the cross product of two vectors is orthogonal to both. Suppose that the cross product is associative. Then (a × b) × c = a × (b × c). Let's take the scalar product of both sides with a. Left side: [(a × b) × c] · a. Right side: [a × (b × c)] · a.\n\nThe right side is [a × (b × c)] · a. Since a cross product is orthogonal to the original vectors, [a × something] is orthogonal to a, so the right side is 0. The left side: [(a × b) × c] · a. Using vector triple product formula: (u × v) × w = (u·w)v - (v·w)u. So (a × b) × c = (a·c)b - (b·c)a. Then [(a·c)b - (b·c)a] · a = (a·c)(b·a) - (b·c)(a·a). If we assume associativity, this should equal 0. So (a·c)(a·b) - (b·c)|a|² = 0 for all vectors a, b, c. But this is not true in general. For example, take a, b, c such that a·c ≠ 0 and a·b ≠ 0, and b·c and |a|² are such that the expression doesn't vanish. Therefore, the assumption of associativity leads to a contradiction. Therefore, cross product is not associative.\n\nBut wait, this uses the vector triple product expansion, which might be derived using components. If I use that identity, am I not implicitly relying on component-based proofs? Because the triple product formula is usually proven using components or index notation. So maybe that's not allowed.\n\nAlternatively, use the Jacobi identity. If the cross product were associative, then the Jacobi identity would simplify. Jacobi identity: a × (b × c) + b × (c × a) + c × (a × b) = 0. If associativity holds, then a × (b × c) = (a × b) × c, so substituting into Jacobi, we get (a × b) × c + (b × c) × a + (c × a) × b = 0. But if associativity holds, then each term can be cyclically permuted. However, without associativity, the Jacobi identity holds, but with associativity, this cyclic sum would equal zero. Wait, but this seems a bit abstract. Maybe not the clearest path.\n\nPerhaps another approach: use the fact that the cross product in 3D is related to Lie algebras, and the Jacobi identity is a fundamental property, which inherently means that the cross product isn't associative. But that's probably too advanced for a basic proof.\n\nAlternatively, use the fact that if the cross product were associative, then the algebra would be a Jordan algebra or associative, but it's known to be a Lie algebra, which is not associative. But again, this is more abstract.\n\nWait, maybe go back to the basics. Suppose that the cross product is associative. Then for any vectors a, b, c, (a × b) × c = a × (b × c). Let's take the magnitude of both sides. |(a × b) × c| = |a × (b × c)|. Using |u × v| = |u||v|sinθ. Left side: |a × b||c|sinφ, where φ is the angle between a × b and c. Right side: |a||b × c|sinψ, where ψ is the angle between a and b × c. But unless these magnitudes are equal for all a, b, c, which they are not, this would lead to a contradiction. However, it's not straightforward to see. Maybe specific vectors can be chosen. For example, take a = b = c as a unit vector. Then (a × a) × a = 0 × a = 0, and a × (a × a) = a × 0 = 0. Not helpful.\n\nTake a = i, b = j, c = i. Then (i × j) × i = k × i = j. On the other hand, i × (j × i) = i × (-k) = -j. So j vs -j. Hence, (a × b) × c ≠ a × (b × c). Therefore, this is a counterexample. But again, this uses specific basis vectors, which might be considered using components. The user wants to avoid that. So is there a way to construct such a counterexample without referencing components?\n\nAlternatively, use orthogonal vectors with certain properties. Let me take three mutually orthogonal vectors a, b, c with |a| = |b| = |c| = 1. Then a × b is parallel to c (assuming right-hand rule). Then (a × b) × c = c × c = 0. But a × (b × c) = a × (a) since b × c would be parallel to a. Wait, no. If a, b, c are orthonormal, then b × c = a (assuming right-hand). Then a × (b × c) = a × a = 0. Hmm, both sides zero. Not helpful.\n\nWait, earlier when I took a = i, b = j, c = i, that gives a counterexample. But to express this without components, maybe describe vectors in a coordinate-free way. Suppose we have a vector a, and another vector b orthogonal to a, and c = a. Then compute (a × b) × a vs a × (b × a). Let's see. (a × b) is orthogonal to both a and b. Then (a × b) × a is a vector orthogonal to both (a × b) and a. Since (a × b) is orthogonal to a, their cross product is in the plane of a and b. Wait, perhaps using the triple product identity again. (a × b) × a = (a · a) b - (b · a) a = |a|² b - 0 = |a|² b. On the other hand, b × a = -a × b, so a × (b × a) = a × (-a × b) = -a × (a × b). Then a × (a × b) = (a · b) a - (a · a) b = 0 - |a|² b = -|a|² b. Therefore, a × (b × a) = -(-|a|² b) = |a|² b. Wait, wait: let's compute it step by step.\n\nFirst, b × a = -a × b. Then a × (b × a) = a × (-a × b) = -a × (a × b). Now, using the vector triple product formula: a × (a × b) = a(a · b) - b(a · a). Since a and b are orthogonal, a · b = 0. So this becomes 0 - |a|² b = -|a|² b. Therefore, a × (b × a) = -(-|a|² b) = |a|² b. Meanwhile, (a × b) × a = |a|² b. So in this case, they are equal. Hmm, that's not a counterexample. Wait, but earlier when I used i, j, i, the result was different. Let me check. If a = i, b = j, c = i. Then (i × j) × i = k × i = j. And i × (j × i) = i × (-k) = -j. So j vs -j. But according to the previous calculation, they should be equal? Wait, no, perhaps I made a mistake.\n\nWait, (a × b) × a = |a|² b. In the case of a = i, b = j, |a|² = 1, so (i × j) × i = k × i = j, which matches |a|² b = 1 * j. But a × (b × a) = i × (j × i) = i × (-k) = -j. But according to the earlier formula, a × (b × a) = |a|² b, which would be j. But in reality, it's -j. So there's a discrepancy. Wait, why? Because the triple product expansion might depend on the order. Let's re-examine the triple product formula.\n\nThe formula is (u × v) × w = (u · w) v - (v · w) u. So in the case of (a × b) × a, u = a × b, v = a, w = a. Wait, no. Wait, no, (u × v) × w is the same as ((a × b) × a). Let me let u = a × b, v = a, then (u × v) × w is not the case here. Wait, no. Wait, let's correct this. The original expression is (a × b) × c. If we set c = a, then it's (a × b) × a. Applying the triple product formula: (u × v) × w = (u · w) v - (v · w) u. Here, u = a × b, v = a, w = a. So ( (a × b) × a ) = ( (a × b) · a ) a - (a · a ) (a × b ). But (a × b) · a = 0, since a cross product is orthogonal to the original vectors. So this simplifies to 0 - |a|² (a × b). Wait, that's -|a|² (a × b). But earlier when we computed with i, j, i, we got j. But according to this, it should be - (a × b). Wait, i × j = k, so - (i × j) = -k. But (i × j) × i = k × i = j. So this contradicts the formula. Wait, I must have messed up the triple product formula.\n\nWait, the correct vector triple product formula is:\n\nu × (v × w) = v(u · w) - w(u · v).\n\nSo (a × b) × a is equal to a × b cross a. Wait, applying the formula:\n\nFirst, let u = a × b, v = a, w = a.\n\nWait, no. The formula is u × (v × w) = v (u · w) - w (u · v). So if we have (a × b) × a, which is (u) × a where u = a × b. So applying the formula:\n\nu × a = a × b × a = - a × (a × b) = - [ a (a · b ) - b (a · a ) ] = - [ 0 - |a|² b ] = |a|² b.\n\nBut when we did the example with i, j, i:\n\n(i × j) × i = k × i = j.\n\nWhich matches |a|² b since |i|² = 1 and b = j. So it's correct. Then why in the previous case when we computed a × (b × a), which is a × ( -a × b ), we get - a × (a × b ) = - [ a (a · b ) - b (a · a ) ] = - [ 0 - |a|² b ] = |a|² b. So a × (b × a ) = |a|² b. But in the specific example with a = i, b = j, we get i × (j × i) = i × (-k) = -j. Wait, this is conflicting. According to the formula, a × (b × a ) = |a|² b, which would be j in this case, but the actual computation gives -j. What's the issue here?\n\nWait, let's compute step by step. b × a = j × i = -k. Then a × (b × a ) = i × (-k ) = - (i × k ) = - (-j ) = j. Oh! Wait, I made a mistake earlier. i × k is -j, so i × (-k ) is - (i × k ) = - (-j ) = j. So indeed, a × (b × a ) = j, which is |i|² j = 1 * j. So the formula holds. So in this case, both (a × b ) × a and a × (b × a ) equal j. So that example doesn't produce a discrepancy. Then where did the earlier example where I thought they were different come from?\n\nWait, let's take a = i, b = j, c = k.\n\nThen (a × b ) × c = (k ) × k = 0.\n\na × (b × c ) = i × (i ) = 0. Both zero. Not helpful.\n\nAnother example: a = i, b = i, c = j.\n\n(a × b ) × c = (0 ) × j = 0.\n\na × (b × c ) = i × (k ) = j. So 0 vs j. So here's a counterexample. But in this case, a and b are the same vector. So (a × b ) = 0, so the left side is 0. The right side is a × (b × c ) = i × (i × j ) = i × k = -j. Wait, no. Wait, b = i, c = j. So b × c = i × j = k. Then a × (b × c ) = i × k = -j. But earlier I thought it was j. So here, left side is 0, right side is -j. Therefore, (a × b ) × c ≠ a × (b × c ). Hence, the cross product isn't associative. But in this case, a and b are the same vector, leading to a zero cross product. So the left side is zero, but the right side is non-zero, hence a contradiction.\n\nBut to present this without components, one could say: Let a, b be equal vectors and c be another vector not collinear with them. Then (a × b ) × c = 0 × c = 0, whereas a × (b × c ) is a × (something not collinear with a ), which is non-zero. Hence, the two sides are unequal, disproving associativity.\n\nBut does this rely on components? Well, it's using the properties that a × a = 0 and that if a and c are not collinear, then b × c is not collinear with a, hence a × (b × c ) is not zero. This is a coordinate-free argument. Hence, this would suffice.\n\nTherefore, the counterexample exists by choosing a = b and c orthogonal to them, leading to (a × b ) × c = 0 and a × (b × c ) ≠ 0. Therefore, the cross product is not associative.",
    "Original_Problem": "Assumption: \nThe cross product operation is defined in three-dimensional space,\nThe cross product of two vectors results in a vector orthogonal to both,\nThe problem must be solved without using component-wise computation.\nProblem: \nShow that the cross product is not associative without using components.",
    "Original_Problem_with_numerical_assumption": "Assumption:\nAssumption 1: The cross product operation is defined in three-dimensional space,\nAssumption 2: The cross product of two vectors results in a vector orthogonal to both,\nAssumption 3: The problem must be solved without using component-wise computation.\nProblem: \nShow that the cross product is not associative without using components.",
    "Proof_problem": "Yes",
    "Redundant_assumption": "$$\\v a\\times(\\v a\\times \\v b)\\ne\\v 0\\ .$$",
    "Problem_with_redundant_assumption": "Assumption:\nAssumption 1: The cross product operation is defined in three-dimensional space,\nAssumption 2: The cross product of two vectors results in a vector orthogonal to both,\nAssumption 3: The problem must be solved without using component-wise computation.\nAssumption 4: $$\\v a\\times(\\v a\\times \\v b)\\ne\\v 0\\ .$$\nProblem: \nShow that the cross product is not associative without using components."
}