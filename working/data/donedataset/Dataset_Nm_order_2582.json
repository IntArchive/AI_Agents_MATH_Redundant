{
    "Link_API": "https://api.stackexchange.com/2.3/questions/661123",
    "Title": "$SAS^{-1}=\\lambda A$ - show $\\lambda^n=1$or A is nilpotent",
    "Score": 13,
    "Category": "Chứng minh",
    "Tags": "linear-algebra",
    "Link": "https://math.stackexchange.com/questions/661123/sas-1-lambda-a-show-lambdan-1or-a-is-nilpotent",
    "Content": "I found a question im struggling with in http://www.math.upenn.edu/ugrad/calc/m240/240la.pdf Consider an $n*n$ matrix A with real or complex coefficients and $S$ an invertible matrix. show that if $SAS^{-1}=\\lambda A$ then $\\lambda^n=1\\ or\\ A\\ is\\ nilpotent$ now, showing that $\\lambda^{n} = 1$ is easy when we take the determinant of both sides and $\\det(A)\\neq0$. but showing that the equality holds for $\\det(A) = 0$ doesn't prove that $A$ is nilpotent, because not every singular matrix is nilpotent. Any help will be appreciated!\nThank You!",
    "AcceptedAnswer": "Unfortunately, this question is wrong.\nI will give a counter example, and then show the results I have. Counter Example Let $A=\\begin{pmatrix}1&0&0\\\\0&-1&0\\\\0&0&0\\end{pmatrix}$, $S=\\begin{pmatrix}0&1&0\\\\1&0&0\\\\0&0&1\\end{pmatrix}$, and $\\lambda=-1$. Note that $S$ switches the first basis element with the second, and keeps the third.\nFrom here (or by a simple calculation) it follows that $S^{-1}=S$, and also $SAS^{-1}=\\begin{pmatrix}-1&0&0\\\\0&1&0\\\\0&0&0\\end{pmatrix}$, which means:\n$SAS^{-1}=\\lambda A$ However, $\\lambda^3=(-1)^3=-1$ and $A$ clearly isn't nilpotent. Alternative Result Notation:\n$A\\in M_n(\\mathbb{C})$ is a matrix,\n$\\alpha_1,\\ldots,\\alpha_n$ are the eigenvalues of $A$ (with multiplicities),\n$S\\in M_n(\\mathbb{C})$ is invertible,\n$\\lambda\\in\\mathbb{C}$ is a scalar,\n$V_{B,\\beta}$ is the eigenspace of $\\beta$ with respect to the matrix $B$\nand finally $p_B$ is the characteristic polynomial of $B$. As you noted, in the case $\\det A\\neq0$ it can be shown that $\\lambda^n=1$.\nThis can be generalized as follows: Theorem. Denote by $k$ be the algebraic multiplicity of $0$.\nIf $SAS^{-1}=\\lambda A$ then $\\lambda^{n-k}=1$. Proof. If $k=n$ the claim is trivial, since $\\lambda^0=1$. From here we assume $k<n$\nAlso, WLOG we assume that the $0$ eigenvalues are last in the list, i.e. $\\alpha_1,\\ldots,\\alpha_{n-k}\\neq 0$. First we prove that $|\\lambda|=1$. We think of multiplying by $\\lambda$ geometrically, this operation rotates and stretches the value, but since it takes a finite list to itself, it can't stretch.\nFormally, if $|\\lambda|>1$, take an eigenvalue with minimal absolute value, its product with $\\lambda$ is also an eigenvalue, but it has a bigger absolute value which contradicts the minimality (the case $|\\lambda|<1$ is similar). Thus indeed $|\\lambda|=1$, and multiplying by $\\lambda$ is equivalent to rotating by some angle. Now we are moving to prove that $\\lambda^{n-k}=1$. We know that $p_A(x)=\\prod_{i=1}^n{(x-\\alpha_i)}$, and it is a well known fact that $p_A(x)=p_{SAS^{-1}}(x)$.\nUsing the lemma below, $p_{\\lambda A}(x)=\\prod_{i=1}^n{(x-\\lambda\\alpha_i)}$.\nWe are given that $SAS^{-1}=\\lambda A$.\nAltogether $\\prod_{i=1}^n{(x-\\alpha_i)}=p_A(x)=p_{SAS^{-1}}(x)=p_{\\lambda A}(x)=\\prod_{i=1}^n{(x-\\lambda\\alpha_i)}$. It follows the list $\\alpha_1,\\ldots\\alpha_n$ is equal (up to permutation) to the list $\\lambda\\alpha_1,\\ldots,\\lambda\\alpha_n$.\nParticularly, multiplying an eigenvalue by $\\lambda$ gives an eigenvalue. Geometrically thinking of our eigenvalues will give the desired result.\nWe should think of them as orbits around the origin, where having multiplicities amounts to two different coinciding orbits. To put this more formally, for each $1\\leq i\\leq n-k$, consider $O_i=\\{\\alpha_i,\\lambda\\alpha_i,\\lambda^2\\alpha_i,\\lambda^3\\alpha_i,\\ldots\\}$.\nDefine them with multiplicity, such that two orbits that contain different instances of the same eigenvalue are different. As we noted, all of the values in $O_i$ are eigenvalues, so $O_i$ must be finite, also it is evident (geometrically and algebraically) that they are of the same length, say $|O_i|=l$. Finally it is evident (with the same picture in mind) that $\\lambda^l\\alpha_i=\\alpha_i$ thus $\\lambda^l=1$. The collection of orbits partitions the list $\\alpha_1,\\ldots,\\alpha_{n-k}$ into $p=\\frac{n-k}{l}=$ different orbits, which is a natural number. Therefore $\\lambda^{n-k}=\\lambda^{lp}=(\\lambda^l)^p=1^p=1$, as required. $\\square$ Lemma. Let $A\\in M_n(\\mathbb{C})$ and $\\lambda\\in\\mathbb{C}$.\nLet $\\alpha_1,\\ldots,\\alpha_n$ be the (not necessarily distinct) eigenvalues of $A$.\nThen $\\lambda\\alpha_1,\\ldots,\\lambda\\alpha_n$ are the eigenvalues of $\\lambda A$.\nThis is equivalent to $p_{\\lambda A}=\\prod_{i=1}^n{(x-\\lambda\\alpha_i)}$. Proof. Let $v_i\\in V_{A,\\alpha_i}$, i.e. $Av_i=\\alpha_i v_i$. \nIt follows that $(\\lambda A)v_i=\\lambda(Av_i)=\\lambda(\\alpha_i v_i)=(\\lambda\\alpha_i)v_i$, which means that $\\lambda\\alpha_i$ is an eigenvalue of $\\lambda A$ and $v_i\\in V_{\\lambda A,\\alpha_i}$. The last paragraph shows that $V_{\\lambda A,\\lambda\\alpha_i}=V_{A,\\alpha_i}$ (as the trick works in both directions), and the lemma follows. $\\square$",
    "llm_answer_create_structured_problem": "###BEGIN_OF_FORMAT###\nAssumption: \nA is an $n \\times n$ matrix with real or complex coefficients,\nS is an invertible $n \\times n$ matrix,\n$SAS^{-1} = \\lambda A$ for some scalar $\\lambda$.\nProblem:\nShow that either $\\lambda^n = 1$ or $A$ is nilpotent.\n###",
    "reasoning_create_structured_problem": "Okay, let's try to tackle this problem step by step. So the problem says that if we have an n×n matrix A and an invertible matrix S such that SAS⁻¹ = λA, then we need to show that either λⁿ = 1 or A is nilpotent. \n\nFirst, let me recall what I know. When dealing with similar matrices, they share the same eigenvalues. Since S is invertible, SAS⁻¹ is similar to A, so they should have the same eigenvalues. But here, SAS⁻¹ equals λA. So if A has an eigenvalue μ, then λA would have eigenvalue λμ. But since SAS⁻¹ and A are similar, their eigenvalues must be the same. Therefore, for each eigenvalue μ of A, λμ must also be an eigenvalue of A. Hmm, that seems important.\n\nWait, but this might only hold if A is diagonalizable. Since A might not be diagonalizable, maybe I need to approach this differently. Let's think about the equation SAS⁻¹ = λA. If I rearrange this, I get S A = λ A S. Maybe I can consider some kind of commutation relation here?\n\nAnother approach is to consider the determinant. The user mentioned that taking determinants gives (det(S) det(A) det(S⁻¹)) = det(λA). But det(S) det(S⁻¹) = 1, so det(A) = λⁿ det(A). If det(A) ≠ 0, then we can divide both sides by det(A) to get 1 = λⁿ, hence λⁿ = 1. That part they said is easy. But when det(A) = 0, we can't do that, so we need to show that A is nilpotent in that case.\n\nBut why would det(A) = 0 imply A is nilpotent? Not all singular matrices are nilpotent, right? For example, a matrix with a zero eigenvalue but others non-zero is singular but not nilpotent. So there must be something else here.\n\nLet me think about the eigenvalues. If SAS⁻¹ = λA, then for any eigenvalue μ of A, λμ must also be an eigenvalue. But since similar matrices have the same eigenvalues, the eigenvalues of A must be closed under scaling by λ. If A is not nilpotent, then it has at least one non-zero eigenvalue μ. Then scaling μ by λ must give another eigenvalue of A, which would be λμ. But then scaling that by λ gives λ²μ, and so on. Since A is n×n, it can have at most n distinct eigenvalues. But unless λ is a root of unity, these scaled eigenvalues would keep generating new eigenvalues, which can't happen. So if λ is not a root of unity, then the only possibility is that all eigenvalues are zero, making A nilpotent. But how does this relate to λⁿ =1?\n\nWait, if λ is a root of unity, then λⁿ =1 for some n? Wait, no. The problem states that either λⁿ =1 or A is nilpotent. So if λ is such that λⁿ =1, then even if A has non-zero eigenvalues, the scaling by λ doesn't create new eigenvalues because λⁿ =1, so the eigenvalues would cycle after n steps. But in a matrix of size n, you can't have more than n eigenvalues, so maybe this condition λⁿ =1 allows the eigenvalues to cycle in a way that's consistent with the matrix size. Alternatively, if λ is not a root of unity, then A must be nilpotent because otherwise, you get infinitely many eigenvalues by scaling, which is impossible. But since the field here is real or complex, maybe the argument is different.\n\nAlternatively, let's consider the minimal polynomial of A. If SAS⁻¹ = λA, then A satisfies some polynomial equation. Suppose A is not nilpotent, so there's a non-zero eigenvalue. Then from the equation, the eigenvalues of A must satisfy μ = λμ, but wait, no. Wait, if SAS⁻¹ = λA, then the eigenvalues of A are the same as the eigenvalues of λA. So if μ is an eigenvalue of A, then λμ is also an eigenvalue. But since similar matrices have the same eigenvalues, the eigenvalues of λA must be the same as the eigenvalues of A. Therefore, for every eigenvalue μ of A, λμ is also an eigenvalue. So if μ ≠ 0, then λμ must be an eigenvalue, and then λ(λμ) = λ²μ must also be an eigenvalue, and so on. If λ is not a root of unity, then these are all distinct, which would imply that A has infinitely many eigenvalues, which is impossible since A is n×n. Therefore, the only possibility is that all eigenvalues are zero, making A nilpotent. If λ is a root of unity, then λⁿ=1 (assuming we're working over the complex numbers, perhaps λ is a primitive nth root of unity). Wait, but maybe the minimal polynomial of A would require that λⁿ=1. Alternatively, considering that the eigenvalues must satisfy μ = λⁿμ. If μ ≠0, then λⁿ=1. So if there exists a non-zero eigenvalue, then λⁿ=1. If all eigenvalues are zero, A is nilpotent. Therefore, either λⁿ=1 or A is nilpotent.\n\nThis seems like the right path. So formalizing this, if A is not nilpotent, then it has at least one non-zero eigenvalue μ. Then from the eigenvalue reasoning, we must have that λⁿ=1. Therefore, either λⁿ=1 or A is nilpotent.\n\nBut the user mentioned that showing for det(A)=0 doesn't lead directly to nilpotent. So perhaps we need to use the fact that if A is not nilpotent, then it has a non-zero eigenvalue, leading to λⁿ=1. So the key is that if A is not nilpotent, then it has a non-zero eigenvalue, which would force λⁿ=1.\n\nTherefore, the assumptions are:\n\n- A is an n×n matrix with real or complex coefficients.\n- S is an invertible n×n matrix.\n- SAS⁻¹ = λA.\n\nThen the problem is to show that either λⁿ=1 or A is nilpotent.\n\nWait, the original problem didn't specify that n is the size of the matrix? Wait, the title says \"show λ^n=1 or A is nilpotent\", and the question mentions \"an n*n matrix A\". So yes, n is the size of the matrix.\n\nSo the assumptions would be:\n\nA is an n×n matrix.\n\nS is an invertible n×n matrix.\n\nSAS⁻¹ = λA.\n\nAnd the problem is to show that either λⁿ =1 or A is nilpotent.\n\nThe user's original problem mentions that when det(A) ≠0, it's easy to get λⁿ=1 by taking determinants. When det(A)=0, we have to show A is nilpotent.\n\nBut the conclusion is that either λⁿ=1 or A is nilpotent. So even if det(A)=0, if λⁿ=1, then maybe A is not nilpotent. Wait, but if λⁿ=1, then even if det(A)=0, maybe A is not nilpotent but satisfies SAS⁻¹=λA with λⁿ=1. So the conclusion is that either λⁿ=1 or A is nilpotent. It's possible for both to happen, but at least one of them must hold.\n\nSo in the structured format, the assumptions are:\n\nA is an n×n matrix with real or complex coefficients.\n\nS is an invertible n×n matrix.\n\nSAS⁻¹ = λA.\n\nProblem: Show that either λⁿ = 1 or A is nilpotent.\n\nBut wait, the user's original problem had a typo? The title says \"show λ^n=1 or A is nilpotent\". The problem statement in the question says the same. So the assumptions are as above. The key is to list each assumption line by line, each addressing one property.\n\nSo:\n\nAssumption:\nA is an n×n matrix with real or complex coefficients,\nS is an invertible n×n matrix,\nSAS⁻¹ = λA.\n\nProblem: Show that either λⁿ = 1 or A is nilpotent.\n\nYes, that's the structured version.",
    "Original_Problem": "Assumption: \nA is an $n \\times n$ matrix with real or complex coefficients,\nS is an invertible $n \\times n$ matrix,\n$SAS^{-1} = \\lambda A$ for some scalar $\\lambda$.\nProblem:\nShow that either $\\lambda^n = 1$ or $A$ is nilpotent.",
    "Original_Problem_with_numerical_assumption": "Assumption:\nAssumption 1: A is an $n \\times n$ matrix with real or complex coefficients,\nAssumption 2: S is an invertible $n \\times n$ matrix,\nAssumption 3: $SAS^{-1} = \\lambda A$ for some scalar $\\lambda$.\nProblem:\nShow that either $\\lambda^n = 1$ or $A$ is nilpotent.",
    "Proof_problem": "Yes",
    "Redundant_assumption": "$SAS^{-1}=\\lambda A$",
    "Problem_with_redundant_assumption": "Assumption:\nAssumption 1: A is an $n \\times n$ matrix with real or complex coefficients,\nAssumption 2: S is an invertible $n \\times n$ matrix,\nAssumption 3: $SAS^{-1} = \\lambda A$ for some scalar $\\lambda$.\nAssumption 4: $SAS^{-1}=\\lambda A$\nProblem:\nShow that either $\\lambda^n = 1$ or $A$ is nilpotent."
}