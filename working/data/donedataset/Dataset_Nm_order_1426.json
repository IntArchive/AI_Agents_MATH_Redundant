{
    "Link_API": "https://api.stackexchange.com/2.3/questions/1246705",
    "Title": "Linear homomorphisms of square matrices are conjugations",
    "Score": 21,
    "Category": "Chứng minh",
    "Tags": "linear-algebra, matrices, lie-groups, lie-algebras",
    "Link": "https://math.stackexchange.com/questions/1246705/linear-homomorphisms-of-square-matrices-are-conjugations",
    "Content": "I was doing some linear algebra exercises and came across the following tough problem : Let $M_{n\\times n}(\\mathbf{R})$ denote the set of all the matrices whose entries are real numbers. Suppose $\\phi:M_{n\\times n}(\\mathbf{R})\\to M_{n\\times n}(\\mathbf{R})$ is a nonzero linear transform (i.e. there is a matrix $A$ such that $\\phi(A)\\neq 0$) such that for all $A,B\\in M_{n\\times n}(\\mathbf{R})$\n  $$\\phi(AB)=\\phi(A)\\phi(B).$$\n  Prove that there exists a invertible matrix $T\\in M_{n\\times n}(\\mathbf{R})$ such that \n  $$\\phi(A)=TAT^{-1}$$\n  for all $A\\in M_{n\\times n}(\\mathbf{R})$. This is an exercise from my textbook and I am all thumbs when I attempted to solve it . Can someone tell me as to how should I , at least , start the problem ?",
    "AcceptedAnswer": "This kind of problems are known as linear preserver problems in the literature. The following is a sketch of proof that immediately comes to my mind. Certainly there are simpler ways to solve the problem (especially if one makes use of existing results on linear preserver problems), but anyway, let $\\{e_1,\\ldots,e_n\\}$ be the standard basis of $\\mathbb R^n$ and $E_{ij}=e_ie_j^T$. Prove that $\\phi$ is injective. Hint. Suppose the contrary that $\\phi(X)=0$ for some matrix $X$ whose $(r,s)$-th entry is nonzero. Now consider $\\phi(E_{ir}XE_{sj})$ for every $(i,j)$. Prove that $\\phi$ preserves non-invertibility ( hint: if $X$ is singular, then $XY=0$ for some nonzero $Y$), $\\phi$ preserves invertibility ( hint: if $\\phi(P)$ is singular for some invertible $P$, then $\\phi(P)Y=0$ for some nonzero matrix $Y$; since $\\phi$ is an injective linear operator over a finite dimensional vector space, $Y=\\phi(B)$ for some nonzero $B$, but then ...), $\\phi(I)=I$. This is the only interesting step in the whole proof: show that every $\\phi(E_{ii})$ is a rank-1 idempotent matrix. Hint: the rank of an idempotent matrix is equal to its trace. Argue that without loss of generality, we may assume that $\\phi(E_{11})=E_{11}$. Show that whenever $i,j\\ne1$, the first column and the first row of $\\phi(E_{ij})$ are zero ( hint: $E_{ij}E_{11}=0=E_{11}E_{ij}$). By mathematical induction/recursion, show that we may further assume that $\\phi(E_{ii})=E_{ii}$ for every $i$. For any off-diagonal coordinate pair $(i,j)$, show that $\\phi(E_{ij})$ is a scalar multiple of $E_{ij}$ ( hint: we have $E_{kk}E_{ij}=0$ for every $k\\ne i$ and $E_{ij}E_{kk}=0$ for every $k\\ne j$). Hence prove that in addition to all the previous assumptions (i.e. $\\phi(E_{ii})=E_{ii}$ and $\\phi(E_{ij})$ is a scalar multiple of $E_{ij}$ for every $i,j$), we may further assume that $\\phi(E_{\\color{red}{1}j})=E_{\\color{red}{1}j}$ for every $j$. Since $\\phi$ preserves invertibility and non-invertibility, prove that $\\phi(E_{ij})=E_{ij}$ for every $(i,j)$.",
    "llm_answer_create_structured_problem": "###BEGIN_OF_FORMAT###\nAssumption: \n$\\phi$ is a linear transformation from $M_{n\\times n}(\\mathbf{R})$ to $M_{n\\times n}(\\mathbf{R})$,\n$\\phi$ is nonzero (there exists a matrix $A$ such that $\\phi(A) \\neq 0$),\n$\\phi(AB) = \\phi(A)\\phi(B)$ for all $A, B \\in M_{n\\times n}(\\mathbf{R})$.\nproblem:\nProve that there exists an invertible matrix $T \\in M_{n\\times n}(\\mathbf{R})$ such that $\\phi(A) = TAT^{-1}$ for all $A \\in M_{n\\times n}(\\mathbf{R})$.\n###",
    "reasoning_create_structured_problem": "Okay, let me try to work through this problem. So, the question is about linear transformations on the space of square matrices that preserve the multiplication structure, meaning φ(AB) = φ(A)φ(B) for all matrices A and B. And we need to show that such a φ must be conjugation by some invertible matrix T. Hmm, right.\n\nFirst, let me recall what a linear homomorphism is here. Since φ is linear and preserves multiplication, it's a ring homomorphism on the matrix algebra. But matrix rings are simple, meaning they don't have non-trivial two-sided ideals. So, any ring homomorphism should be either injective or the zero map. But the problem states that φ is nonzero, so by simplicity, φ must be injective. Wait, does that hold? Wait, if φ is a ring homomorphism and the kernel is an ideal, and since the matrix ring is simple, the kernel must be trivial or the entire ring. Since φ is nonzero, kernel is trivial, so φ is injective. Okay, so φ is an injective linear transformation that preserves multiplication.\n\nBut in the problem statement, φ is given as a linear transformation between the spaces of matrices, which are vector spaces over R. So, φ is a linear operator on the space of matrices, right? So, the dimension of the space is n², so φ is a linear operator on an n²-dimensional space. But since φ is also multiplicative, φ(AB) = φ(A)φ(B), that makes φ a ring homomorphism. \n\nNow, in finite-dimensional algebras, such homomorphisms between simple algebras (like matrix algebras) are often given by conjugation by an invertible matrix. That seems like the conclusion we need. But how to prove that?\n\nLet me think. Suppose there exists an invertible matrix T such that φ(A) = TAT⁻¹. Then φ is clearly linear, multiplicative, and invertible since T is invertible. So, that direction is straightforward. The challenge is the converse: proving that any such φ must be of this form.\n\nSince φ is a linear map, we can represent it as a tensor or something, but maybe there's a smarter way. Let's consider that φ preserves multiplication. So, in particular, φ(I) must be φ(I)φ(I) = φ(I²) = φ(I), so φ(I)² = φ(I). Therefore, φ(I) is an idempotent matrix. But since φ is injective (as we concluded earlier), φ(I) can't be zero (since I is not zero and φ is injective). Therefore, φ(I) is a non-zero idempotent. But if φ is also injective, maybe φ(I) must be invertible? Wait, idempotent and invertible implies that φ(I) is the identity matrix. Because if P² = P and P is invertible, then multiplying both sides by P⁻¹ gives P = I. So, φ(I) = I.\n\nWait, but how do we know φ is injective? Earlier reasoning: since φ is a non-zero homomorphism and the matrix ring is simple, so kernel is trivial. So φ is injective. Therefore, φ(I) is not zero, and φ(I)² = φ(I), so φ(I) is an idempotent. Since φ is injective, the image of φ is a subalgebra isomorphic to the original matrix algebra. But in a matrix algebra, the only non-zero idempotent that is central is the identity matrix. Wait, but φ(I) is in the image, which is a subalgebra. However, if the image is the entire matrix algebra (since φ is injective and the dimensions are the same, assuming finite-dimensional here), so φ is actually an automorphism. Then, by Skolem-Noether theorem, all automorphisms of matrix algebras are inner, which is exactly conjugation by an invertible matrix. Wait, that might be the key.\n\nSkolem-Noether theorem states that any automorphism of a central simple algebra is inner. Since M_n(R) is a central simple algebra over R, then any automorphism is given by conjugation by an invertible matrix. But here, φ is an injective homomorphism (hence an automorphism, since domain and codomain have the same finite dimension). Therefore, φ is an automorphism, and by Skolem-Noether, it's inner. Hence, there exists T invertible such that φ(A) = TAT⁻¹.\n\nBut wait, the problem states that φ is a linear transformation from M_{n x n}(R) to itself, which is non-zero and multiplicative. Then, if φ is injective (as per the simplicity argument), and since the dimension is finite, it's also surjective, hence an automorphism. Then Skolem-Noether applies. But does Skolem-Noether hold over R? I know it holds over fields, and R is a field, so yes. So maybe that's the way.\n\nBut the problem is from a linear algebra textbook, so maybe they expect a more elementary approach without invoking Skolem-Noether. Let me think of how to approach it without that theorem.\n\nSuppose φ: M_n(R) → M_n(R) is a linear map such that φ(AB) = φ(A)φ(B) for all A, B, and φ is non-zero. We need to show that φ is given by conjugation.\n\nFirst, let's note that φ is a homomorphism of R-algebras. The key properties are linearity and multiplicativity.\n\nLet me consider the image of φ. Since φ is multiplicative, the image is a subalgebra of M_n(R). Also, since φ is non-zero, the image is non-trivial. But M_n(R) is simple, so the image must be the entire algebra (since the only ideals are {0} and the whole algebra). But φ is injective? Wait, φ is a linear map between spaces of the same dimension. If the image is the entire space, then φ is surjective, hence invertible. So φ is an automorphism. Then by Skolem-Noether, it's inner. But maybe this is circular, since Skolem-Noether's proof might require similar steps.\n\nAlternatively, let's try to construct T explicitly. Let me think of φ as acting on matrix units. Suppose {E_ij} is the standard basis of matrix units. Then φ(E_ij) must satisfy certain multiplicative properties. For example, E_ij E_kl = δ_jk E_il. Therefore, φ(E_ij) φ(E_kl) = φ(δ_jk E_il). So, unless we can express φ(E_ij) in terms that mimic the matrix units, perhaps conjugated by some matrix.\n\nAlternatively, consider that φ preserves rank or certain properties? Maybe not directly. Alternatively, suppose we can find a matrix T such that φ(A) = TAT^{-1}. Then, for all A, this holds. To find such a T, maybe consider the action of φ on rank 1 matrices. Suppose that for a rank 1 matrix xy^T (where x and y are vectors), φ(xy^T) = T x (Ty)^T? Not sure.\n\nAlternatively, suppose we fix a basis. Let's think of φ as a linear operator on M_n(R). Let me consider that for each A, φ(A) = T A T^{-1}. Then, T is invertible. So, the idea is to show that such a T exists.\n\nAnother approach: Let's consider that φ is an algebra homomorphism, so it preserves the identity element. Wait, but we already deduced that φ(I) is an idempotent. But if φ is injective, then φ(I) must be invertible. Because if φ(I) is idempotent and injective, then φ(I) has full rank? Wait, not necessarily. Wait, if φ is injective, then its image has dimension n². But the algebra generated by φ(I) would be a subalgebra. Hmm, maybe this isn't the right path.\n\nWait, let's suppose that φ(I) = I. If we can show that, then we can maybe proceed. Let me check. Since φ is multiplicative, φ(I) = φ(I * I) = φ(I) φ(I). So φ(I) is idempotent. Suppose φ(I) is some idempotent matrix P. Then, P^2 = P. But if φ is injective, then the kernel is trivial. Suppose φ(I) = P. Then, for any matrix A, φ(A) = φ(A * I) = φ(A) φ(I) = φ(A) P. Similarly, φ(A) = φ(I * A) = φ(I) φ(A) = P φ(A). So, φ(A) = P φ(A) = φ(A) P. So P must commute with all elements in the image of φ. But since φ is injective, its image is the entire M_n(R) (if φ is surjective). Wait, no, injective linear maps in finite-dimensional spaces are not necessarily surjective unless the dimensions are equal. Here, domain and codomain are both M_n(R), which have the same dimension, so injective implies surjective. Therefore, φ is an automorphism, hence surjective. Therefore, the image is M_n(R). So, P commutes with all matrices in M_n(R). Therefore, P is in the center of M_n(R), which is scalar matrices. But P is idempotent, so P must be 0 or I. Since φ is injective, φ(I) cannot be 0 (since I ≠ 0 and kernel is trivial). Therefore, φ(I) = I.\n\nOkay, so φ preserves the identity matrix. Now, since φ is an automorphism, it must preserve the center, which is scalar matrices. So, φ maps scalar matrices to scalar matrices. In fact, for any scalar λ, φ(λ I) = λ φ(I) = λ I. So, scalar matrices are preserved.\n\nNow, let's consider that φ is an automorphism. So, the Skolem-Noether theorem tells us that all automorphisms of M_n(R) are inner, i.e., given by conjugation by some invertible matrix T. Therefore, φ(A) = T A T^{-1} for some invertible T. Hence, that's the result we need.\n\nBut is there a way to prove this without invoking Skolem-Noether? Maybe construct T explicitly. Let me think. Suppose we can find a vector space isomorphism T: R^n → R^n such that φ(A) = T A T^{-1} for all A. How to construct such a T?\n\nAlternatively, note that an automorphism φ of M_n(R) corresponds to an automorphism of the endomorphism algebra of R^n. By the automorphism theorem, such automorphisms are given by conjugation by invertible linear transformations. So, that's exactly the statement. But again, this might be using the same high-level theorem.\n\nAlternatively, choose a basis. Let's think of φ in terms of a basis. Let {E_ij} be the standard matrix units. Then φ(E_ij) must satisfy the same multiplication rules as E_ij, up to similarity. Because φ(E_ij) φ(E_kl) = φ(E_ij E_kl) = δ_jk φ(E_il). So, the images of the matrix units under φ must form a set of matrix units for some basis. Therefore, there exists a basis such that φ(E_ij) = T E_ij T^{-1} for some invertible T. Then, since φ is linear, it would follow that φ(A) = T A T^{-1} for all A.\n\nSo, how to construct such a T? Let's think of φ acting on the matrix units. Let F_ij = φ(E_ij). Then, F_ij F_kl = δ_jk F_il. So, the F_ij form another set of matrix units. In that case, there exists an invertible matrix T such that F_ij = T E_ij T^{-1}. To see this, note that a set of matrix units in M_n(R) is equivalent to a choice of basis. So, if we have another set of matrix units {F_ij}, there exists a basis with respect to which the F_ij are the standard matrix units, which is equivalent to conjugation by T.\n\nTherefore, once we establish that the images of the matrix units under φ form another set of matrix units, then we can conclude that φ is conjugation by T. Hence, the existence of such T follows.\n\nSo, putting this all together, the steps would be:\n\n1. Show that φ is injective (since it's a non-zero homomorphism on a simple algebra).\n2. Therefore, φ is an automorphism (since injective linear maps on finite-dimensional spaces of the same dimension are surjective).\n3. Use the fact that automorphisms of matrix algebras are inner (Skolem-Noether) or construct T by considering the images of matrix units.\n\nTherefore, the problem reduces to applying these steps, and the key theorem is Skolem-Noether. However, in an elementary linear algebra context, one might need to construct T explicitly by considering the action of φ on matrix units or using the preservation of the identity and the structure of matrix multiplication.\n\nSo, to answer the original problem, the assumptions are:\n\n- φ is a linear transformation from M_{n×n}(R) to itself.\n- φ is nonzero (i.e., there exists some matrix A with φ(A) ≠ 0).\n- φ preserves multiplication: φ(AB) = φ(A)φ(B) for all A, B.\n\nThe problem is to show that such φ is conjugation by an invertible matrix T.\n\nTherefore, the formatted problem should list these assumptions line by line and present the problem statement accordingly.",
    "Original_Problem": "Assumption: \n$\\phi$ is a linear transformation from $M_{n\\times n}(\\mathbf{R})$ to $M_{n\\times n}(\\mathbf{R})$,\n$\\phi$ is nonzero (there exists a matrix $A$ such that $\\phi(A) \\neq 0$),\n$\\phi(AB) = \\phi(A)\\phi(B)$ for all $A, B \\in M_{n\\times n}(\\mathbf{R})$.\nproblem:\nProve that there exists an invertible matrix $T \\in M_{n\\times n}(\\mathbf{R})$ such that $\\phi(A) = TAT^{-1}$ for all $A \\in M_{n\\times n}(\\mathbf{R})$.",
    "Original_Problem_with_numerical_assumption": "Assumption:\nAssumption 1: $\\phi$ is a linear transformation from $M_{n\\times n}(\\mathbf{R})$ to $M_{n\\times n}(\\mathbf{R})$,\nAssumption 2: $\\phi$ is nonzero (there exists a matrix $A$ such that $\\phi(A) \\neq 0$),\nAssumption 3: $\\phi(AB) = \\phi(A)\\phi(B)$ for all $A, B \\in M_{n\\times n}(\\mathbf{R})$.\nProblem:\nProve that there exists an invertible matrix $T \\in M_{n\\times n}(\\mathbf{R})$ such that $\\phi(A) = TAT^{-1}$ for all $A \\in M_{n\\times n}(\\mathbf{R})$.",
    "Proof_problem": "Yes",
    "Redundant_assumption": "$Y=\\phi(B)$",
    "Problem_with_redundant_assumption": "Assumption:\nAssumption 1: $\\phi$ is a linear transformation from $M_{n\\times n}(\\mathbf{R})$ to $M_{n\\times n}(\\mathbf{R})$,\nAssumption 2: $\\phi$ is nonzero (there exists a matrix $A$ such that $\\phi(A) \\neq 0$),\nAssumption 3: $\\phi(AB) = \\phi(A)\\phi(B)$ for all $A, B \\in M_{n\\times n}(\\mathbf{R})$.\nAssumption 4: $Y=\\phi(B)$\nProblem:\nProve that there exists an invertible matrix $T \\in M_{n\\times n}(\\mathbf{R})$ such that $\\phi(A) = TAT^{-1}$ for all $A \\in M_{n\\times n}(\\mathbf{R})$."
}