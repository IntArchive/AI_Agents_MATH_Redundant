{
    "Link_API": "https://api.stackexchange.com/2.3/questions/2019122",
    "Title": "unbiased estimate of the covariance",
    "Score": 23,
    "Category": "Chứng minh",
    "Tags": "statistics, covariance",
    "Link": "https://math.stackexchange.com/questions/2019122/unbiased-estimate-of-the-covariance",
    "Content": "How can I prove that\n$$\n\\frac 1 {n-1} \\sum_{i=1}^n (X_i - \\bar X)(Y_i-\\bar Y)\n$$\nis an unbiased estimate of the covariance $\\operatorname{Cov}(X, Y)$\nwhere $\\bar X = \\dfrac 1 n \\sum_{i=1}^n X_i$ and $\\bar Y = \\dfrac 1 n \\sum_{i=1}^n Y_i$ and $(X_1, Y_1), \\ldots ,(X_n, Y_n)$ an independent sample from random vector  $(X, Y)$?",
    "AcceptedAnswer": "Let $\\mu=\\operatorname{E}(X)$ and $\\nu = \\operatorname{E}(Y).$ Then \\begin{align}\n& \\sum_{i=1}^n (X_i - \\bar X)(Y_i-\\bar Y) \\\\[10pt]\n= {} & \\sum_{i=1}^n \\Big( (X_i - \\mu) + (\\mu - \\bar X)\\Big) \\Big((Y_i - \\nu) + (\\nu - \\bar Y)\\Big) \\\\[10pt]\n= {} & \\left( \\sum_i (X_i-\\mu)(Y_i-\\nu) \\right) + \\left( \\sum_i (X_i-\\mu)(\\nu - \\bar Y) \\right) \\\\\n& {} +\\left( \\sum_i (\\mu-\\bar X)(Y_i - \\nu) \\right) + \\left( \\sum_i(\\mu-\\bar X)(\\nu - \\bar Y) \\right).\n\\end{align} The expected value of the first of the four terms above is $$\n\\sum_{i}^n \\operatorname{E}\\big( (X_i-\\mu)(Y_i-\\nu) \\big) = \\sum_{i}^n \\operatorname{cov}(X_i,Y_i) = n\\operatorname{cov}(X,Y).\n$$ The expected value of the second term is \\begin{align}\n& \\sum_i -\\operatorname{cov}(X_i, \\bar Y) = \\sum_i - \\operatorname{cov}\\left(X_i, \\frac {Y_1+\\cdots+Y_n} n  \\right) \\\\[10pt]\n= {} & -n\\operatorname{cov}\\left( X_1, \\frac{Y_1+\\cdots+Y_n} n \\right) = - \\operatorname{cov}(X_1, Y_1+\\cdots +Y_n) \\\\[10pt]\n& = -\\operatorname{cov}(X_1,Y_1) + 0 + \\cdots + 0 = -\\operatorname{cov}(X,Y).\n\\end{align} The third term is similarly that same number. The fourth term is \\begin{align}\n& \\sum_i \\overbrace{\\operatorname{cov}(\\bar X,\\bar Y)}^{\\text{No “} i \\text{'' appears here.}} = n \\operatorname{cov}(\\bar X, \\bar Y) = n \\operatorname{cov}\\left( \\frac 1 n \\sum_i X_i, \\frac 1 n \\sum_i Y_i \\right) \\\\[10pt]\n= {} & n \\cdot \\frac 1 {n^2} \\Big( \\, \\underbrace{\\cdots + \\operatorname{cov}(X_i, Y_j) + \\cdots}_{n^2\\text{ terms}} \\, \\Big).\n\\end{align} This last sum is over all pairs of indices $i$ and $j$ . But the covariances are $0$ except the ones in which $i=j$ . Hence there are just $n$ nonzero terms, and we have $$\nn\\cdot \\frac 1 {n^2} \\left( \\sum_i \\operatorname{cov} (X_i,Y_i) \\right) = n\\cdot \\frac 1 {n^2} \\cdot n \\operatorname{cov}(X,Y) = \\operatorname{cov}(X,Y).\n$$ I leave the rest as an exercise.",
    "llm_answer_create_structured_problem": "###BEGIN_OF_FORMAT###\nAssumption: \n$(X_1, Y_1), (X_2, Y_2), \\ldots, (X_n, Y_n)$ are independent,\n$(X_i, Y_i)$ is a sample from the random vector $(X, Y)$ for all $i = 1, 2, \\ldots, n$,\n$\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i$,\n$\\bar{Y} = \\frac{1}{n} \\sum_{i=1}^n Y_i$.\nProblem:\nProve that $$\\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})$$ is an unbiased estimator of $\\operatorname{Cov}(X, Y)$.\n###END_OF_FORMAT###",
    "reasoning_create_structured_problem": "Okay, let's see. The user wants me to act as a bilingual math professor and structure the given Math Stack Exchange question into the specified format. The example they provided shows breaking down the assumptions and the problem statement clearly. \n\nFirst, I need to look at the problem from Math Stack Exchange. The question is about proving that the sample covariance is an unbiased estimator of the population covariance. The formula given is (1/(n-1)) times the sum of (X_i - X̄)(Y_i - Ȳ) for i from 1 to n. The user mentions that the sample consists of independent observations from the random vector (X, Y). \n\nNow, the task is to filter out the unrelated information and list the assumptions line by line, each addressing one property or condition applied to one object. Then, restate the problem concisely.\n\nLooking at the original problem: The key assumptions here are that we have a random sample, which is independent. Each pair (X_i, Y_i) is an independent sample from the same random vector (X, Y). Also, the definitions of X̄ and Ȳ are given as the sample means. \n\nSo, the assumptions should include:\n1. (X_1, Y_1), ..., (X_n, Y_n) are independent samples. Each of these pairs is an independent observation.\n2. Each pair (X_i, Y_i) is a sample from the random vector (X, Y). So, they have the same distribution as (X,Y).\n3. X̄ is the sample mean of the X_i's, defined as (1/n) sum X_i.\n4. Similarly, Ȳ is the sample mean of the Y_i's, defined as (1/n) sum Y_i.\n\nThese are the individual assumptions. Each line addresses one condition. The problem then is to prove that the given expression is an unbiased estimator of Cov(X,Y).\n\nI need to ensure that each assumption is a single point. Let me check again. The independence of the samples, the identical distribution (since they are from the same vector), and the definitions of the sample means. That should cover all necessary assumptions. The original problem mentions \"independent sample\", so that's the key. Also, the problem doesn't specify any particular conditions on X and Y other than being part of a random vector, so maybe no other assumptions like finite variance are needed? But since covariance is being discussed, it's implied that variances and covariances exist, but the user might not have stated that. However, in the example provided, the assumptions were only the given ones, not implicit ones. So perhaps we shouldn't add assumptions about finite variances unless explicitly stated. The original question here doesn't mention that, so maybe we shouldn't include it.\n\nTherefore, the assumptions should be as listed. The problem statement is to prove unbiasedness of the sample covariance estimator. So structuring it as per the format.",
    "Original_Problem": "Assumption: \n$(X_1, Y_1), (X_2, Y_2), \\ldots, (X_n, Y_n)$ are independent,\n$(X_i, Y_i)$ is a sample from the random vector $(X, Y)$ for all $i = 1, 2, \\ldots, n$,\n$\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i$,\n$\\bar{Y} = \\frac{1}{n} \\sum_{i=1}^n Y_i$.\nProblem:\nProve that $$\\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})$$ is an unbiased estimator of $\\operatorname{Cov}(X, Y)$.",
    "Original_Problem_with_numerical_assumption": "Assumption:\nAssumption 1: $(X_1, Y_1), (X_2, Y_2), \\ldots, (X_n, Y_n)$ are independent,\nAssumption 2: $(X_i, Y_i)$ is a sample from the random vector $(X, Y)$ for all $i = 1, 2, \\ldots, n$,\nAssumption 3: $\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i$,\nAssumption 4: $\\bar{Y} = \\frac{1}{n} \\sum_{i=1}^n Y_i$.\nProblem:\nProve that $$\\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})$$ is an unbiased estimator of $\\operatorname{Cov}(X, Y)$.",
    "Proof_problem": "Yes",
    "Redundant_assumption": "$$\n\\sum_{i}^n \\operatorname{E}\\big( (X_i-\\mu)(Y_i-\\nu) \\big) = \\sum_{i}^n \\operatorname{cov}(X_i,Y_i) = n\\operatorname{cov}(X,Y).\n$$",
    "Problem_with_redundant_assumption": "Assumption:\nAssumption 1: $(X_1, Y_1), (X_2, Y_2), \\ldots, (X_n, Y_n)$ are independent,\nAssumption 2: $(X_i, Y_i)$ is a sample from the random vector $(X, Y)$ for all $i = 1, 2, \\ldots, n$,\nAssumption 3: $\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i$,\nAssumption 4: $\\bar{Y} = \\frac{1}{n} \\sum_{i=1}^n Y_i$.\nAssumption 5: $$\n\\sum_{i}^n \\operatorname{E}\\big( (X_i-\\mu)(Y_i-\\nu) \\big) = \\sum_{i}^n \\operatorname{cov}(X_i,Y_i) = n\\operatorname{cov}(X,Y).\n$$\nProblem:\nProve that $$\\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})$$ is an unbiased estimator of $\\operatorname{Cov}(X, Y)$."
}