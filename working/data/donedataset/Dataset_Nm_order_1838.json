{
    "Link_API": "https://api.stackexchange.com/2.3/questions/117702",
    "Title": "Orthogonal projections with $\\sum P_i =I$, proving that $i\\ne j \\Rightarrow  P_{j}P_{i}=0$",
    "Score": 17,
    "Category": "Chứng minh",
    "Tags": "linear-algebra, quantum-mechanics",
    "Link": "https://math.stackexchange.com/questions/117702/orthogonal-projections-with-sum-p-i-i-proving-that-i-ne-j-rightarrow-p",
    "Content": "I am reading Introduction to Quantum Computing by Kaye, Laflamme, and Mosca. As an exercise, they write \"Prove that if the operators $P_{i}$ satisfy $P_{i}^{*}=P_{i}$ and $P_{i}^{2}=P_{i}$ , then $P_{i}P_{j}=0$ for $i\\ne j$.'' In the context of this problem, it has been assumed that $I=\\sum_{i=1}^{n} P_{i}$, where I suppose that $n$ could be infinite. I have shown that this is true in the trivial case $n=2$, but the general case has been eluding me. How should I attack this?",
    "AcceptedAnswer": "For each $j$,\n$$P_j=P_jIP_j=P_j\\left(\\sum_{k=1}^n P_k\\right)P_j=\\sum_{k=1}^nP_jP_kP_j=P_j+\\sum_{k\\neq j}P_jP_kP_j,$$ so $\\sum\\limits_{k\\neq j}P_jP_kP_j=0$.  For each $i\\neq j$, $P_jP_iP_j=(P_iP_j)^*P_iP_j$ is a positive operator, and a sum of positive operators is positive, so $-P_jP_iP_j=\\sum\\limits_{k\\neq i,j}P_jP_kP_j$ is also positive.  This is only possible if $P_jP_iP_j=0$.  Since $\\|P_iP_j\\|^2=\\|(P_iP_j)^*P_iP_j\\|=\\|P_jP_iP_j\\|$, it follows that $P_iP_j=0$.",
    "llm_answer_create_structured_problem": "###BEGIN_OF_FORMAT###\nAssumption: \nEach operator $P_i$ satisfies $P_i^* = P_i$,\nEach operator $P_i$ satisfies $P_i^2 = P_i$,\nThe operators $P_i$ satisfy $\\sum_{i=1}^{n} P_i = I$,\n$i$ and $j$ are distinct indices.\nProblem:\nProve that $P_i P_j = 0$ for $i \\ne j$.\n###END_OF_FORMAT###",
    "reasoning_create_structured_problem": "Okay, let's try to tackle this problem. So, we have these operators P_i that are supposed to be orthogonal projections. The conditions given are that each P_i is self-adjoint (P_i* = P_i) and idempotent (P_i² = P_i). Also, their sum equals the identity operator, I. The goal is to prove that for any i ≠ j, the product P_iP_j is zero.\n\nFirst, I need to recall what orthogonal projections are. An orthogonal projection is a linear operator that satisfies P* = P and P² = P. So each P_i here is indeed an orthogonal projection. The key property is that they project vectors onto a subspace, and they are orthogonal in the sense that their ranges are orthogonal to each other.\n\nThe main condition here is that the sum of all these projections equals the identity operator. So, for any vector v in the Hilbert space, applying all P_i's and summing them up gives back v. This seems like a decomposition of the space into orthogonal subspaces. \n\nLet me think about the case when n=2, which the user mentioned they solved. If there are two projections P₁ and P₂ such that P₁ + P₂ = I, then multiplying both sides by P₁, we get P₁² + P₁P₂ = P₁. Since P₁² = P₁, this simplifies to P₁ + P₁P₂ = P₁, which implies P₁P₂ = 0. Similarly, multiplying by P₂ gives P₂P₁ = 0. So, in the n=2 case, it works out.\n\nNow, how to generalize this to n projections? Let's take arbitrary i and j with i ≠ j. We need to show that P_iP_j = 0. Let's try the same approach. Let's consider the sum of all P_k's equal to I. So, Σ_{k=1}^n P_k = I. Let's fix i and j. Let's multiply both sides of the equation by P_i on the left and P_j on the right. Then, we have P_i (Σ P_k) P_j = P_i I P_j = P_i P_j. On the left side, expanding the sum, each term becomes P_i P_k P_j. \n\nBut for k ≠ i and k ≠ j, we don't know much. Wait, maybe a different approach. Let me instead consider multiplying both sides by P_i on the left. So, Σ_{k=1}^n P_i P_k = P_i I = P_i. But since P_i is a projection, P_i² = P_i. So, the sum becomes P_i + Σ_{k≠i} P_i P_k = P_i. Subtracting P_i from both sides, we get Σ_{k≠i} P_i P_k = 0. \n\nHmm, this gives a sum of operators equal to zero. But we need to show each individual term P_i P_k is zero. Since all terms in the sum are operators, and their sum is zero, does that imply each term is zero? Not necessarily, unless we have some additional conditions. For example, if all the terms are positive semi-definite operators, then their sum being zero would imply each is zero. Are these products positive semi-definite?\n\nWait, P_i and P_k are projections. Let's consider P_i P_k. If we take the adjoint, (P_i P_k)* = P_k* P_i* = P_k P_i, since they are self-adjoint. So, P_i P_k is self-adjoint only if P_i and P_k commute. Otherwise, it's not necessarily self-adjoint. So, maybe they aren't positive semi-definite. Therefore, the sum being zero might not force each term to be zero.\n\nAlternative approach: Let's use the fact that the projections are orthogonal. Let's take two different projections P_i and P_j. Since Σ P_k = I, then P_i + P_j + Σ_{k≠i,j} P_k = I. Let's rearrange this as P_i + P_j = I - Σ_{k≠i,j} P_k. Now, since all P_k are projections, the right-hand side is also a projection if the sum of the remaining projections is a projection. But I'm not sure if that helps.\n\nWait, maybe we can use the property that for orthogonal projections, the product P_i P_j is zero if and only if their ranges are orthogonal. So, if we can show that the ranges of P_i and P_j are orthogonal subspaces, then their product is zero. Since the sum of all projections is the identity, the entire space is the direct sum of the ranges of the P_k's. If these ranges are orthogonal, then the projections must commute and their products are zero. But how to show that?\n\nAlternatively, maybe use the fact that if the sum of projections is the identity, then the projections are mutually orthogonal. Let's suppose that for some i ≠ j, P_i P_j ≠ 0. Then there exists a vector v such that P_i P_j v ≠ 0. Let's denote w = P_j v. Then P_i w = w since w is in the range of P_j. Wait, no. If P_i and P_j are projections, but not necessarily orthogonal, then P_i P_j v might not be in the range of either. This might get complicated.\n\nWait, perhaps take the trace. If we take the trace of both sides of Σ P_k = I, then trace(Σ P_k) = trace(I). The trace of the identity operator on a finite-dimensional space is the dimension. But the problem doesn't specify finite-dimensional. Hmm, but if n is infinite, the trace might not be defined. So maybe this approach isn't general.\n\nAlternatively, let's square both sides of the equation Σ P_k = I. Then, (Σ P_k)^2 = I^2 = I. Expanding the left side, we get Σ P_k^2 + Σ_{i≠j} P_i P_j = I. But since each P_k is a projection, P_k^2 = P_k. Therefore, Σ P_k + Σ_{i≠j} P_i P_j = I. But we already know that Σ P_k = I, so substituting that in, we get I + Σ_{i≠j} P_i P_j = I. Therefore, Σ_{i≠j} P_i P_j = 0. So, the sum of all products of distinct projections is zero.\n\nNow, if all the terms P_i P_j are zero, then their sum is zero. But the converse isn't necessarily true unless we have some additional structure. However, if we can show that each term P_i P_j is a positive operator, then their sum being zero would imply each is zero. But are these products positive?\n\nWait, if we take P_i and P_j, then P_i P_j is not necessarily self-adjoint unless they commute. Because (P_i P_j)* = P_j P_i. So unless P_i and P_j commute, P_i P_j is not self-adjoint. Therefore, it's not necessarily a positive operator. So that approach might not work.\n\nAlternatively, let's consider the operator P_i P_j P_i. Since P_j is self-adjoint, P_i P_j P_i is a positive operator because for any vector v, ⟨v, P_i P_j P_i v⟩ = ⟨P_i v, P_j P_i v⟩ ≥ 0, since P_j is a projection (hence positive). Similarly, since P_i is a projection, this is non-negative. So, P_i P_j P_i is positive semi-definite. Then, if we sum over all j ≠ i, we have Σ_{j≠i} P_i P_j P_i. But from the previous result, Σ_{j≠i} P_j = I - P_i. So, Σ_{j≠i} P_i P_j P_i = P_i (I - P_i) P_i = P_i (P_i - P_i^2) = P_i (P_i - P_i) = 0. Therefore, the sum of these positive operators is zero, hence each term must be zero. Therefore, P_i P_j P_i = 0 for all j ≠ i. \n\nSince P_i P_j P_i is zero, taking the trace, trace(P_i P_j P_i) = trace(P_j P_i P_i) = trace(P_j P_i) = 0. But trace(P_j P_i) = trace(P_i P_j). So, trace(P_i P_j) = 0. Since P_i and P_j are projections, their product has trace equal to the dimension of the intersection of their ranges. If the trace is zero, that intersection must be trivial. But this is in finite dimensions. However, the problem may be in infinite dimensions. Hmm, but the trace argument might not hold there.\n\nAlternatively, from P_i P_j P_i = 0, we can take the operator norm. If P_i P_j P_i = 0, then for any vector v, ⟨v, P_i P_j P_i v⟩ = 0. This implies that ||P_j P_i v||² = 0, so P_j P_i v = 0 for all v. Therefore, P_j P_i = 0. Then, taking adjoint, P_i P_j = 0 as well. Hence, P_i P_j = 0 for all i ≠ j. That seems to work.\n\nSo, to recap: Starting from Σ P_k = I, square both sides to get Σ P_k + Σ_{i≠j} P_i P_j = I. Then subtract Σ P_k (which is I) to get Σ_{i≠j} P_i P_j = 0. Then consider for each i, multiply by P_i on both sides and take appropriate products to show each term is zero.\n\nAlternatively, the more straightforward path is as follows:\n\nGiven that Σ P_k = I, multiply both sides by P_i:\n\nP_i Σ P_k = P_i I → Σ P_i P_k = P_i.\n\nBut since P_i P_k is part of the sum, and when k = i, we have P_i² = P_i. Therefore, P_i + Σ_{k≠i} P_i P_k = P_i. Subtracting P_i from both sides gives Σ_{k≠i} P_i P_k = 0.\n\nNow, if we can show that each term in this sum is zero, then we are done. But how? The sum being zero doesn't immediately imply each term is zero unless we have orthogonality. However, if we consider that for projections, the product P_i P_k is also a projection (but only if they commute). However, in general, the product of two projections is not a projection unless they commute. So this might not hold.\n\nBut wait, perhaps take the adjoint of the equation Σ_{k≠i} P_i P_k = 0. Taking the adjoint gives Σ_{k≠i} P_k P_i = 0. So, combining the two equations:\n\nΣ_{k≠i} P_i P_k = 0 and Σ_{k≠i} P_k P_i = 0.\n\nBut this still doesn't directly help unless we can consider individual terms. However, earlier when we considered multiplying by P_i on the left and P_j on the right, we saw that P_i P_j must be zero. Let's try that.\n\nMultiply the original equation Σ P_k = I by P_i on the left and P_j on the right (where i ≠ j):\n\nP_i (Σ P_k) P_j = P_i I P_j → Σ P_i P_k P_j = P_i P_j.\n\nBut on the left side, most terms are zero except when k = j, because P_k P_j = P_j when k = j, and for k ≠ j, it's P_k P_j. Wait, no, actually, it's P_i multiplied by P_k then by P_j. So:\n\nFor each term in the sum, P_i P_k P_j. If k ≠ j, then P_k P_j is some operator, but we don't know. However, when k = j, the term is P_i P_j P_j = P_i P_j. So, Σ_k P_i P_k P_j = P_i P_j + Σ_{k≠j} P_i P_k P_j = P_i P_j. But the left side is also equal to P_i P_j. Therefore, this gives us no new information. Hmm.\n\nMaybe another approach: Let's take two distinct indices i and j. Consider the operator P_i + P_j. Since all P_k are positive operators (as projections), their sum is also positive. But I don't know if that helps.\n\nWait, let's recall that in the case of orthogonal projections, if their sum is also a projection, then they must be orthogonal. But here, the sum of all P_k is the identity, which is a projection. However, the sum of two projections is a projection only if they are orthogonal. But this is for the sum of all of them, not just two. So maybe this line of thought isn't useful.\n\nWait, but suppose we have P_i and P_j. Since Σ P_k = I, then P_i + P_j = I - Σ_{k≠i,j} P_k. The right-hand side is a projection if the sum of the remaining projections is a projection. But unless the remaining projections are orthogonal among themselves, their sum might not be a projection. So this might not hold. Therefore, maybe P_i + P_j is not a projection, which complicates things.\n\nAlternative idea: Let's use the fact that for any vector v, applying all projections and summing gives back v. So, for any v, Σ P_k v = v. If we apply P_i to both sides, we get Σ P_i P_k v = P_i v. Since P_i P_k v is the projection of P_k v onto the range of P_i. If i ≠ k, then perhaps these projections are orthogonal. But how to formalize this.\n\nWait, take a vector v in the range of P_i. Then P_i v = v. Applying the sum Σ P_k v = v. Since v is in the range of P_i, P_k v for k ≠ i must be orthogonal to v. Because otherwise, their sum would have components outside the range of P_i. But since the sum equals v, which is entirely in the range of P_i, all other terms P_k v for k ≠ i must be orthogonal to v. Therefore, for any v in the range of P_i, P_k v is orthogonal to v for k ≠ i. Hence, ⟨v, P_k v⟩ = 0 for all k ≠ i. But since P_k is self-adjoint, ⟨v, P_k v⟩ = ⟨P_k v, v⟩ = ⟨v, P_k v⟩*, which implies it's real. But since it's zero, we have that P_k v is orthogonal to v. \n\nBut how does this help us show that P_i P_k = 0? Let's take a vector w and apply P_i P_k to it. Let's denote u = P_k w. Then P_i u = P_i P_k w. We need to show that this is zero for all w. Suppose that u is in the range of P_k. Then, applying P_i to u, we need to get zero. From the previous result, if u is in the range of P_k (k ≠ i), then for any v in the range of P_i, ⟨v, u⟩ = 0. Therefore, the range of P_k is orthogonal to the range of P_i. Hence, P_i u = 0 for any u in the range of P_k. Therefore, P_i P_k w = 0 for any w. Hence, P_i P_k = 0.\n\nTherefore, the ranges of P_i and P_k are orthogonal subspaces, hence their projections commute and their product is zero. This would imply that P_i P_j = 0 for i ≠ j.\n\nSo, putting this together: Since the sum of the projections is the identity, any vector can be written as the sum of its projections onto the ranges of the P_k. These projections must be orthogonal because the sum of non-orthogonal projections would result in a vector with a norm greater than the original (by the Pythagorean theorem), but here the sum equals the original vector, implying that all cross terms (inner products) between different projections must be zero. Hence, the ranges are orthogonal, so the projections must commute and their product is zero.\n\nTherefore, the key steps are:\n\n1. Start with Σ P_k = I.\n\n2. Square both sides to get Σ P_k + Σ_{i≠j} P_i P_j = I.\n\n3. Subtract Σ P_k to find that Σ_{i≠j} P_i P_j = 0.\n\n4. Use positivity or properties of projections to conclude each term must be zero.\n\nAlternatively, use the vector decomposition and orthogonality of ranges to conclude P_i P_j = 0.\n\nThis seems to cover the necessary logic. Now, translating this into a structured answer.",
    "Original_Problem": "Assumption: \nEach operator $P_i$ satisfies $P_i^* = P_i$,\nEach operator $P_i$ satisfies $P_i^2 = P_i$,\nThe operators $P_i$ satisfy $\\sum_{i=1}^{n} P_i = I$,\n$i$ and $j$ are distinct indices.\nProblem:\nProve that $P_i P_j = 0$ for $i \\ne j$.",
    "Original_Problem_with_numerical_assumption": "Assumption:\nAssumption 1: Each operator $P_i$ satisfies $P_i^* = P_i$,\nAssumption 2: Each operator $P_i$ satisfies $P_i^2 = P_i$,\nAssumption 3: The operators $P_i$ satisfy $\\sum_{i=1}^{n} P_i = I$,\nAssumption 4: $i$ and $j$ are distinct indices.\nProblem:\nProve that $P_i P_j = 0$ for $i \\ne j$.",
    "Proof_problem": "Yes",
    "Redundant_assumption": "$$P_j=P_jIP_j=P_j\\left(\\sum_{k=1}^n P_k\\right)P_j=\\sum_{k=1}^nP_jP_kP_j=P_j+\\sum_{k\\neq j}P_jP_kP_j,$$",
    "Problem_with_redundant_assumption": "Assumption:\nAssumption 1: Each operator $P_i$ satisfies $P_i^* = P_i$,\nAssumption 2: Each operator $P_i$ satisfies $P_i^2 = P_i$,\nAssumption 3: The operators $P_i$ satisfy $\\sum_{i=1}^{n} P_i = I$,\nAssumption 4: $i$ and $j$ are distinct indices.\nAssumption 5: $$P_j=P_jIP_j=P_j\\left(\\sum_{k=1}^n P_k\\right)P_j=\\sum_{k=1}^nP_jP_kP_j=P_j+\\sum_{k\\neq j}P_jP_kP_j,$$\nProblem:\nProve that $P_i P_j = 0$ for $i \\ne j$."
}