{
    "Link_API": "https://api.stackexchange.com/2.3/questions/612880",
    "Title": "Scalar product and uniform convergence of polynomials",
    "Score": 22,
    "Category": "Chứng minh",
    "Tags": "real-analysis, ordinary-differential-equations, orthogonal-polynomials",
    "Link": "https://math.stackexchange.com/questions/612880/scalar-product-and-uniform-convergence-of-polynomials",
    "Content": "Given two functions $u$ and $v$, you can compute $(u|v) = \\int_0^1 (u(t)'-u(t))(v(t)'-v(t)) \\,\\mathrm{d}t$. It resembles the scalar product $\\int_{-1}^1 u(t)v(t) \\,\\mathrm{d}t$, which leads to Legendre polynomials , with a major difference: while the latter is a scalar product over continuous functions, the former is not, since for $f(t)=e^t$, $(f|f)=0$. However, it's obviously a scalar product over polynomials (the solutions of $(f|f)=0$ are not polynomials, and all other defining properties of a scalar product are trivial). We can therefore find an orthonormal basis of polynomials, using Gram-Schmidt orthogonalization on the basis $[1,t,t^2,...]$. Let's call $P_n$ this orthonormal basis. Now, after doing this numerically with a CAS, and after some plots, it looks like $\\frac{P_n}{P_n(0)}$ tends uniformly to the function $t \\to e^t$ (on $[0,1]$), a solution of $y'-y=0$, which is closely related to the definition of the scalar product above. But I don't know how to prove this. Any idea? Also, if I do the same with $(u|v)=\\int_0^1 (u''(t)-u(t))(v''(t)-v(t)) \\,\\mathrm{d}t$, I'm not absolutely sure, but numerically it looks like, up to a constant factor $P_{2n} \\to \\cos t$ and $P_{2n+1} \\to \\sin t$, uniformly on $[0,1]$, so the orthogonal basis of polynomials leads to a basis of solutions of $y''-y=0$. I'd like to know if the process can be generalized to other linear differential equations (provided they don't have polynomial solutions). Does it work with equations with nonconstant coefficients? This is not homework. It's something I discovered while playing with a Ti89 long ago (initially, to test a program doing Gram-Schmidt orthogonalization), and I have never been able to prove or disprove the numerical evidence. $\\color{red}{\\textbf{Now, why this question?}}$ Apart from mathematical curiosity, there may be some motivation: the method seems quite robust, there is convergence, probably under certain conditions, for several weight functions and differential equations. And we get uniform approximation of solutions of linear differential equations, by polynomials, which may be quite useful in practice. Since many linear ODE give rise to special functions, that would give a cheap way to compute uniform approximations of their solutions, on arbitrary (but bounded) intervals. To help visualize, here are some sample graphics done with Maxima, computed with a straightforward Gram-Schmidt algorithm. The two following are $P_4(x)/P_4(0)-\\exp(x)$ and $P_9(x)/P_9(0)-\\exp(x)$, in the case $(u|v) = \\int_0^1 (u(t)'-u(t))(v(t)'-v(t)) \\,\\mathrm{d}t$. The division by $P_n(0)$ is only done in order to \"normalize\" polynomials, since $\\exp 0=1$. Now example with \"trigonometric\" differential equation $y''+y=0$. And $(u|v) = \\int_0^{2\\pi} (u(t)''+u(t))(v(t)''+v(t)) \\,\\mathrm{d}t$. Notice the integration bound differ from the preceding, and now uniform convergence of $P_n$ takes place on $[0,2\\pi]$. The normalization is here $P_{2n}(x)/P_{2n}(0)$ as $\\cos 0 = 1$ ans $P_{2n+1}(x)/P_{2n+1}(\\pi/2)$, as $\\sin \\pi/2=1$, and precisely, $P_{8}(x)/P_{8}(0)-\\cos x$ and $P_{9}(x)/P_{9}(\\pi/2)-\\sin x$ are respectively plotted. Notice, it's also possible to add a weighting function to our scalar product, e.g. $(u|v) = \\int_0^1 (u(t)'-u(t))(v(t)'-v(t)) \\mathcal{W}(t)\\,\\mathrm{d}t$. For example, with$\\mathcal{W}(t)=\\frac{1}{\\sqrt{t(1-t)}}$ on $[0,1]$, there is still convergence, and for example $P_9(x)/P_9(0) -\\exp x$ yields the following plot.",
    "AcceptedAnswer": "We prove the conjecture using the following more general formula.\nSuppose $Y_n$ are orthogonal polynomials with respect to the\ninner product on $[t_0,t_1]$ weighted by $w$:\n$$\n(f,g) = \\int_{t=t_0}^{t_1} f(t) \\, g(t) \\, w(t)\n$$\nThen (up to a constant factor)\n$$\n\\Upsilon_n(t) = e^t \\int_t^\\infty e^{-x} Y_n(x) dx\n$$\nare polynomials of degree $n$ orthogonal with respect to the inner product\n$$\n(f|g) = \\int_{t=t_0}^{t_1} (f'(t)-f(t)) \\, (g'(t)-g(t)) \\, w(t).\n$$\nNote that the formula for $\\Upsilon_n$ has the equivalent form\n$$\n\\Upsilon_n(t) = e^t \\Bigl( C_n - \\int_0^t e^{-x} Y_n(x) dx \\Bigr)\n$$\nwhere $C_n = \\Upsilon_n(0) = \\int_0^\\infty e^{-x} Y_n(x) dx$. In our setting, $t_0=0$, $t_1=1$, $w(t)=1$, and the $Y_n$ are shifted\nLegendre polynomials .\nIt is known that $Y_n$ has leading coefficient $2n\\choose n$\nand satisfies $|Y_n(t)| \\leq 1$ for $0 \\leq t \\leq 1$.\nThus $\\left|\\int_0^t e^{-x} Y_n(x) dx \\right| < 1$ for $0 \\leq t \\leq 1$.\nBut $C_n$ grows quickly with $n$: by general properties of orthogonal\npolynomials, all the roots of $Y_n$ are real and in $(0,1)$;\ntherefore $Y_n(x) > {2n\\choose n}(x-1)^n$ for $x>1$, whence\n$\\int_1^\\infty e^{-x} Y_n(x) dx > {2n\\choose n} n!/e$,\nand $C_n$ is within $1$ of this.  Therefore $C_n \\rightarrow\\infty$\nas $n \\rightarrow \\infty$ (numerically $C_4 = 1001$ and\n$C_9 = 10622799089$; this agrees with OEIS sequence A002119 up to a factor $(-1)^n$).  Hence\n$$\n\\sup_{0 \\leq t \\leq 1} \\left| \\frac{\\Upsilon_n(t)}{\\Upsilon_n(0)} - e^t \\right|\n< 1/C_n\n$$\ndecays rapidly to zero. To prove the formula for $\\Upsilon_n$, we observe that\n$(u, \\Upsilon'_n - \\Upsilon_n) = 0$ for every $u$ of the form $f'-f$\nwith $f \\in {\\cal P}_n$, the vector space of polynomials of degree $<n$.\nBut the space of such $f'-f$ is just ${\\cal P}_n$ itself (because the\nlinear map $f \\mapsto f' - f$ of ${\\cal P}_n$ has triangular matrix\nwith nonzero diagonal entries; or alternatively because it has kernel zero).\nTherefore $\\Upsilon'_n - \\Upsilon_n$ is a scalar multiple of $Y_n$.\nThe integral formula for $\\Upsilon_n$ is then obtained by solving\nthe differential equation $\\Upsilon'_n - \\Upsilon_n = -Y_n$:\nmultiply by the integrating factor $e^{-t}$ to get\n$\\frac{d}{dt}(e^{-t}\\Upsilon_n(t)) = -e^{-t} Y_n(t)$, and note that\n$e^{-t}\\Upsilon_n(t) \\rightarrow 0$ as $t \\rightarrow \\infty$\nbecause $\\Upsilon_n$ is a polynomial.",
    "llm_answer_create_structured_problem": "###BEGIN_OF_FORMAT###\nAssumption: \nThe scalar product is defined as $(u|v) = \\int_0^1 (u'(t) - u(t))(v'(t) - v(t)) \\, dt$ over polynomials,\nThe scalar product is positive definite over the space of polynomials,\nAn orthonormal basis $\\{P_n\\}$ is constructed using Gram-Schmidt orthogonalization on the monomial basis $\\{1, t, t^2, \\ldots\\}$,\nThe function $e^t$ is a solution to the differential equation $y' - y = 0$,\nThe domain of consideration is the interval $[0,1]$,\nNormalization is performed by dividing each $P_n$ by $P_n(0)$.\nProblem: \nProve that the normalized polynomials $\\frac{P_n}{P_n(0)}$ converge uniformly to $e^t$ on $[0,1]$, and determine if this process generalizes to other linear differential equations, including those with non-constant coefficients.\n###",
    "reasoning_create_structured_problem": "Okay, let me try to parse this Math Stack Exchange question. The user is talking about scalar products and uniform convergence of polynomials related to differential equations. The main problem seems to be about proving that the orthonormal polynomials obtained through Gram-Schmidt process, when normalized by their value at 0, converge uniformly to solutions of certain differential equations. \n\nFirst, the user defines a scalar product: (u|v) = integral from 0 to 1 of (u'(t) - u(t))(v'(t) - v(t)) dt. They mention that this isn't a scalar product over all continuous functions because for f(t)=e^t, (f|f)=0, but it is a scalar product over polynomials. Then using Gram-Schmidt on the basis [1, t, t^2, ...], they get an orthonormal basis P_n. Numerically, when they take P_n divided by P_n(0), it seems to converge uniformly to e^t on [0,1]. Similarly, for another scalar product involving second derivatives, they observe convergence to sine and cosine functions. The user wants to prove this convergence and generalize it to other linear differential equations, possibly with non-constant coefficients.\n\nSo, the key assumptions here are the specific scalar product definitions, the Gram-Schmidt process applied to polynomials, the normalization by P_n(0), and the domain [0,1]. Also, the differential equations involved are y' - y = 0 and y'' + y = 0. The problem is to prove uniform convergence of the normalized polynomials to the solutions of these ODEs and see if this method can be generalized.\n\nI need to structure this into the required format. The assumptions should list each condition or property applied to one object. Let's break it down:\n\nAssumptions:\n1. The scalar product is defined as $(u|v) = \\int_0^1 (u'(t) - u(t))(v'(t) - v(t)) \\, dt$.\n2. The scalar product is considered over the space of polynomials.\n3. An orthonormal basis $\\{P_n\\}$ is constructed using Gram-Schmidt orthogonalization on the basis $\\{1, t, t^2, \\ldots\\}$.\n4. The normalized polynomial $\\frac{P_n}{P_n(0)}$ is considered.\n5. The domain of interest is the interval $[0, 1]$.\n6. The differential equation $y' - y = 0$ has the solution $e^t$.\n7. Another scalar product $(u|v) = \\int_0^{2\\pi} (u''(t) + u(t))(v''(t) + v(t)) \\, dt$ leads to solutions of $y'' + y = 0$.\n8. The question considers generalization to other linear differential equations without polynomial solutions, possibly with non-constant coefficients.\n\nProblem: Prove that $\\frac{P_n}{P_n(0)}$ converges uniformly to $t \\mapsto e^t$ on $[0, 1]$. Investigate whether the process generalizes to other linear differential equations, including those with non-constant coefficients.\n\nWait, the user also mentioned the trigonometric case with [0,2π] and convergence there. But the main problem seems to be about the first case (exponential) and then generalization. Since the problem part should capture the main question. Let me check the original question again.\n\nOriginal problem's main question is: \"Any idea?\" on proving the uniform convergence to e^t. Then, they want to know if the process can be generalized to other linear differential equations, especially those without polynomial solutions and with non-constant coefficients. So the problem has two parts: proving convergence for the first scalar product case and generalizing the method.\n\nBut according to the example given by the user, the problem part should be concise. Let me structure the problem part to focus on the primary question first. The user starts with the scalar product leading to e^t, so the main problem is to prove the uniform convergence of P_n / P_n(0) to e^t on [0,1]. Then the second part is about generalization. But maybe split into two problems? However, the user's example only has one problem statement. Hmm.\n\nWait, the example given by the user had the problem part as \"Prove that A(n)... is divisible by 91.\" So in the original question here, the problem is to prove the uniform convergence and also address the generalization. However, the user's own example didn't split into multiple problems. Maybe the main problem is the first one, and the rest are additional inquiries. Alternatively, the problem part can encapsulate all the questions posed.\n\nAlternatively, since the user asks two main things: 1) Prove uniform convergence of P_n / P_n(0) to e^t. 2) Generalize to other ODEs. But according to the example, the problem part is a single statement. So perhaps the problem is to prove the uniform convergence and determine if the process can be generalized. Let me check the original question's wording: \"Any idea? Also, if I do the same... I'd like to know if the process can be generalized...\"\n\nTherefore, the problem part should include both parts: proving the convergence for the first case and generalizing. But to fit the format, which requires \"the problem\" in one statement. Maybe:\n\nProblem: Prove that $\\frac{P_n}{P_n(0)}$ converges uniformly to $t \\mapsto e^t$ on $[0,1]$, and investigate whether this process generalizes to other linear differential equations, including those with non-constant coefficients.\n\nBut the user also mentioned another case with y''+y=0, leading to sine and cosine. However, the primary problem seems to be the first part. The other parts are additional questions. But since the user is asking for both, perhaps the problem statement should mention both. Alternatively, since the example given by the user was concise, maybe focus on the first part. Wait, the original question's title is \"Scalar product and uniform convergence of polynomials\". The main problem is the convergence, and the rest are related observations. Let me see the original question again.\n\nOriginal problem says: \"But I don't know how to prove this. Any idea?\" referring to the convergence to e^t. Then the user mentions another case with trigonometric functions and asks if the process can be generalized. So technically, the problem is two-fold: prove convergence for the first case, and determine if the method generalizes. So the problem statement should capture both. Alternatively, maybe split into two problems, but according to the format example, it's one problem. Perhaps structure the problem as the user's main question, which is proving convergence for the first case and then generalizing. But in the example, the problem was a single statement. So perhaps:\n\nProblem: Prove that $\\frac{P_n}{P_n(0)}$ uniformly converges to $t \\mapsto e^t$ on $[0,1]$ and determine if this method generalizes to other linear differential equations, including those with non-constant coefficients.\n\nBut I need to ensure all the necessary assumptions are listed. Let me check the assumptions again. The scalar product is defined over polynomials, orthonormal basis via Gram-Schmidt, normalization by P_n(0), the interval [0,1], the specific differential equation y' - y =0, etc. Also, the other scalar product case might be another assumption, but since the user is asking about generalization, maybe it's part of the problem. Hmm. The problem part should include the main question. The assumptions are the conditions given in the problem. So in the original question, the user defines the scalar product, constructs the orthonormal basis, observes convergence, and wants a proof. So the assumptions are the scalar product definition, the basis construction, normalization, and domain. The problem is to prove the convergence and answer the generalization.\n\nTherefore, structuring:\n\nAssumptions:\n1. The scalar product is defined as $(u|v) = \\int_0^1 (u'(t) - u(t))(v'(t) - v(t)) \\, dt$ over polynomials.\n2. $\\{P_n\\}$ is an orthonormal basis of polynomials obtained via Gram-Schmidt on $\\{1, t, t^2, \\ldots\\}$.\n3. The function $f(t) = e^t$ solves $y' - y = 0$.\n4. The domain of consideration is $[0, 1]$.\n5. Normalization is done by dividing $P_n$ by $P_n(0)$.\n\nProblem: Prove that $\\frac{P_n}{P_n(0)}$ converges uniformly to $e^t$ on $[0,1]$, and investigate whether the method generalizes to other linear differential equations, particularly those without polynomial solutions and with non-constant coefficients.\n\nBut the scalar product over polynomials is important. So maybe clarify that assumption. Also, note that the scalar product is non-degenerate on polynomials. Let me check: the user mentions \"the former is not [a scalar product over continuous functions], since for $f(t)=e^t$, $(f|f)=0$, but it's obviously a scalar product over polynomials (the solutions of $(f|f)=0$ are not polynomials...\". So the assumption that the scalar product is positive definite on polynomials is important. So add that:\n\nAssumption: The scalar product $(u|v)$ is positive definite over the space of polynomials.\n\nSo revised assumptions:\n\n1. The scalar product $(u|v) = \\int_0^1 (u'(t) - u(t))(v'(t) - v(t)) \\, dt$ is defined over polynomials.\n2. The scalar product is positive definite over polynomials.\n3. An orthonormal basis $\\{P_n\\}$ is constructed via Gram-Schmidt orthogonalization on the monomial basis $\\{1, t, t^2, \\ldots\\}$.\n4. The function $e^t$ satisfies $y' - y = 0$.\n5. The domain of interest is $[0,1]$.\n6. Normalization is performed by dividing each $P_n$ by $P_n(0)$.\n\nProblem: Prove that the normalized polynomials $\\frac{P_n}{P_n(0)}$ converge uniformly to $e^t$ on $[0,1]$, and determine if this process can be generalized to other linear differential equations, including those with non-constant coefficients.\n\nYes, that seems to capture the key points. The user also mentions a similar case with the trigonometric functions, but that's part of the investigation into generalization, so it's covered in the problem statement. The assumptions focus on the first case, and the problem asks for the proof and generalization.",
    "Original_Problem": "Assumption: \nThe scalar product is defined as $(u|v) = \\int_0^1 (u'(t) - u(t))(v'(t) - v(t)) \\, dt$ over polynomials,\nThe scalar product is positive definite over the space of polynomials,\nAn orthonormal basis $\\{P_n\\}$ is constructed using Gram-Schmidt orthogonalization on the monomial basis $\\{1, t, t^2, \\ldots\\}$,\nThe function $e^t$ is a solution to the differential equation $y' - y = 0$,\nThe domain of consideration is the interval $[0,1]$,\nNormalization is performed by dividing each $P_n$ by $P_n(0)$.\nProblem: \nProve that the normalized polynomials $\\frac{P_n}{P_n(0)}$ converge uniformly to $e^t$ on $[0,1]$, and determine if this process generalizes to other linear differential equations, including those with non-constant coefficients.",
    "Original_Problem_with_numerical_assumption": "Assumption:\nAssumption 1: The scalar product is defined as $(u|v) = \\int_0^1 (u'(t) - u(t))(v'(t) - v(t)) \\, dt$ over polynomials,\nAssumption 2: The scalar product is positive definite over the space of polynomials,\nAssumption 3: An orthonormal basis $\\{P_n\\}$ is constructed using Gram-Schmidt orthogonalization on the monomial basis $\\{1, t, t^2, \\ldots\\}$,\nAssumption 4: The function $e^t$ is a solution to the differential equation $y' - y = 0$,\nAssumption 5: The domain of consideration is the interval $[0,1]$,\nAssumption 6: Normalization is performed by dividing each $P_n$ by $P_n(0)$.\nProblem: \nProve that the normalized polynomials $\\frac{P_n}{P_n(0)}$ converge uniformly to $e^t$ on $[0,1]$, and determine if this process generalizes to other linear differential equations, including those with non-constant coefficients.",
    "Proof_problem": "Yes",
    "Redundant_assumption": "$$\n(f,g) = \\int_{t=t_0}^{t_1} f(t) \\, g(t) \\, w(t)\n$$",
    "Problem_with_redundant_assumption": "Assumption:\nAssumption 1: The scalar product is defined as $(u|v) = \\int_0^1 (u'(t) - u(t))(v'(t) - v(t)) \\, dt$ over polynomials,\nAssumption 2: The scalar product is positive definite over the space of polynomials,\nAssumption 3: An orthonormal basis $\\{P_n\\}$ is constructed using Gram-Schmidt orthogonalization on the monomial basis $\\{1, t, t^2, \\ldots\\}$,\nAssumption 4: The function $e^t$ is a solution to the differential equation $y' - y = 0$,\nAssumption 5: The domain of consideration is the interval $[0,1]$,\nAssumption 6: Normalization is performed by dividing each $P_n$ by $P_n(0)$.\nAssumption 7: $$\n(f,g) = \\int_{t=t_0}^{t_1} f(t) \\, g(t) \\, w(t)\n$$\nProblem: \nProve that the normalized polynomials $\\frac{P_n}{P_n(0)}$ converge uniformly to $e^t$ on $[0,1]$, and determine if this process generalizes to other linear differential equations, including those with non-constant coefficients."
}