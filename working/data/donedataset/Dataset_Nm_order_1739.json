{
    "Link_API": "https://api.stackexchange.com/2.3/questions/653159",
    "Title": "If $A$ is positive definite, then $\\int_{\\mathbb{R}^n}\\mathrm{e}^{-\\langle Ax,x\\rangle}\\text{d}x=\\left|\\det\\left({\\pi}^{-1}A\\right)\\right|^{-1/2}$",
    "Score": 18,
    "Category": "Chứng minh",
    "Tags": "real-analysis, integration, matrices, normal-distribution, determinant",
    "Link": "https://math.stackexchange.com/questions/653159/if-a-is-positive-definite-then-int-mathbbrn-mathrme-langle-ax-x",
    "Content": "Let $A$ be a positive definite real $n\\times n$ matrix. How can I prove that\n$$\n\\int_{\\mathbb{R}^n}\\mathrm{e}^{-\\langle Ax,x\\rangle}\\text{d}x=\\left|\\,\\det\\left(\\pi^{-1}{A}\\right)\\right|^{-1/2}=\\pi^{n/2}\\lvert\\,\\det A\\rvert^{-1/2}\\!,\n$$ where $\\langle\\cdot,\\cdot\\rangle$ denotes the inner product in $\\mathbb R^n$, i.e. $\\langle x,y\\rangle =x^Ty$.",
    "AcceptedAnswer": "As the matrix $A$ is symmetric (a positive definite is by definition also symmetric), \nthen it is also diagonalizable, i.e., $A=U^TDU$, where $U$ is orthogonal ($U^TU=UU^T=I$ and $|\\det U|=1$) and \n$$\nD=\\mathrm{diag}(\\lambda_1,\\ldots,\\lambda_n),\n$$ \nwhere $\\lambda_i>0$, for $i=1,\\ldots,n$. Clearly $\\det A=\\prod_{i=1}^n \\lambda_i$. First, we have that\n$$\n\\langle x,Ax\\rangle=\\langle x,U^TDUx\\rangle=\\langle Ux,DUx\\rangle,\n$$\nand hence\n$$\n\\int_{\\mathbb R^n}\\exp(-\\langle x,Ax\\rangle)\\,dx =\\int_{\\mathbb R^n}\\exp(-\\langle Ux,DUx\\rangle)\\,dx.\n$$\nThe Theorem of Integration by Change of Variables provides that\n\\begin{align}\n\\int_{\\mathbb R^n}\\exp(-\\langle Ux,DUx\\rangle)\\,dx\\stackrel{y=Ux}=&\\int_{\\mathbb R^n}\\exp(-\\langle y,Dy\\rangle)\\,|\\det U|\\,dy \\\\ \n=&\\int_{\\mathbb R^n}\\exp(-\\langle y,Dy\\rangle)\\,dy\\\\ =&\\int_{\\mathbb R^n}\n\\exp(-(\\lambda_1y_1^2+\\lambda_2y_2^2+\\cdots+\\lambda_ny_n^2)\\rangle)\\,dy_1\\,dy_2\\cdots dy_n \\\\\n\\stackrel{(*)}{=}&\\prod_{i=1}\\int_{\\mathbb R}\\exp(-\\lambda_iy_i^2)\\,dy_i\\stackrel{(\\dagger)}{=}\\prod_{i=1}^n\\left(\\frac{\\pi}{\\lambda_i}\\right)^{1/2}=\\pi^{n/2}|\\det A|^{-1/2}  \\\\=&\\big|\\det(\\pi^{-1}A)\\big|^{-1/2},\n\\end{align}\nwhere at $(\\star)$ we used Fubini's Theorem , while at $(\\dagger)$ the fact that\n$$\n\\int_{\\mathbb R}\\mathrm{e}^{-\\lambda x^2}=\\sqrt{\\frac{\\pi}{\\lambda}},\n$$\nwhenever $\\lambda>0$.",
    "llm_answer_create_structured_problem": "###BEGIN_OF_FORMAT###\nAssumption: \n$A$ is a real $n \\times n$ matrix,\n$A$ is positive definite,\n$\\langle \\cdot, \\cdot \\rangle$ denotes the standard inner product in $\\mathbb{R}^n$ (i.e., $\\langle x, y \\rangle = x^T y$).\nProblem:\nProve that  \n$$\\int_{\\mathbb{R}^n} \\mathrm{e}^{-\\langle Ax, x \\rangle} \\text{d}x = \\pi^{n/2} \\lvert \\det A \\rvert^{-1/2}.$$\n###",
    "reasoning_create_structured_problem": "Okay, let me try to work through this problem step by step. The problem is to prove that for a positive definite real matrix A, the integral of e^(-⟨Ax, x⟩) over all of R^n equals π^(n/2) times the determinant of A to the power of -1/2. \n\nFirst, I remember that for the case when A is the identity matrix, this integral is a standard Gaussian integral. In one dimension, the integral of e^(-a x²) dx from -infty to infty is sqrt(π/a). For n dimensions, if A is the identity matrix, the integral would be (π)^(n/2) because the determinant of the identity matrix is 1. So maybe the general case involves diagonalizing the matrix A since positive definite matrices can be diagonalized.\n\nSince A is positive definite, it's symmetric and all its eigenvalues are positive. So, there exists an orthogonal matrix O such that O^T A O is diagonal. Let me denote D as the diagonal matrix of eigenvalues. Then, A = O D O^T. Changing variables using this orthogonal transformation might simplify the integral. \n\nLet me consider a change of variables y = O^T x. Then, the inner product ⟨Ax, x⟩ becomes ⟨O D O^T x, x⟩. Since O is orthogonal, O^T x is y, so ⟨D y, y⟩. This is just the sum over the eigenvalues times y_i squared. So the integral becomes the product of integrals over each y_i of e^(-λ_i y_i²) dy_i, where λ_i are the eigenvalues. Each of these integrals is sqrt(π/λ_i), so multiplying them together gives (π)^{n/2} / sqrt(prod λ_i). But the product of eigenvalues is the determinant of D, which is the same as the determinant of A since similar matrices have the same determinant. \n\nTherefore, the integral is π^{n/2} / sqrt(det A). But the problem states it as |det(π^{-1} A)|^{-1/2}. Let's check that. det(π^{-1} A) is (π^{-n}) det A, so |det(π^{-1} A)|^{-1/2} = (π^{-n} det A)^{-1/2} = π^{n/2} (det A)^{-1/2}, which matches the result we got. So that part checks out.\n\nWait, but the problem statement has absolute value signs. However, since A is positive definite, its determinant is positive, so the absolute value is redundant but doesn't hurt. \n\nAnother approach might be using the multivariate normal distribution. The integral of the exponential of a quadratic form is related to the normalization constant of a multivariate Gaussian. The formula given is consistent with that, where the covariance matrix is (2A)^{-1} or something similar. But in the standard Gaussian integral, the integral is (2π)^{n/2} det(Σ)^{1/2} for the covariance matrix Σ. Hmm, here the exponent is -⟨Ax, x⟩, whereas the standard Gaussian is -1/2 x^T Σ^{-1} x. So perhaps if we set Σ^{-1} = 2A, then the integral would be (2π)^{n/2} det(Σ)^{1/2} = (2π)^{n/2} det( (2A)^{-1} )^{1/2} = (2π)^{n/2} (2^{-n} det A^{-1})^{1/2} ) = (2π)^{n/2} 2^{-n/2} det A^{-1/2} ) = π^{n/2} det A^{-1/2}, which matches the desired result. So that also confirms the formula. \n\nBut the problem wants us to prove this from scratch, maybe without relying on probability theory. So going back to the diagonalization approach seems solid. Since A can be diagonalized by an orthogonal transformation, the integral factorizes into a product of one-dimensional integrals, each contributing a sqrt(π/λ_i). The product of sqrt(1/λ_i) is 1/sqrt(det A), and the π terms give π^{n/2}. \n\nAnother thing to note is that the change of variables using an orthogonal matrix preserves the volume element, i.e., the absolute value of the determinant of the Jacobian is 1, since O is orthogonal. So, dy = |det O^T| dx = dx. Therefore, the substitution is valid and doesn't introduce any scaling factors. \n\nAlternatively, if we use a general linear transformation, the Jacobian determinant would come into play. But in this case, since we're using an orthogonal transformation (rotation/reflection), the volume element remains the same. \n\nSo, putting it all together: diagonalize A, change variables to y where the quadratic form becomes a sum of squares, compute the product of the 1D integrals, and relate the product of eigenvalues to the determinant of A. That should give the desired result. \n\nI think this approach is correct. The key assumptions are that A is positive definite (so eigenvalues are positive, ensuring convergence of the integral, and allowing the diagonalization) and real (so the orthogonal matrix diagonalization is possible). The problem might also implicitly use that the integral over R^n can be decomposed into a product of integrals after diagonalization, which relies on Fubini's theorem. \n\nTherefore, the proof would involve these steps:\n1. Diagonalize A using an orthogonal matrix.\n2. Perform a change of variables to diagonalize the quadratic form.\n3. Express the integral as a product of one-dimensional Gaussian integrals.\n4. Compute each integral and multiply them.\n5. Express the result in terms of the determinant of A.",
    "Original_Problem": "Assumption: \n$A$ is a real $n \\times n$ matrix,\n$A$ is positive definite,\n$\\langle \\cdot, \\cdot \\rangle$ denotes the standard inner product in $\\mathbb{R}^n$ (i.e., $\\langle x, y \\rangle = x^T y$).\nProblem:\nProve that  \n$$\\int_{\\mathbb{R}^n} \\mathrm{e}^{-\\langle Ax, x \\rangle} \\text{d}x = \\pi^{n/2} \\lvert \\det A \\rvert^{-1/2}.$$",
    "Original_Problem_with_numerical_assumption": "Assumption:\nAssumption 1: $A$ is a real $n \\times n$ matrix,\nAssumption 2: $A$ is positive definite,\nAssumption 3: $\\langle \\cdot, \\cdot \\rangle$ denotes the standard inner product in $\\mathbb{R}^n$ (i.e., $\\langle x, y \\rangle = x^T y$).\nProblem:\nProve that  \n$$\\int_{\\mathbb{R}^n} \\mathrm{e}^{-\\langle Ax, x \\rangle} \\text{d}x = \\pi^{n/2} \\lvert \\det A \\rvert^{-1/2}.$$",
    "Proof_problem": "Yes",
    "Redundant_assumption": "$$\n\\int_{\\mathbb R^n}\\exp(-\\langle x,Ax\\rangle)\\,dx =\\int_{\\mathbb R^n}\\exp(-\\langle Ux,DUx\\rangle)\\,dx.\n$$",
    "Problem_with_redundant_assumption": "Assumption:\nAssumption 1: $A$ is a real $n \\times n$ matrix,\nAssumption 2: $A$ is positive definite,\nAssumption 3: $\\langle \\cdot, \\cdot \\rangle$ denotes the standard inner product in $\\mathbb{R}^n$ (i.e., $\\langle x, y \\rangle = x^T y$).\nAssumption 4: $$\n\\int_{\\mathbb R^n}\\exp(-\\langle x,Ax\\rangle)\\,dx =\\int_{\\mathbb R^n}\\exp(-\\langle Ux,DUx\\rangle)\\,dx.\n$$\nProblem:\nProve that  \n$$\\int_{\\mathbb{R}^n} \\mathrm{e}^{-\\langle Ax, x \\rangle} \\text{d}x = \\pi^{n/2} \\lvert \\det A \\rvert^{-1/2}.$$"
}