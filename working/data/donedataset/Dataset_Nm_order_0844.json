{
    "Link_API": "https://api.stackexchange.com/2.3/questions/26170",
    "Title": "Matrix Inverses and Eigenvalues",
    "Score": 26,
    "Category": "Chứng minh",
    "Tags": "linear-algebra, matrices, eigenvalues-eigenvectors, inverse",
    "Link": "https://math.stackexchange.com/questions/26170/matrix-inverses-and-eigenvalues",
    "Content": "I was working on this problem here below, but seem to not know a precise or clean way to show the proof to this question below. I had about a few ways of doing it, but the statements/operations were pretty loosely used. The problem is as follows: Show that ${\\bf A}^{-1}$ exists if and only if the eigenvalues $ \\lambda _i$ ,  $1 \\leq i \\leq n$ of $\\bf{A}$ are all non-zero, and then ${\\bf A}^{-1}$ has the eigenvalues given by $ \\frac{1}{\\lambda _i}$, $1 \\leq i \\leq n$. Thanks.",
    "AcceptedAnswer": "(Assuming $\\mathbf{A}$ is a square matrix, of course). Here's a solution that does not invoke determinants or diagonalizability, but only the definition of eigenvalue/eigenvector, and the characterization of invertibility in terms of the nullspace. ( Added for clarity: $\\mathbf{N}(\\mathbf{A}) = \\mathrm{ker}(\\mathbf{A}) = \\{\\mathbf{x}\\mid \\mathbf{A}\\mathbf{x}=\\mathbf{0}\\}$, the nullspace/kernel of $\\mathbf{A}$.) \\begin{align*}\n\\mbox{$\\mathbf{A}$ is not invertible} &\\Longleftrightarrow \\mathbf{N}(\\mathbf{A})\\neq{\\mathbf{0}}\\\\\n&\\Longleftrightarrow \\mbox{there exists $\\mathbf{x}\\neq\\mathbf{0}$ such that $\\mathbf{A}\\mathbf{x}=\\mathbf{0}$}\\\\\n&\\Longleftrightarrow \\mbox{there exists $\\mathbf{x}\\neq\\mathbf{0}$ such that $\\mathbf{A}\\mathbf{x}=0\\mathbf{x}$}\\\\\n&\\Longleftrightarrow \\mbox{there exists an eigenvector of $\\mathbf{A}$ with eigenvalue $\\lambda=0$}\\\\\n&\\Longleftrightarrow \\mbox{$\\lambda=0$ is an eigenvalue of $\\mathbf{A}$.}\n\\end{align*} Note that this argument holds even in the case where $\\mathbf{A}$ has no eigenvalues (when working over a non-algebraically closed field, of course), where the condition \"the eigenvalues of $\\mathbf{A}$ are all nonzero\" is true by vacuity. For $\\mathbf{A}$ invertible:\n\\begin{align*}\n\\mbox{$\\lambda\\neq 0$ is an eigenvalue of $\\mathbf{A}$} &\\Longleftrightarrow \\mbox{$\\lambda\\neq 0$ and there exists $\\mathbf{x}\\neq \\mathbf{0}$ such that $\\mathbf{A}\\mathbf{x}=\\lambda\\mathbf{x}$}\\\\\n&\\Longleftrightarrow\\mbox{there exists $\\mathbf{x}\\neq\\mathbf{0}$ such that $\\mathbf{A}({\\textstyle\\frac{1}{\\lambda}}\\mathbf{x}) = \\mathbf{x}$}\\\\\n&\\Longleftrightarrow\\mbox{there exists $\\mathbf{x}\\neq \\mathbf{0}$ such that $\\mathbf{A}^{-1}\\mathbf{A}({\\textstyle\\frac{1}{\\lambda}}\\mathbf{x}) = \\mathbf{A}^{-1}\\mathbf{x}$}\\\\\n&\\Longleftrightarrow\\mbox{there exists $\\mathbf{x}\\neq \\mathbf{0}$ such that $\\frac{1}{\\lambda}\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{x}$}\\\\\n&\\Longleftrightarrow\\mbox{$\\frac{1}{\\lambda}$ is an eigenvalue of $A^{-1}$.}\n\\end{align*}",
    "llm_answer_create_structured_problem": "###BEGIN_OF_FORMAT###\nAssumption: \n$\\mathbf{A}$ is a square matrix,\n$\\mathbf{A}^{-1}$ is the inverse of $\\mathbf{A}$,\n$\\lambda_i$ are eigenvalues of $\\mathbf{A}$ for $1 \\leq i \\leq n$,\n$\\mathbf{A}^{-1}$ has eigenvalues $\\frac{1}{\\lambda_i}$ for $1 \\leq i \\leq n$.\nProblem:\nShow that $\\mathbf{A}^{-1}$ exists if and only if all eigenvalues $\\lambda_i$ of $\\mathbf{A}$ are non-zero, and $\\mathbf{A}^{-1}$ has eigenvalues $\\frac{1}{\\lambda_i}$.\n###END_OF_FORMAT###",
    "reasoning_create_structured_problem": "Okay, let me try to work through this problem step by step. So, the task is to show that the inverse of a matrix A exists if and only if all the eigenvalues of A are non-zero, and then that the inverse matrix A⁻¹ has eigenvalues which are the reciprocals of the eigenvalues of A. \n\nFirst, I need to recall some fundamental concepts about matrices, inverses, and eigenvalues. Let's start by breaking down the problem into two main parts: the \"if and only if\" statement regarding the existence of A⁻¹ and the non-zero eigenvalues, and then the part about the eigenvalues of A⁻¹ being 1/λ_i.\n\nStarting with the first part: proving that A⁻¹ exists if and only if all eigenvalues of A are non-zero. \n\nI remember that a matrix is invertible (nonsingular) if and only if its determinant is non-zero. So, maybe I can connect the determinant to the eigenvalues. Eigenvalues are the roots of the characteristic equation det(A - λI) = 0. The determinant of A is the product of its eigenvalues. So if all eigenvalues λ_i are non-zero, then the product of the eigenvalues (which is det(A)) is non-zero, hence A is invertible. Conversely, if A is invertible, then det(A) ≠ 0, which implies that none of the eigenvalues can be zero (since the product would be zero otherwise). So that seems like a solid connection.\n\nNow, for the second part: showing that if A⁻¹ exists, then its eigenvalues are the reciprocals of the eigenvalues of A. Let's think about eigenvalues. If λ is an eigenvalue of A with eigenvector v, then Av = λv. If A is invertible, we can multiply both sides by A⁻¹ to get v = λA⁻¹v, which rearranges to A⁻¹v = (1/λ)v. So that shows that 1/λ is an eigenvalue of A⁻¹ corresponding to the same eigenvector v. But I need to make sure that all eigenvalues of A⁻¹ are of this form. Also, I need to check that there are no other eigenvalues introduced here. Since the eigenvalues of A⁻¹ are exactly the reciprocals of the eigenvalues of A when A is invertible, this should hold. \n\nBut wait, maybe I should also consider the algebraic multiplicities. Each eigenvalue 1/λ_i of A⁻¹ should have the same multiplicity as λ_i in A. Since the characteristic polynomial of A⁻¹ can be related to that of A. Let me recall that if p(λ) is the characteristic polynomial of A, then the characteristic polynomial of A⁻¹ is λ^n p(1/λ) / det(A), or something like that. Maybe that's overcomplicating. The earlier argument with Av = λv leading to A⁻¹v = (1/λ)v seems sufficient for the eigenvalue part.\n\nPutting it all together:\n\n1. A is invertible ⇨⇒ det(A) ≠ 0 ⇨⇒ product of eigenvalues ≠ 0 ⇨⇒ all eigenvalues ≠ 0.\n\n2. If λ is an eigenvalue of A with eigenvector v, then A⁻¹v = (1/λ)v, so 1/λ is an eigenvalue of A⁻¹.\n\nTherefore, the proof structure would be:\n\n- For the forward direction: Assume A⁻¹ exists, then det(A) ≠ 0, hence all eigenvalues are non-zero.\n\n- For the reverse direction: If all eigenvalues are non-zero, then det(A) ≠ 0, so A⁻¹ exists.\n\n- For the eigenvalues of A⁻¹: Use the eigen equation to show each 1/λ_i is an eigenvalue, and since these are all eigenvalues (as the set is exhausted), the statement holds.\n\nPotential points to check:\n\n- Are there any conditions where A might not be diagonalizable, but still have an inverse? Even if A is not diagonalizable, the eigenvalues still determine invertibility. The existence of inverse doesn't require diagonalizability, just non-zero determinant.\n\n- What if there's a defective eigenvalue (algebraic multiplicity > geometric multiplicity)? But eigenvalues are still roots of the characteristic polynomial. The invertibility depends on determinant, which is the product of eigenvalues (with algebraic multiplicities), so even if the matrix isn't diagonalizable, the eigenvalues being non-zero ensures invertibility.\n\nAnother approach: Using the fact that the eigenvalues of A⁻¹ are the reciprocals. Suppose A is invertible, then there's a relationship between their characteristic polynomials. If the characteristic polynomial of A is p(λ) = det(A - λI) = (-1)^n λ^n + ... + det(A). Then the characteristic polynomial of A⁻¹ would be det(A⁻¹ - μI) = det(A⁻¹) det(I - μA) = (1/det(A)) det(-μA + I) = (1/det(A)) (-μ)^n det(A - (1/μ)I). This would relate the characteristic polynomial of A⁻¹ to that of A evaluated at 1/μ. Therefore, if p(λ) = 0 for λ, then p(1/μ) = 0 implies μ = 1/λ. Hence, eigenvalues of A⁻¹ are reciprocals of eigenvalues of A.\n\nBut maybe that's a bit too involved for the problem. The problem might just be expecting the eigenvector approach.\n\nSo, in summary:\n\nTo show A⁻¹ exists ⇨⇒ all eigenvalues are non-zero:\n\n- Use determinant: det(A) = product of eigenvalues. So if all eigenvalues non-zero, product non-zero ⇒ det(A) ≠ 0 ⇒ invertible. Conversely, if invertible, det(A) ≠ 0 ⇒ product of eigenvalues ≠ 0 ⇒ none are zero.\n\nEigenvalues of A⁻¹ are 1/λ_i:\n\n- Take Av = λv ⇒ A⁻¹v = (1/λ)v. Hence, if v is eigenvector of A with λ, then same v is eigenvector of A⁻¹ with 1/λ.\n\nHence, the statement holds.\n\nI need to make sure that this covers all cases and that there are no gaps. For example, even if A is not diagonalizable, the eigenvalues are still defined via their characteristic polynomial, and the invertibility is still tied to the determinant. The eigenvectors might not form a complete basis, but as long as there exists at least one eigenvector for each eigenvalue (which might not be the case in defective matrices), but actually, even defective matrices have eigenvalues, and the existence of the inverse is purely based on the determinant. So even if some eigenvalues are defective, the logic still holds.\n\nTherefore, the answer seems to hinge on connecting invertibility to determinant, determinant to eigenvalues, and then using the eigen equation to show the reciprocal eigenvalues for the inverse.",
    "Original_Problem": "Assumption: \n$\\mathbf{A}$ is a square matrix,\n$\\mathbf{A}^{-1}$ is the inverse of $\\mathbf{A}$,\n$\\lambda_i$ are eigenvalues of $\\mathbf{A}$ for $1 \\leq i \\leq n$,\n$\\mathbf{A}^{-1}$ has eigenvalues $\\frac{1}{\\lambda_i}$ for $1 \\leq i \\leq n$.\nProblem:\nShow that $\\mathbf{A}^{-1}$ exists if and only if all eigenvalues $\\lambda_i$ of $\\mathbf{A}$ are non-zero, and $\\mathbf{A}^{-1}$ has eigenvalues $\\frac{1}{\\lambda_i}$.",
    "Original_Problem_with_numerical_assumption": "Assumption:\nAssumption 1: $\\mathbf{A}$ is a square matrix,\nAssumption 2: $\\mathbf{A}^{-1}$ is the inverse of $\\mathbf{A}$,\nAssumption 3: $\\lambda_i$ are eigenvalues of $\\mathbf{A}$ for $1 \\leq i \\leq n$,\nAssumption 4: $\\mathbf{A}^{-1}$ has eigenvalues $\\frac{1}{\\lambda_i}$ for $1 \\leq i \\leq n$.\nProblem:\nShow that $\\mathbf{A}^{-1}$ exists if and only if all eigenvalues $\\lambda_i$ of $\\mathbf{A}$ are non-zero, and $\\mathbf{A}^{-1}$ has eigenvalues $\\frac{1}{\\lambda_i}$.",
    "Proof_problem": "Yes",
    "Redundant_assumption": "$\\mathbf{x}\\neq \\mathbf{0}$",
    "Problem_with_redundant_assumption": "Assumption:\nAssumption 1: $\\mathbf{A}$ is a square matrix,\nAssumption 2: $\\mathbf{A}^{-1}$ is the inverse of $\\mathbf{A}$,\nAssumption 3: $\\lambda_i$ are eigenvalues of $\\mathbf{A}$ for $1 \\leq i \\leq n$,\nAssumption 4: $\\mathbf{A}^{-1}$ has eigenvalues $\\frac{1}{\\lambda_i}$ for $1 \\leq i \\leq n$.\nAssumption 5: $\\mathbf{x}\\neq \\mathbf{0}$\nProblem:\nShow that $\\mathbf{A}^{-1}$ exists if and only if all eigenvalues $\\lambda_i$ of $\\mathbf{A}$ are non-zero, and $\\mathbf{A}^{-1}$ has eigenvalues $\\frac{1}{\\lambda_i}$."
}