{
    "Link_API": "https://api.stackexchange.com/2.3/questions/2472789",
    "Title": "Is there a *simple* example showing that uncorrelated random variables need not be independent?",
    "Score": 32,
    "Category": "Chứng minh",
    "Tags": "probability-theory, probability-distributions, random-variables, examples-counterexamples, random",
    "Link": "https://math.stackexchange.com/questions/2472789/is-there-a-simple-example-showing-that-uncorrelated-random-variables-need-not",
    "Content": "Is there a simple example showing that given $X,Y$ uncorrelated\n  (covariance is zero), $X,Y$ are not independent? I have looked up two references, however, I am dissatisfied with both. In Reference $1$, $X,Y$ are assumed to be independent uniform RVs\nfrom $(0,1)$, construct $Z = X+Y, W = X - Y$, then the claim is that\n$Z,W$ is uncorrelated but not independent. Unfortunately, finding the\nPDF of $Z,W$ is not trivial. In Reference $2$, $\\phi$ is assumed to be uniform RV from $(0,\n   2\\pi)$, and construct $X = \\cos(\\phi)$, $Y = \\sin(\\phi)$. Then the claim is that $X,Y$ are uncorrelated but not independent. Unfortunately, the PDFs of $X,Y$ takes on the form of rarely mentioned arcsine distribution. I just wish to have an example at hand where I can whip out to show that uncorrelated does not necessarily implies independent. Is this do-able?",
    "AcceptedAnswer": "Here's a (perhaps) simpler example. Let $X$ be $N(0,1)$ and $Y = X^2.$ Then $$ E(XY) = E(X^3) = 0 =E(X)E(Y),$$ so $X$ and $Y$ are uncorrelated, but clearly they aren't independent (if you know $X$ , then you know $Y).$",
    "llm_answer_create_structured_problem": "###BEGIN_OF_FORMAT###\nAssumption: \n$X$ is a discrete random variable taking values $-1$, $0$, and $1$ with equal probability $\\frac{1}{3}$,\n$Y$ is defined as $Y = X^2$,\nThe covariance between $X$ and $Y$ is zero,\nThe joint probability distribution of $X$ and $Y$ is not equal to the product of their marginal distributions.\nProblem:\nShow that $X$ and $Y$ are uncorrelated but not independent.\n###",
    "reasoning_create_structured_problem": "Okay, let me try to work through this problem. The user wants a simple example where two random variables are uncorrelated but not independent. They mentioned two references but found them too complicated because of the PDFs involved. So I need to come up with an example that's straightforward, maybe using discrete random variables instead of continuous ones since their PDFs are easier to handle.\n\nFirst, I remember that independence implies uncorrelatedness, but the converse isn't true. To show that, the example must have covariance zero but the variables still dependent. Let's think of a simple discrete case. Maybe a classic example with a small sample space.\n\nHow about a case where X and Y are both Bernoulli variables? Let's see. Suppose we have a sample space with four outcomes, each with probability 1/4. Let X be 1 in the first two outcomes and 0 otherwise, and Y be 1 in the first and third outcomes. Then check covariance. But wait, maybe that's not the right setup.\n\nAlternatively, consider a more standard example. Let me recall. If X and Y are defined such that their product has an expected value equal to the product of their expected values, they're uncorrelated. But they could still be dependent through some other relationship.\n\nWait, there's the example where you take X uniformly distributed on {-1, 0, 1} and let Y = X². Then X and Y are uncorrelated because Cov(X,Y) = E[XY] - E[X]E[Y]. Since E[X] = 0, Cov(X,Y) = E[X³]. But X³ is the same as X here, so E[X³] = 0. So covariance is zero. But clearly Y depends on X, since Y is a function of X. That might work. Let me check.\n\nWait, in this case, X can be -1, 0, 1 each with probability 1/3. Then Y is 1, 0, 1. Then E[X] = 0. E[Y] = (1 + 0 + 1)/3 = 2/3. E[XY] = E[X*X²] = E[X³] = (-1)^3*(1/3) + 0 + 1^3*(1/3) = (-1 + 0 + 1)/3 = 0. So Cov(X,Y) = 0 - 0*(2/3) = 0. So they are uncorrelated. But Y is completely determined by X, so they are dependent. That seems like a good example.\n\nBut the user mentioned that in one reference they used cosine and sine of a uniform angle, which gives uncorrelated but dependent variables. But the user found the PDFs complicated. The example I just thought of uses a discrete distribution, which is simpler. Let me verify if this example is commonly accepted. Yes, this is a standard example. So maybe this is a good candidate.\n\nAnother classic example is using a symmetric distribution. Suppose we take a random variable X that can be -1, 0, 1 each with probability 1/3, and let Y = |X|. Then Y is 1, 0, 1. Then similar to before, Cov(X,Y) would be zero. But Y is dependent on X. However, wait, Y here is |X|, so similar to the previous example. Maybe that's another way to present it.\n\nAlternatively, let's think of a two-variable case. Let’s define a joint distribution where X and Y can each take values -1, 0, 1. Suppose the joint probabilities are set such that the covariance is zero but the variables are dependent. For instance, set up the probabilities so that E[XY] = E[X]E[Y]. Let's make X and Y have zero mean. Let's try constructing such a distribution.\n\nConsider the following joint distribution:\n\nX | Y | P(X,Y)\n-1| -1| 0\n-1| 0 | 1/4\n-1| 1 | 0\n0 | -1| 1/4\n0 | 0 | 0\n0 | 1 | 1/4\n1 | -1| 0\n1 | 0 | 1/4\n1 | 1 | 0\n\nWait, let me check if this sums to 1. The non-zero entries are 1/4 each, and there are four of them, so total probability 1. Now, E[X] = (-1)*(1/4) + 0*(1/4 + 1/4) + 1*(1/4) = (-1 + 1)/4 = 0. Similarly for Y. E[XY] is calculated as sum over all x,y of x*y*P(x,y). Let's compute:\n\nFor X=-1, Y=0: (-1)*0*1/4 = 0\n\nX=0,Y=-1: 0*(-1)*1/4=0\n\nX=0,Y=1: 0*1*1/4=0\n\nX=1,Y=0:1*0*1/4=0\n\nAll other entries are zero. So E[XY] = 0. Therefore, Cov(X,Y) = 0 - 0*0 = 0. So they are uncorrelated. But are they independent? Let's check if P(X=0,Y=0) = P(X=0)P(Y=0). Here, P(X=0,Y=0) is 0, but P(X=0) is (1/4 + 0 +1/4) = 1/2. P(Y=0) is (1/4 +0 +1/4) = 1/2. So P(X=0)P(Y=0) = 1/4. But P(X=0,Y=0)=0 ≠ 1/4. Therefore, they are dependent.\n\nThis is a valid example. But is this simpler than the ones the user mentioned? The variables are discrete, so the PMF is straightforward. The user wanted something where they can \"whip out\" quickly. Maybe this is better than dealing with continuous distributions and their PDFs.\n\nAlternatively, another simple example: let X be a random variable that takes values -1, 0, 1 each with probability 1/3, and define Y = X². As before, Cov(X,Y)=0, but Y is a function of X, hence dependent. That's even simpler.\n\nSo I think either of these examples would work. The key is to have a simple setup where covariance is zero but dependence is clear. The example with Y = X² is very straightforward because once you explain X's distribution, Y is determined, and the covariance calculation is simple. Also, checking independence is easy because knowing X gives complete information about Y, so they can't be independent.\n\nTherefore, the problem can be structured by presenting this example. The assumptions would involve specifying the distribution of X, defining Y in terms of X, and then stating that even though they're uncorrelated, they're dependent. The problem would be to show that X and Y are uncorrelated but not independent.",
    "Original_Problem": "Assumption: \n$X$ is a discrete random variable taking values $-1$, $0$, and $1$ with equal probability $\\frac{1}{3}$,\n$Y$ is defined as $Y = X^2$,\nThe covariance between $X$ and $Y$ is zero,\nThe joint probability distribution of $X$ and $Y$ is not equal to the product of their marginal distributions.\nProblem:\nShow that $X$ and $Y$ are uncorrelated but not independent.",
    "Original_Problem_with_numerical_assumption": "Assumption:\nAssumption 1: $X$ is a discrete random variable taking values $-1$, $0$, and $1$ with equal probability $\\frac{1}{3}$,\nAssumption 2: $Y$ is defined as $Y = X^2$,\nAssumption 3: The covariance between $X$ and $Y$ is zero,\nAssumption 4: The joint probability distribution of $X$ and $Y$ is not equal to the product of their marginal distributions.\nProblem:\nShow that $X$ and $Y$ are uncorrelated but not independent.",
    "Proof_problem": "Yes",
    "Redundant_assumption": "$$ E(XY) = E(X^3) = 0 =E(X)E(Y),$$",
    "Problem_with_redundant_assumption": "Assumption:\nAssumption 1: $X$ is a discrete random variable taking values $-1$, $0$, and $1$ with equal probability $\\frac{1}{3}$,\nAssumption 2: $Y$ is defined as $Y = X^2$,\nAssumption 3: The covariance between $X$ and $Y$ is zero,\nAssumption 4: The joint probability distribution of $X$ and $Y$ is not equal to the product of their marginal distributions.\nAssumption 5: $$ E(XY) = E(X^3) = 0 =E(X)E(Y),$$\nProblem:\nShow that $X$ and $Y$ are uncorrelated but not independent."
}