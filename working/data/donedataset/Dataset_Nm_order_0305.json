{
    "Link_API": "https://api.stackexchange.com/2.3/questions/245327",
    "Title": "Weak Law of Large Numbers for Dependent Random Variables with Bounded Covariance",
    "Score": 43,
    "Category": "Chứng minh",
    "Tags": "probability-theory, measure-theory, covariance, probability-limit-theorems, law-of-large-numbers",
    "Link": "https://math.stackexchange.com/questions/245327/weak-law-of-large-numbers-for-dependent-random-variables-with-bounded-covariance",
    "Content": "I'm currently stuck on the following problem which involves proving the weak law of large numbers for a sequence of dependent but identically distributed random variables. Here's the full statement: Let $(X_n)$ be a sequence of dependent identically distributed random variables with finite variance. Let $\\displaystyle S_n = \\sum_{i=1}^n X_i $ denote the  $n^\\text{th}$ partial sum of the random variables $(X_n)$. Assume that Cov$(X_i,X_j) \\leq c^{|i-j|}$ for $i, j \\leq n$ where $|c| \\leq 1$. Is it possible to show that $\\displaystyle \\frac{S_n}{n} \\rightarrow \\mathbb{E}[X_1]$ in probability? In other words, is it true that given any $\\epsilon>0$, $$ \\lim_{n\\rightarrow \\infty} \\mathbb{P}\\bigg[\\Big|\\frac{S_n}{n} - \\mathbb{E}[X_1]\\Big| > \\epsilon\\bigg] = 0$$ EDIT: Following some comments, it turns out that I had the right approach so I've gone ahead and answered my own question below.",
    "AcceptedAnswer": "Fix $\\epsilon > 0$ and $n \\in \\mathbb{N}$, then we can use Chebyshev's inequality to see that $$\\mathbb{P}\\bigg[\\Big|\\frac{S_n}{n} - \\mathbb{E}[X_1]\\Big| > \\epsilon\\bigg] \\leq \\frac{\\text{Var}\\Big(\\frac{S_n}{n}\\Big)}{\\epsilon^2}$$ where $$\\displaystyle \\text{Var}\\Big(\\frac{S_n}{n}\\Big)=  \\frac{\\text{Var}(S_n)}{n^2} \\leq \\frac{\\sum_{i=1}^n\\sum_{j=1}^n \\text{Cov}{(X_i,X_j)}}{n^2} \\leq \\frac{\\sum_{i=1}^n\\sum_{j=1}^n c^{|i-j|}}{n^2} $$ We can then explicitly calculate the double sum $\\sum_{i=1}^n\\sum_{j=1}^n c^{|i-j|}$  as follows: $$\\begin{align} \n\\sum_{i=1}^n\\sum_{j=1}^n c^{|i-j|} &= \\sum_{i=1}^n c^{|i-i|} + 2\\sum_{i=1}^n\\sum_{j=1}^{i-1} c^{|i-j|} \\\\ \n&= n + 2\\sum_{i=1}^n\\sum_{j=1}^{i-1} c^{|i-j|} \\\\\n&= n + 2\\sum_{i=1}^n\\sum_{j=1}^{i-1} c^{i-j} \\\\\n&= n + 2\\sum_{i=1}^n c^i \\frac{1 - c^{-i}}{1-c^{-1}} \\\\\n&= n + 2\\sum_{i=1}^n \\frac{c^i + 1}{1-c^{-1}} \\\\\n&= n + \\frac{2c}{c-1} \\sum_{i=1}^n c^{i}-1 \\\\\n&= n + \\frac{2c}{c-1} \\big(\\frac{1-c^{n+1}}{1-c} -n \\big)\\\\\n&= n + \\frac{2c}{(c-1)^2}(c^{n+1}+1) +  \\frac{2c}{c-1}n\\\\\n\\\n\\end{align}$$ Thus, $$\\lim_{n\\rightarrow\\infty} \\mathbb{P}\\bigg[\\Big|\\frac{S_n}{n} - \\mathbb{E}[X_1]\\Big| > \\epsilon\\bigg]  = \\lim_{n\\rightarrow\\infty} \\frac{\\text{Var}\\Big(\\frac{S_n}{n}\\Big)}{\\epsilon^2} \\leq \\lim_{n\\rightarrow\\infty} \\frac{n + \\frac{2c}{(c-1)^2}(c^{n+1}+1) +  \\frac{2c}{c-1}n}{n^2 \\epsilon^2} = 0  $$ Seeing how our choice of $\\epsilon$ was arbitrary, the statement above holds for any $\\epsilon > 0 $ and shows that $\\frac{S_n}{n} \\rightarrow E[X_1]$ in probability, as desired. This shows the validity of the theorem for $c<1$, but not for $c=1$.\nWe can easily extend the demonstration to all cases in which $|\\mbox{Cov}(X_i,X_j)|\\le f_{|i-j|}$ where $\\lim_{i\\to\\infty}f_i=0$. Indeed in this case it is simple to show that \n$$\\lim_{n\\to\\infty}{1\\over n^2}\\sum_{i=1}^n\\sum_{j=1}^n \\text{Cov}{(X_i,X_j)}\\le \\lim_{n\\to\\infty}{1\\over n^2}\\sum_{i=1}^n\\sum_{j=1}^n |\\text{Cov}{(X_i,X_j)}|\\le \\lim_{n\\to\\infty}{1\\over n^2}\\sum_{i=1}^n\\sum_{j=1}^nf_{|i-j|}=0$$",
    "llm_answer_create_structured_problem": "###BEGIN_OF_FORMAT###\nAssumption: \nThe random variables $(X_n)$ are identically distributed,\nThe random variables $(X_n)$ are dependent,\nEach $X_i$ has finite variance,\nFor all $i, j \\leq n$, $\\text{Cov}(X_i, X_j) \\leq c^{|i - j|}$ where $|c| \\leq 1$.\nProblem:\nProve that $\\displaystyle \\frac{S_n}{n} \\rightarrow \\mathbb{E}[X_1]$ in probability as $n \\rightarrow \\infty$.\n###END_OF_FORMAT###",
    "reasoning_create_structured_problem": "Okay, let's tackle this problem step by step. So, the user wants me to structure the given Math Stack Exchange question into the specified format. Let me start by reading the original problem carefully.\n\nThe title is about the Weak Law of Large Numbers (WLLN) for dependent random variables with bounded covariance. The question asks whether it's possible to prove that the sample mean S_n/n converges in probability to E[X_1] given certain conditions.\n\nFirst, I need to extract the assumptions. The problem states:\n\n1. The random variables (X_n) are dependent and identically distributed (i.d.d.). Wait, actually, the original says \"dependent but identically distributed\". So they are identically distributed, not independent. So first assumption: Each X_i is identically distributed.\n\n2. They have finite variance. So second assumption: Each X_i has finite variance.\n\n3. The partial sum S_n is the sum from i=1 to n of X_i. That's part of the problem setup, not an assumption.\n\n4. The covariance between X_i and X_j is bounded by c^{|i-j|} where |c| ≤ 1. So third assumption: For all i, j ≤ n, Cov(X_i, X_j) ≤ c^{|i-j|}, and |c| ≤ 1.\n\nThe problem is to show that S_n/n converges to E[X_1] in probability. That is, for any ε > 0, the probability that |S_n/n - E[X_1]| > ε tends to 0 as n approaches infinity.\n\nWait, the original problem mentions \"dependent identically distributed\". So the variables are dependent, but identically distributed. Also, the covariance condition is given. The user later mentions they answered their own question, but we just need to structure the problem as per the format.\n\nSo, the assumptions would be:\n\n- For each i, X_i is identically distributed. Wait, but the problem states \"dependent but identically distributed random variables\". So each X_i has the same distribution as X_1. So that's one assumption: The random variables (X_n) are identically distributed.\n\n- Each X_i has finite variance. So that's another assumption.\n\n- The covariance between X_i and X_j satisfies Cov(X_i, X_j) ≤ c^{|i-j|} for some |c| ≤ 1. But this applies for all i, j ≤ n. So that's another assumption.\n\nAdditionally, the problem involves showing convergence in probability of S_n/n to the expectation. The problem itself is to prove that S_n/n converges in probability to E[X_1], given these assumptions.\n\nWait, but the original problem's title mentions \"dependent random variables\", so the dependence is already part of the problem's context. But the assumptions should list the conditions given. So:\n\nAssumptions:\n\n1. (X_n) is a sequence of identically distributed random variables.\n\n2. Each X_i has finite variance.\n\n3. Cov(X_i, X_j) ≤ c^{|i-j|} for all i, j ≤ n, where |c| ≤ 1.\n\nThe problem is to prove that S_n/n → E[X_1] in probability as n → ∞.\n\nWait, but the original problem says \"dependent but identically distributed\". So perhaps the dependency is part of the setup. But the assumptions should list the specific conditions. The dependency is captured by the covariance condition. So maybe the first assumption is that they are dependent, but that's vague. However, in the problem statement, the key assumptions are the covariance bound and the finite variance. The identical distribution is another assumption. So the list would be:\n\nAssumption:\n\n- The random variables (X_n) are identically distributed.\n\n- Each X_i has finite variance.\n\n- For all i, j ≤ n, Cov(X_i, X_j) ≤ c^{|i-j|} where |c| ≤ 1.\n\nWait, but the problem also mentions that they are dependent. However, the dependence is implied by the covariance condition, as if they were independent, the covariance would be zero. But maybe the problem is structured such that the dependence is given, and the covariance condition is an additional assumption. However, in the example provided by the user, they listed assumptions as separate points. For example, in the example problem, the assumptions were \"n is an integer\" and \"n is divisible by 7\". So each assumption is a separate condition.\n\nSo here, the assumptions should be:\n\n1. (X_n) is a sequence of identically distributed random variables.\n\n2. Each X_i has finite variance.\n\n3. For each pair (i, j), Cov(X_i, X_j) ≤ c^{|i - j|} where |c| ≤ 1.\n\nBut wait, the original problem says \"dependent but identically distributed\". But \"dependent\" is a general condition, while the covariance bound is a specific form of dependence. Since the problem statement gives the covariance condition, perhaps we don't need to list \"dependent\" as a separate assumption because it's implied by the covariance being non-zero. However, the user's example included \"n is an integer\" and \"n is divisible by 7\". So in that case, perhaps here we need to list each condition given in the problem. The original problem says \"dependent but identically distributed random variables with finite variance. Assume that Cov(X_i,X_j) ≤ c^{|i-j|} for i, j ≤n where |c| ≤ 1.\"\n\nSo, structured:\n\nAssumptions:\n\n- The random variables (X_n) are identically distributed.\n\n- Each X_i has finite variance.\n\n- The random variables (X_n) are dependent.\n\n- For all i, j ≤ n, Cov(X_i, X_j) ≤ c^{|i - j|} where |c| ≤ 1.\n\nWait, but the dependency is already given in the problem statement. However, the user's example extracted assumptions from the problem. Let me check the example again.\n\nIn the example, the original problem says \"for every integer n\", so the assumption is \"n is an integer\", and \"n is divisible by 7\" even though the problem as stated was to prove divisibility by 91. So the user included both the general condition (n is integer) and a specific one (divisible by 7). Wait, no, the original problem in the example was to prove that A(n) is divisible by 91 for every integer n. But the user formatted it with assumptions \"n is an integer, n is divisible by 7\". Wait, that doesn't align. Wait, no, in the example, the original problem is to prove for every integer n, A(n) is divisible by 91. But the formatted problem in the example lists assumptions as n is an integer, n is divisible by 7. That seems incorrect, but perhaps it's a mistake. Wait, the example's formatted problem includes \"n is an integer\" and \"n is divisible by 7\", but the original problem didn't require n to be divisible by 7. Maybe that's a mistake in the example. But regardless, perhaps the user wants us to list all the assumptions given in the problem, even if they are redundant.\n\nBut in the current problem, the problem states that the variables are dependent, identically distributed, have finite variance, and the covariance is bounded by c^{|i-j|}. So each of these should be separate assumptions. However, \"dependent\" is a general condition, and the covariance bound is a specific condition. But according to the example, even if a condition is given in the problem statement, it's listed as an assumption. So perhaps we should list:\n\nAssumptions:\n\n- The random variables (X_n) are identically distributed.\n\n- Each X_i has finite variance.\n\n- The random variables (X_n) are dependent.\n\n- For all i, j ≤ n, Cov(X_i, X_j) ≤ c^{|i - j|}, where |c| ≤ 1.\n\nBut the dependence is already implied by the covariance being non-zero, but perhaps the problem states they are dependent, so it should be an assumption. However, sometimes in probability, \"dependent\" just means not independent, but here the covariance condition is more specific. Maybe the problem's main assumptions are:\n\n1. Identically distributed.\n\n2. Finite variance.\n\n3. Covariance bound.\n\nThe dependence is part of the setup but perhaps not an explicit assumption. Let me check the original problem's exact wording.\n\nOriginal problem statement:\n\n\"Let (X_n) be a sequence of dependent identically distributed random variables with finite variance. Let S_n = sum_{i=1}^n X_i. Assume that Cov(X_i,X_j) ≤ c^{|i-j|} for i, j ≤n where |c| ≤ 1. Is it possible to show that S_n/n → E[X_1] in probability?\"\n\nSo the problem says:\n\n- dependent, identically distributed.\n\n- finite variance.\n\n- Covariance condition.\n\nSo the assumptions are:\n\n1. (X_n) is a sequence of dependent, identically distributed random variables.\n\nBut wait, in the example, each assumption is a separate line addressing one property. So perhaps we need to split \"dependent\" and \"identically distributed\" into separate assumptions. However, the example's first assumption was \"n is an integer\", which was part of the original problem's \"for every integer n\". So in this case, \"dependent\" and \"identically distributed\" are two separate properties of the sequence (X_n). So:\n\nAssumptions:\n\n- The random variables (X_n) are identically distributed.\n\n- The random variables (X_n) are dependent.\n\n- Each X_i has finite variance.\n\n- For all i, j ≤ n, Cov(X_i, X_j) ≤ c^{|i - j|}, where |c| ≤ 1.\n\nBut \"dependent\" is a broad term, and the covariance condition is a specific form of dependence. However, the problem states both that they are dependent and gives a specific covariance structure. So perhaps both should be included. But maybe \"dependent\" is redundant given the covariance condition, but since the problem mentions both, we should list them.\n\nAlternatively, perhaps the problem's main assumptions are:\n\n- Identically distributed.\n\n- Finite variance.\n\n- Covariance bounded by c^{|i-j|} with |c| ≤1.\n\nThe fact that they are dependent is part of the problem's context but not an explicit assumption beyond the covariance condition. However, the problem statement says \"dependent but identically distributed\", so maybe \"dependent\" is an assumption. But in terms of mathematical conditions, the key assumptions are the covariance bounds and finite variance. The dependence is already implied by non-zero covariance, but perhaps the problem mentions it explicitly. Given the example's structure, it's safer to list each condition mentioned in the problem as separate assumptions.\n\nSo, the assumptions are:\n\n1. The random variables (X_n) are identically distributed.\n\n2. The random variables (X_n) are dependent.\n\n3. Each X_i has finite variance.\n\n4. For all i, j ≤ n, Cov(X_i, X_j) ≤ c^{|i - j|}, where |c| ≤ 1.\n\nBut the problem also specifies that the covariance condition applies for i, j ≤ n. So maybe that's part of the assumption.\n\nWait, the original says \"for i, j ≤ n\" which is part of the covariance assumption. So the fourth assumption should include that i and j are indices up to n, but since n is the number of terms in S_n, which is the partial sum up to n. Hmm. But in any case, the problem is about the sequence (X_n), so for each n, the covariance condition applies for all i, j in 1 to n. So perhaps the assumption is written as:\n\n- For each pair of indices i, j ≤ n, Cov(X_i, X_j) ≤ c^{|i - j|}, where |c| ≤ 1.\n\nAlternatively, since n is the number of terms, and the problem is about the limit as n approaches infinity, the covariance condition must hold for all pairs i, j in the sequence, regardless of n. So perhaps the assumption is:\n\n- For all i, j ≥ 1, Cov(X_i, X_j) ≤ c^{|i - j|}, where |c| ≤ 1.\n\nBut the original problem states \"for i, j ≤ n\". Wait, the problem says: \"Assume that Cov(X_i,X_j) ≤ c^{|i-j|} for i, j ≤ n where |c| ≤ 1.\" So it's for each n, and for i, j up to n. But this might be part of the problem's setup for each n. However, when structuring the assumptions, perhaps we need to capture that for any n, the covariance between any two terms in the first n variables is bounded by c^{|i-j|}. So the assumption is:\n\n- For all i, j ≤ n, Cov(X_i, X_j) ≤ c^{|i - j|}, where |c| ≤ 1.\n\nBut since n is variable, maybe this is for all i and j in the entire sequence. Wait, the way it's written in the problem is that for each n, when considering the first n variables, their covariances are bounded by c^{|i-j|}. So perhaps the assumption is:\n\n- For each n ≥ 1 and for all i, j ≤ n, Cov(X_i, X_j) ≤ c^{|i - j|}, where |c| ≤ 1.\n\nBut that might be complicating it. Alternatively, the problem could be assuming that for all i, j in the entire sequence, Cov(X_i, X_j) ≤ c^{|i-j|}, which naturally holds for any i and j, which would be a more general condition. However, the problem states it as \"for i, j ≤ n\", which is within the context of each partial sum S_n. But perhaps the intended assumption is that this covariance bound holds for all pairs i, j in the sequence, which would imply that for any n, the first n variables satisfy the condition.\n\nIn any case, to follow the example's format, each assumption should be a single line addressing one property. So:\n\nAssumption:\n\n- The random variables (X_n) are identically distributed.\n\n- Each X_i has finite variance.\n\n- For all i, j ≤ n, Cov(X_i, X_j) ≤ c^{|i - j|}, where |c| ≤ 1.\n\nAdditionally, the problem mentions they are dependent, so perhaps:\n\n- The random variables (X_n) are dependent.\n\nBut again, the dependence is implied by the covariance condition unless c=0. However, since the problem explicitly states they are dependent, it's safer to include it. However, in the example, the user included \"n is an integer\" and \"n is divisible by 7\" even though the original problem was for every integer n. So maybe the user wants all explicit and implicit assumptions listed.\n\nBut wait, in the example, the original problem was to prove for every integer n, but the formatted problem had assumptions that n is an integer and n is divisible by 7. That seems like a mistake, but perhaps the user intended to list all conditions given. Wait, maybe in the example, the user made a mistake. Because the original problem says \"for every integer n\", which is a universal quantifier, not an assumption on n. But the formatted problem lists \"n is an integer\" as an assumption, which is correct because the problem is about any integer n. Then the second assumption \"n is divisible by 7\" might be incorrect unless the original problem had that condition. Wait, the original problem in the example was to prove that A(n) is divisible by 91 for every integer n. The user's formatted problem lists assumptions as n is integer, n divisible by 7, and the problem is to prove divisibility by 91. That seems inconsistent because 91=13*7, so if n is divisible by 7, then 5^n would have factors related to 7, but the example might have been a mistake. However, perhaps in the example, the user intended to show that even if the problem states \"for every integer n\", but the formatted problem includes specific assumptions. Anyway, the key takeaway is that each assumption should be a single line, each addressing one property or condition applied to one object.\n\nSo, applying that to the current problem:\n\nThe problem states that (X_n) is a sequence of dependent, identically distributed random variables with finite variance. Then they assume the covariance condition. So:\n\nAssumptions:\n\n- The random variables (X_n) are identically distributed.\n\n- The random variables (X_n) are dependent.\n\n- Each X_i has finite variance.\n\n- For all i, j ≤ n, Cov(X_i, X_j) ≤ c^{|i - j|}, where |c| ≤ 1.\n\nBut \"dependent\" is a property of the variables, and the covariance condition is a specific form of dependence. However, since the problem mentions both, perhaps they should be separate assumptions. But maybe the dependence is redundant given the covariance condition, but to follow the user's example, it's better to list all explicitly mentioned conditions.\n\nTherefore, the formatted problem should have these four assumptions. The problem is to prove that S_n/n converges in probability to E[X_1].\n\nBut wait, the original problem says \"Is it possible to show that S_n/n → E[X_1] in probability?\" So the problem is to prove that convergence.\n\nPutting it all together:\n\nAssumption:\n- The random variables $(X_n)$ are identically distributed.\n- The random variables $(X_n)$ are dependent.\n- Each $X_i$ has finite variance.\n- For all $i, j \\leq n$, $\\text{Cov}(X_i, X_j) \\leq c^{|i - j|}$ where $|c| \\leq 1$.\n\nProblem:\nProve that $\\displaystyle \\frac{S_n}{n} \\rightarrow \\mathbb{E}[X_1]$ in probability as $n \\rightarrow \\infty$.\n\nBut wait, the problem's exact statement is: \"Is it possible to show that $\\displaystyle \\frac{S_n}{n} \\rightarrow \\mathbb{E}[X_1]$ in probability?\" So in the problem section, we can rephrase it as \"Prove that $\\displaystyle \\frac{S_n}{n} \\rightarrow \\mathbb{E}[X_1]$ in probability as $n \\rightarrow \\infty$.\"\n\nI think this covers all the necessary assumptions and the problem statement. Let me check again:\n\n- The variables are identically distributed: check.\n\n- They are dependent: check.\n\n- Finite variance: check.\n\n- Covariance condition: check.\n\nProblem is to prove convergence in probability: check.\n\nYes, this should be correct.",
    "Original_Problem": "Assumption: \nThe random variables $(X_n)$ are identically distributed,\nThe random variables $(X_n)$ are dependent,\nEach $X_i$ has finite variance,\nFor all $i, j \\leq n$, $\\text{Cov}(X_i, X_j) \\leq c^{|i - j|}$ where $|c| \\leq 1$.\nProblem:\nProve that $\\displaystyle \\frac{S_n}{n} \\rightarrow \\mathbb{E}[X_1]$ in probability as $n \\rightarrow \\infty$.",
    "Original_Problem_with_numerical_assumption": "Assumption:\nAssumption 1: The random variables $(X_n)$ are identically distributed,\nAssumption 2: The random variables $(X_n)$ are dependent,\nAssumption 3: Each $X_i$ has finite variance,\nAssumption 4: For all $i, j \\leq n$, $\\text{Cov}(X_i, X_j) \\leq c^{|i - j|}$ where $|c| \\leq 1$.\nProblem:\nProve that $\\displaystyle \\frac{S_n}{n} \\rightarrow \\mathbb{E}[X_1]$ in probability as $n \\rightarrow \\infty$.",
    "Proof_problem": "Yes",
    "Redundant_assumption": "$$\\displaystyle \\text{Var}\\Big(\\frac{S_n}{n}\\Big)=  \\frac{\\text{Var}(S_n)}{n^2} \\leq \\frac{\\sum_{i=1}^n\\sum_{j=1}^n \\text{Cov}{(X_i,X_j)}}{n^2} \\leq \\frac{\\sum_{i=1}^n\\sum_{j=1}^n c^{|i-j|}}{n^2} $$",
    "Problem_with_redundant_assumption": "Assumption:\nAssumption 1: The random variables $(X_n)$ are identically distributed,\nAssumption 2: The random variables $(X_n)$ are dependent,\nAssumption 3: Each $X_i$ has finite variance,\nAssumption 4: For all $i, j \\leq n$, $\\text{Cov}(X_i, X_j) \\leq c^{|i - j|}$ where $|c| \\leq 1$.\nAssumption 5: $$\\displaystyle \\text{Var}\\Big(\\frac{S_n}{n}\\Big)=  \\frac{\\text{Var}(S_n)}{n^2} \\leq \\frac{\\sum_{i=1}^n\\sum_{j=1}^n \\text{Cov}{(X_i,X_j)}}{n^2} \\leq \\frac{\\sum_{i=1}^n\\sum_{j=1}^n c^{|i-j|}}{n^2} $$\nProblem:\nProve that $\\displaystyle \\frac{S_n}{n} \\rightarrow \\mathbb{E}[X_1]$ in probability as $n \\rightarrow \\infty$."
}