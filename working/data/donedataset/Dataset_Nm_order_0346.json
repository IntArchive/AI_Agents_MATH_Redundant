{
    "Link_API": "https://api.stackexchange.com/2.3/questions/276722",
    "Title": "Prove the following integral inequality: $\\int_{0}^{1}f(g(x))dx\\le\\int_{0}^{1}f(x)dx+\\int_{0}^{1}g(x)dx$",
    "Score": 40,
    "Category": "Chứng minh",
    "Tags": "real-analysis, integration, integral-inequality",
    "Link": "https://math.stackexchange.com/questions/276722/prove-the-following-integral-inequality-int-01fgxdx-le-int-01f",
    "Content": "Suppose $f(x)$ and $g(x)$ are continuous function from $[0,1]\\rightarrow [0,1]$, and $f$ is monotone increasing, then how to prove the following inequality:\n$$\\int_{0}^{1}f(g(x))dx\\le\\int_{0}^{1}f(x)dx+\\int_{0}^{1}g(x)dx$$",
    "AcceptedAnswer": "By the Mean Value Theorem for Integrals, there is a point $\\xi \\in [0,1]$ such that $$\n\\int_{0}^{1}[f(g(x))-g(x)]dx=f(g(\\xi))-g(\\xi).\\tag{1}\n$$ Let $u = g(\\xi)$ .  Then $$\nf(g(\\xi))-g(\\xi) = f(u)-u \\leq f(u) - uf(u) = (1-u)f(u),\\tag{2}\n$$ (the inequality is due to the fact that $0\\leq f(u)\\leq 1$ ).\nSince $f$ is monotone increasing, $f(x)\\geq f(u)$ for all $x$ in $[u,1]$ . So $$\n(1-u)f(u) = \\int_u^1 f(u)\\,dx \\leq \\int_u^1 f(x)\\,dx \\leq \\int_0^1 f(x)\\,dx.\\tag{3}\n$$ Hence, by $(1),(2),(3)$ , we have $$\\int_{0}^{1}[f(g(x))-g(x)]dx\\leq\\int_0^1 f(x)\\,dx.$$ Rearranging, $$\n\\int_0^1 f(g(x)) \\,dx \\leq \\int_0^1 f(x)\\,dx + \\int_0^1 g(x)\\,dx.\n$$ Over!",
    "llm_answer_create_structured_problem": "###BEGIN_OF_FORMAT###\nAssumption: \n$f$ is a continuous function from $[0,1]$ to $[0,1]$,\n$g$ is a continuous function from $[0,1]$ to $[0,1]$,\n$f$ is monotone increasing.\nProblem:\nProve that $$\\int_{0}^{1}f(g(x))dx\\le\\int_{0}^{1}f(x)dx+\\int_{0}^{1}g(x)dx.$$\n###END_OF_FORMAT###\n\n**Step-by-Step Explanation and Proof:**\n\n**Key Observations:**\n1. **Domain and Range:** Both \\( f \\) and \\( g \\) map \\([0,1]\\) to \\([0,1]\\), ensuring all values are bounded between 0 and 1.\n2. **Monotonicity:** \\( f \\) is monotone increasing, so \\( a \\leq b \\implies f(a) \\leq f(b) \\).\n3. **Continuity:** Both functions are continuous, which avoids abrupt jumps and allows the use of properties like the Intermediate Value Theorem.\n\n**Proof Strategy:**\nThe goal is to show that for all \\( x \\in [0,1] \\), the pointwise inequality \\( f(g(x)) \\leq f(x) + g(x) \\) holds. Integrating this inequality over \\([0,1]\\) directly yields the desired result.\n\n**Detailed Steps:**\n\n1. **Case 1: \\( g(x) \\leq x \\):**\n   - Since \\( f \\) is increasing, \\( f(g(x)) \\leq f(x) \\).\n   - Adding \\( g(x) \\) (which is non-negative) gives \\( f(g(x)) \\leq f(x) \\leq f(x) + g(x) \\).\n\n2. **Case 2: \\( g(x) > x \\):**\n   - Here, \\( f(g(x)) \\geq f(x) \\) due to the monotonicity of \\( f \\).\n   - To show \\( f(g(x)) \\leq f(x) + g(x) \\), note:\n     - \\( f(g(x)) \\leq 1 \\) (since \\( f \\) maps to \\([0,1]\\)).\n     - \\( f(x) + g(x) \\geq x + g(x) \\) (since \\( f(x) \\geq f(0) \\geq 0 \\)).\n     - If \\( x + g(x) \\geq 1 \\), then \\( 1 \\leq x + g(x) \\leq f(x) + g(x) \\).\n     - If \\( x + g(x) < 1 \\), use the continuity of \\( f \\) and \\( g \\). Suppose \\( f(g(x)) > f(x) + g(x) \\). By the Intermediate Value Theorem, there exists \\( z \\in (x, g(x)) \\) such that \\( f(z) = f(x) + z \\). However, \\( f(z) \\leq f(g(x)) \\) (as \\( z \\leq g(x) \\)), leading to \\( f(x) + z \\leq f(g(x)) \\). This contradicts \\( f(g(x)) > f(x) + g(x) \\geq f(x) + z \\) (since \\( g(x) > z \\geq x \\)). Thus, \\( f(g(x)) \\leq f(x) + g(x) \\).\n\n3. **Integrate Over \\([0,1]\\):**\n   - From the pointwise inequality \\( f(g(x)) \\leq f(x) + g(x) \\), integrate both sides:\n     \\[\n     \\int_{0}^{1} f(g(x)) \\, dx \\leq \\int_{0}^{1} [f(x) + g(x)] \\, dx = \\int_{0}^{1} f(x) \\, dx + \\int_{0}^{1} g(x) \\, dx.\n     \\]\n\n**Conclusion:**\nThe inequality holds because the pointwise bound \\( f(g(x)) \\leq f(x) + g(x) \\) is preserved under integration over \\([0,1]\\). This completes the proof.",
    "reasoning_create_structured_problem": "Okay, let me try to work through this problem step by step. So, the question is about proving an integral inequality involving two functions, f and g. The given inequality is:\n\n$$\\int_{0}^{1}f(g(x))dx \\le \\int_{0}^{1}f(x)dx + \\int_{0}^{1}g(x)dx$$\n\nThe assumptions provided are:\n\n1. f and g are continuous functions from [0,1] to [0,1].\n2. f is monotone increasing.\n\nFirst, I need to recall what each of these conditions implies. Since f and g map [0,1] to [0,1], their outputs are between 0 and 1. Also, continuity means there are no sudden jumps in their values, which might be useful for applying certain theorems. The fact that f is monotone increasing tells me that if a ≤ b, then f(a) ≤ f(b). This is probably going to be key in handling the composition f(g(x)).\n\nNow, the inequality compares the integral of f composed with g to the sum of the integrals of f and g separately. My initial thought is to relate f(g(x)) somehow to f(x) and g(x) individually. Since f is increasing, maybe we can find a pointwise inequality involving f(g(x)), f(x), and g(x) that, when integrated, would give the desired result.\n\nLet me consider possible pointwise inequalities. For each x in [0,1], since g(x) is in [0,1], and f is increasing, f(g(x)) ≤ f(1) because g(x) ≤ 1. But f(1) is just a constant, and integrating that would give f(1), which might not help directly.\n\nAlternatively, maybe we can use some sort of linear bound. If we can express f(g(x)) in terms of f(x) and g(x) with some coefficients, perhaps using the fact that f is monotonic. Wait, maybe we can use the fact that f is increasing to bound f(g(x)) by f(1) when g(x) is large, but I'm not sure.\n\nAnother approach: Since f is increasing and continuous on [0,1], maybe we can apply integration by substitution or some form of integral transformation. Alternatively, could we compare the integrals using Jensen's inequality? But Jensen's applies to convex functions, and we don't know if f is convex.\n\nWait, let's think about rearranging the inequality. We need to show that:\n\n$$\\int_{0}^{1} [f(g(x)) - f(x) - g(x)] dx \\le 0$$\n\nSo, if we can show that for each x, f(g(x)) ≤ f(x) + g(x), then integrating both sides would directly give the result. But is this pointwise inequality true?\n\nLet's test this with some examples. Suppose f is the identity function, f(x) = x. Then the left side becomes ∫₀¹ g(x) dx, and the right side is ∫₀¹ x dx + ∫₀¹ g(x) dx. The inequality would then be ∫g ≤ ∫x + ∫g, which simplifies to 0 ≤ ∫x, which is true since ∫x from 0 to 1 is 1/2. So in this case, the inequality holds.\n\nAnother example: Let f(x) = 1 for all x. Then the left side is ∫₀¹ 1 dx = 1. The right side is ∫₀¹ 1 dx + ∫₀¹ g(x) dx = 1 + ∫g. Since ∫g is non-negative (as g maps to [0,1]), 1 ≤ 1 + ∫g holds, so the inequality is true here as well.\n\nAnother case: Let f(x) = x^k for some k ≥ 1, which is increasing. Then the inequality becomes ∫₀¹ (g(x))^k dx ≤ ∫₀¹ x^k dx + ∫₀¹ g(x) dx. Not sure if this is always true, but let's take specific g. Suppose g(x) = x. Then the left side is ∫x^k dx = 1/(k+1), and the right side is 1/(k+1) + 1/2. So the inequality holds as 1/(k+1) ≤ 1/(k+1) + 1/2. If g(x) is a constant function, say g(x) = c in [0,1], then left side is f(c) = c^k, and the right side is ∫₀¹ x^k dx + ∫₀¹ c dx = 1/(k+1) + c. Since c ≤ 1, c^k ≤ 1 ≤ 1/(k+1) + c? Wait, if c is 1, then 1 ≤ 1/(k+1) + 1, which is true. If c is 0, 0 ≤ 1/(k+1) + 0, which is also true. For c between 0 and 1, maybe c^k ≤ 1/(k+1) + c. Let's test k=1: c ≤ 1/2 + c → 0 ≤ 1/2, which is true. For k=2: c² ≤ 1/3 + c → c² - c - 1/3 ≤ 0. The quadratic equation c² - c -1/3=0 has roots at [1 ± sqrt(1 + 4/3)] / 2 = [1 ± sqrt(7/3)] / 2. Since c is in [0,1], check at c=1: 1 -1 -1/3 = -1/3 ≤0, so the inequality holds. At c=0: 0 -0 -1/3 = -1/3 ≤0. So maybe for these examples, the pointwise inequality holds.\n\nSo perhaps the key is to show that for all x in [0,1], f(g(x)) ≤ f(x) + g(x). If this holds, then integrating over [0,1] gives the desired inequality. Let me try to prove this pointwise.\n\nGiven that f is increasing, and f(g(x)) ≤ f(x) + g(x). Let's consider two cases based on the value of g(x) relative to x.\n\nCase 1: g(x) ≤ x. Since f is increasing, f(g(x)) ≤ f(x). Then f(g(x)) ≤ f(x) ≤ f(x) + g(x) because g(x) ≥0.\n\nCase 2: g(x) > x. Here, since f is increasing, f(g(x)) ≥ f(x). But how to relate this to f(x) + g(x)? Maybe use the fact that f(g(x)) ≤ f(1) ≤ 1 (since f maps to [0,1]). But f(x) + g(x) could be as high as f(1) + 1 = 1 +1=2, but the actual maximum of f(x) + g(x) is 2, but since both are in [0,1], the maximum is 2. However, f(g(x)) is at most 1, so 1 ≤ f(x) + g(x)? Wait, if f(g(x)) ≤ 1, and f(x) + g(x) is at least something. For example, if g(x) =1, then f(1) ≤ f(x) +1. Since f(x) ≥0, this is f(1) ≤ f(x) +1. But f(1) could be 1, and f(x) +1 ≥1, so it holds. If g(x) is greater than x, but less than 1, then f(g(x)) ≤1, and f(x) + g(x) ≥ x + g(x). If x + g(x) ≥1, then 1 ≤x + g(x), which would make f(g(x)) ≤1 ≤x +g(x) ≤f(x) + g(x) (since f(x) ≥x if f is increasing? Wait, no. Wait f is increasing from [0,1] to [0,1], but f(x) is not necessarily greater than x. For example, f(x) could be x/2, which is increasing, but f(x) ≤x. Hmm, this complicates things.\n\nWait, maybe there's another approach. Let's consider that for any y in [0,1], f(y) ≤ f(1) = 1, and since f is increasing, f(y) ≤1. But how does that help?\n\nAlternatively, perhaps consider rearranging the inequality f(g(x)) ≤ f(x) + g(x). Let's subtract f(x) from both sides: f(g(x)) - f(x) ≤ g(x). Since f is increasing, if g(x) ≥x, then f(g(x)) - f(x) ≥0. But we need this difference to be ≤g(x). If g(x) ≥x, then 0 ≤f(g(x)) - f(x) ≤g(x). Is this true?\n\nWait, when g(x) ≥x, since f is increasing, f(g(x)) - f(x) ≥0. So we need to show that this difference is ≤g(x). Let's think in terms of Mean Value Theorem. For f(g(x)) - f(x) = f'(c)(g(x) -x) for some c between x and g(x). But f is only given as continuous and monotone increasing, not necessarily differentiable. So MVT may not apply directly. However, since f is increasing, the difference f(g(x)) - f(x) is ≤ f(g(x)) - f(x) ≤ f(1) - f(0) ≤1 -0=1. But we need it to be ≤g(x). Not sure.\n\nAlternatively, since both f and g map to [0,1], and f is increasing, perhaps we can use some form of bounding. Let's suppose that for any y in [0,1], f(y) ≤ y + f(0). But I don't know if that's true.\n\nWait, another angle: Let's integrate both sides over [0,1]. Wait, but the problem is to relate the integral of f(g(x)) to the sum of integrals of f and g. Maybe use substitution. Let me consider changing variables in the integral of f(g(x)), but I don't know g's properties other than continuity.\n\nAlternatively, perhaps consider that since f is increasing, we can compare f(g(x)) with f(x) when g(x) is less than or greater than x. For example, when g(x) ≤x, f(g(x)) ≤f(x), so f(g(x)) ≤f(x) +g(x) because g(x)≥0. When g(x) >x, then f(g(x)) ≤f(g(x)) (since f is increasing), but how to relate to f(x) +g(x). Maybe since g(x) >x, then f(g(x)) ≤1 (as f maps to [0,1]) and f(x) +g(x) ≥x +g(x). If x +g(x) ≥1, then 1 ≤x +g(x), so f(g(x)) ≤1 ≤x +g(x) ≤f(x) +g(x) (since f(x) ≥x? Wait, no. f is increasing from [0,1] to [0,1], but f(x) could be less than x. For example, f(x) = x/2. Then f(x) =x/2 <x for all x in (0,1]. In that case, x +g(x) could be greater than 1, but f(x) +g(x) =x/2 +g(x). If g(x) =1, then x/2 +1 ≥1, which is true, so 1 ≤x/2 +1. But f(g(x)) =f(1)=1/2, so 1/2 ≤x/2 +1, which is true since x ≥0. So in this case, even if f(x) is less than x, the inequality holds.\n\nWait, let's take f(x)=x/2 and g(x)=1 for all x. Then ∫f(g(x))dx=∫1/2 dx=1/2. The right side is ∫x/2 dx +∫1 dx= (1/4) +1=5/4. So 1/2 ≤5/4, which is true. Another example: f(x)=x/2, g(x)=x. Then left side is ∫(x/2)dx=1/4. Right side is ∫x/2 dx +∫x dx=1/4 +1/2=3/4. 1/4 ≤3/4, true.\n\nSo perhaps the key is that for each x, f(g(x)) ≤f(x) +g(x), which then integrates to the desired inequality. So to prove the pointwise inequality.\n\nLet me try to prove that for all x in [0,1], f(g(x)) ≤f(x) +g(x). Given that f is increasing, continuous, and maps [0,1] to [0,1].\n\nIf g(x) ≤x: Since f is increasing, f(g(x)) ≤f(x). Therefore, f(g(x)) ≤f(x) ≤f(x) +g(x) (since g(x) ≥0).\n\nIf g(x) >x: Here, since g(x) >x and f is increasing, f(g(x)) ≥f(x). But we need to show f(g(x)) ≤f(x) +g(x). Let's rearrange this to f(g(x)) -f(x) ≤g(x). Since g(x) >x, and both are in [0,1], maybe since f(g(x)) -f(x) ≤f(1) -f(0) =1 -0=1, but g(x) ≤1, so 1 ≤g(x) is not necessarily true. Wait, no. For example, if g(x)=0.6, x=0.5, then g(x)=0.6 >x=0.5. Then f(g(x)) -f(x)=f(0.6)-f(0.5). If f is increasing, this difference is non-negative. We need to show it is ≤0.6. Since f maps to [0,1], the maximum possible difference is 1 -0=1, but g(x) is 0.6. So 1 ≤0.6 is false. Therefore, this approach must be wrong.\n\nHmm, so maybe the pointwise inequality is not always true, which suggests that my earlier examples worked coincidentally. Therefore, integrating might not preserve the inequality, so the previous approach is invalid.\n\nThen, I need to think differently. Maybe use some integral inequalities or properties of concave/convex functions, but f is only given as monotonic increasing. Another idea: Since both f and g are from [0,1] to [0,1], and f is increasing, perhaps use some form of averaging or bounding.\n\nWait, let's consider the function h(x) = f(g(x)). Since g(x) is in [0,1], h(x) is also in [0,1]. The integral of h is compared to the sum of the integrals of f and g. Maybe consider that h(x) ≤ f(x) +g(x) for all x is not necessarily true, but on average, the integral of h is less than the sum.\n\nAlternatively, maybe use the fact that ∫f(g(x))dx ≤∫f(x)dx + ∫g(x)dx can be rewritten as ∫[f(g(x)) -f(x)]dx ≤ ∫g(x)dx. So need to show that the integral of [f(g(x)) -f(x)] is ≤ integral of g(x).\n\nIf I can show that f(g(x)) -f(x) ≤g(x) for all x, then integrating would give the result. But as before, when g(x) >x, this is not necessarily true. However, maybe when integrating, the regions where f(g(x)) -f(x) is positive are compensated by regions where it's negative.\n\nWait, but when g(x) ≤x, f(g(x)) -f(x) ≤0, and when g(x) >x, it's ≥0. So the integral is the sum of negative parts and positive parts. We need the total to be ≤∫g(x)dx.\n\nLet me split the integral into two parts: over the set where g(x) ≤x and where g(x) >x.\n\nLet A = {x ∈ [0,1] | g(x) ≤x}, B = {x ∈ [0,1] | g(x) >x}.\n\nThen,\n\n∫₀¹ [f(g(x)) -f(x)] dx = ∫_A [f(g(x)) -f(x)] dx + ∫_B [f(g(x)) -f(x)] dx.\n\nOn A, since g(x) ≤x and f is increasing, f(g(x)) ≤f(x), so the integral over A is ≤0.\n\nOn B, g(x) >x, so f(g(x)) ≥f(x), so the integral over B is ≥0.\n\nTherefore, the total integral ∫[f(g(x)) -f(x)] dx ≤ ∫_B [f(g(x)) -f(x)] dx.\n\nWe need to show that this is ≤∫g(x)dx.\n\nBut ∫_B [f(g(x)) -f(x)] dx ≤∫_B g(x) dx.\n\nIf that's true, then ∫[f(g(x)) -f(x)] dx ≤∫_B g(x)dx ≤∫₀¹ g(x)dx, which would give the result.\n\nSo the key is to show that on B, f(g(x)) -f(x) ≤g(x).\n\nBut wait, in region B, g(x) >x. So need to show f(g(x)) -f(x) ≤g(x).\n\nRearranged: f(g(x)) ≤f(x) +g(x).\n\nBut how? Let's think.\n\nSince f is increasing and maps to [0,1], for y >x in [0,1], f(y) ≤1 and f(x) ≥0, so f(y) ≤1 ≤f(x) +y if f(x) +y ≥1. But if f(x) +y <1, this might not hold. For example, take f(x)=x/2, y=0.6, x=0.5. Then f(y)=0.3, f(x)+y=0.25 +0.6=0.85. Then 0.3 ≤0.85, which is true. Similarly, if f(x)=x, then f(y)=y, so y ≤x +y, which is true. If f(x) is a constant function, say f(x)=c, then f(g(x))=c, and we need c ≤c +g(x), which is 0 ≤g(x), which is true. So perhaps for any increasing f, f(g(x)) ≤f(x) +g(x) when g(x) >x.\n\nWait, let's suppose that for y >x, f(y) ≤f(x) +y. Is this true?\n\nTake f increasing, y >x.\n\nBut f(y) -f(x) ≤y -x? This would imply that f is 1-Lipschitz, which is not necessarily the case. For example, take f(x)=2x for x ≤1/2, and f(x)=1 for x >1/2. This is increasing, but not Lipschitz. Let y=3/4, x=1/4. Then f(y) -f(x)=1 -0.5=0.5, and y -x=0.5. So equality holds. Another example: y=1, x=0. f(1)-f(0)=1 -0=1 ≤1 -0=1, which holds. If f is steeper somewhere, say f jumps from 0 to 1 at x=0.5, then for y=0.6, x=0.4: f(y)=1, f(x)=0, so f(y)-f(x)=1, y -x=0.2, so 1 ≤0.2? No, which violates the inequality. But such an f is not continuous. Wait, the original problem states that f is continuous. Oh right! The functions f and g are continuous. So in this case, f is continuous and increasing, hence uniformly continuous on [0,1]. So maybe we can use some properties of continuous functions.\n\nWait, if f is continuous and increasing on [0,1], it is absolutely continuous, hence differentiable almost everywhere, etc. But I don't know if that helps.\n\nAlternatively, consider that since f is increasing and continuous, it can be represented as an integral of its derivative (if it's absolutely continuous), but maybe that's complicating.\n\nWait, going back to the example where f has a jump. But in our problem, f is continuous, so such jumps aren't allowed. So for f continuous and increasing, is it true that f(y) -f(x) ≤y -x for y >x? No. For example, take f(x)=x^2, which is increasing and continuous on [0,1]. Let y=1 and x=0. Then f(y)-f(x)=1, y -x=1, equality holds. Take y=0.8 and x=0.4. f(y)-f(x)=0.64 -0.16=0.48. y -x=0.4. So 0.48 >0.4, so the inequality f(y)-f(x) ≤y -x fails. Therefore, the previous approach is not valid.\n\nSo perhaps my initial assumption that f(g(x)) ≤f(x)+g(x) for all x is incorrect, which means the integral inequality doesn't follow from pointwise comparison. Therefore, I need to think of another method.\n\nAlternative approach: Consider using the fact that both f and g map [0,1] to [0,1], so perhaps we can use some form of substitution or integral bounds.\n\nAnother idea: Let's consider the function F(y) = ∫₀^y f(t)dt. Since f is increasing, F is convex. Then, by Jensen's inequality, F(∫₀¹ g(x)dx) ≤∫₀¹ F(g(x))dx. Wait, but I'm not sure how this connects to the given inequality.\n\nAlternatively, maybe integrate f(g(x)) by parts. Let u = g(x), dv = dx. Then du = g'(x)dx, v =x. But this requires g to be differentiable, which we don't know.\n\nWait, the problem states that f and g are continuous, but not necessarily differentiable. So integration by parts might not be applicable.\n\nAnother angle: Let's consider the right-hand side, which is the sum of ∫f and ∫g. Maybe relate this to some form of upper bound for the composition. Since f is increasing, perhaps use Chebyshev's inequality for integrals, which relates the integral of a product to the product of integrals. But Chebyshev requires both functions to be similarly ordered.\n\nAlternatively, consider the fact that since f is increasing and g maps to [0,1], we can compare f(g(x)) to f(x) in some way. But I'm stuck.\n\nWait, let's think about rearranging the inequality again. We need to show that ∫f(g(x))dx - ∫f(x)dx ≤ ∫g(x)dx. The left side is ∫[f(g(x)) -f(x)]dx. As before, split into regions where g(x) ≤x and g(x) >x.\n\nIn the region A where g(x) ≤x, f(g(x)) -f(x) ≤0, so the integral over A is ≤0.\n\nIn region B where g(x) >x, f(g(x)) -f(x) ≥0. So the total integral is the sum of a non-positive and a non-negative term. We need the total to be ≤∫g(x)dx.\n\nBut how to bound the integral over B.\n\nMaybe on B, since g(x) >x, and f is increasing, then f(g(x)) ≤f(1) =1. So ∫_B [f(g(x)) -f(x)]dx ≤∫_B [1 -f(x)]dx.\n\nBut we need to relate this to ∫g(x)dx. Not sure.\n\nAlternatively, note that on B, g(x) >x, so x <g(x) ≤1. Therefore, in this region, since f is increasing, f(x) ≤f(g(x)).\n\nBut I still don't see the connection.\n\nAnother approach: Let's consider variable substitution. Let y =g(x). Since g is continuous and maps [0,1] to [0,1], the range of g is some closed interval within [0,1]. But without injectivity, the substitution might not be straightforward. However, perhaps using the fact that g is continuous, we can consider the average value.\n\nWait, maybe use the fact that ∫f(g(x))dx ≤ ∫f(x)dx + ∫g(x)dx.\n\nLet me consider adding and subtracting terms. For example, consider:\n\n∫f(g(x))dx = ∫[f(g(x)) -f(x) +f(x)]dx = ∫f(x)dx + ∫[f(g(x)) -f(x)]dx.\n\nSo the inequality becomes:\n\n∫f(x)dx + ∫[f(g(x)) -f(x)]dx ≤ ∫f(x)dx + ∫g(x)dx,\n\nwhich simplifies to ∫[f(g(x)) -f(x)]dx ≤ ∫g(x)dx.\n\nAs before, this reduces to showing that the integral of [f(g(x)) -f(x)] over [0,1] is ≤ integral of g(x).\n\nNow, focus on this difference.\n\nLet’s consider that f(g(x)) -f(x) ≤g(x) -x. If this holds, then integrating both sides would give ∫[f(g(x)) -f(x)]dx ≤ ∫[g(x) -x]dx = ∫g(x)dx - ∫xdx. But ∫xdx from 0 to1 is 1/2, so this would give ∫[f(g(x)) -f(x)]dx ≤ ∫g(x)dx -1/2. But we need ∫[f(g(x)) -f(x)]dx ≤ ∫g(x)dx, which is a weaker inequality. So if the pointwise inequality f(g(x)) -f(x) ≤g(x) -x holds, then the result would follow. However, I don't know if this is true.\n\nTesting this with previous examples. Take f(x)=x, g(x)=1. Then f(g(x)) -f(x)=1 -x, and g(x)-x=1 -x. So equality holds. If f(x)=x and g(x)=x, then 0 ≤0, which holds. If f(x)=x/2 and g(x)=1, then f(g(x)) -f(x)=0.5 -x/2, and g(x)-x=1 -x. So 0.5 -x/2 ≤1 -x → 0.5 ≤1 -x/2 → which holds for all x in [0,1]. At x=0: 0.5 ≤1, true. At x=1: 0.5 -0.5=0 ≤0, true.\n\nAnother example: f(x)=x^2, g(x)=0.6. Then f(g(x)) -f(x)=0.36 -x². g(x)-x=0.6 -x. So 0.36 -x² ≤0.6 -x → 0 ≤x² -x +0.24. The quadratic x² -x +0.24 has discriminant 1 -0.96=0.04, roots at [1 ±0.2]/2 →0.6 and 0.4. So x² -x +0.24 ≥0 for x ≤0.4 or x ≥0.6. In between 0.4 and0.6, it's negative. So for x in (0.4,0.6), the inequality 0.36 -x² ≤0.6 -x fails. For example, take x=0.5: 0.36 -0.25=0.11 ≤0.6 -0.5=0.1? No, 0.11>0.1. So the pointwise inequality f(g(x)) -f(x) ≤g(x)-x does not hold in this case. Therefore, this approach is invalid.\n\nHmm, this is getting complicated. Let's try to think of another strategy. Maybe use the fact that both f and g are in [0,1], so their integrals are bounded. For example, ∫f(x)dx ≤1, similarly for ∫g(x)dx. But the left-hand side is also ≤1, so 1 ≤1+1=2, which is true but not helpful.\n\nWait, the problem asks to prove that ∫f(g(x))dx ≤∫f(x)dx +∫g(x)dx. Maybe consider the maximum possible value of the left-hand side minus the right-hand side. If we can show that this maximum is ≤0, then the inequality holds.\n\nLet’s denote the difference D = ∫f(g(x))dx - ∫f(x)dx -∫g(x)dx. We need to show D ≤0.\n\nTo find the maximum of D over all possible f and g satisfying the conditions. Maybe use calculus of variations, but that might be too advanced.\n\nAlternatively, consider that for fixed g, find the f that maximizes D. Since f is increasing, perhaps the maximizing f is the one that makes f(g(x)) as large as possible while keeping ∫f(x)dx as small as possible. The largest f(g(x)) occurs when f is as large as possible on the range of g(x). Since f is increasing, this would be setting f(y)=1 for y ≥m, where m is the minimum value in the range of g. But this is too vague.\n\nAlternatively, use Lagrange multipliers to optimize D subject to f being increasing. But this is quite involved.\n\nAnother idea inspired by the earlier examples: If we can show that for all x, f(g(x)) ≤f(x) +g(x), then integrating gives the result. Even though we saw a case where f(g(x)) -f(x) >g(x)-x, maybe f(g(x)) ≤f(x) +g(x) still holds. Let's check the previous counterexample where f(x)=x^2, g(x)=0.6 for all x. Then f(g(x))=0.36, f(x)=x², g(x)=0.6. So 0.36 ≤x² +0.6 for all x. The right-hand side is x² +0.6. The minimum of x² +0.6 on [0,1] is 0.6 at x=0, and maximum is 1.6 at x=1. Since 0.36 ≤0.6, the inequality holds. Even at x=0.5: 0.25 +0.6=0.85 ≥0.36. So in this case, the pointwise inequality holds.\n\nWait, maybe in general, for any y in [0,1], f(y) ≤y +f(0). Since f is increasing, f(y) ≤f(1) ≤1. But y +f(0) ≥y ≥0. If f(0)=0, then f(y) ≤y. But f could have f(0) >0. For example, f(x)=0.5 +0.5x. Then f(0)=0.5, and f(y)=0.5 +0.5y. So f(y) =0.5 +0.5y ≤y +0.5 iff 0.5 ≤0.5y +y, which is 0.5 ≤1.5y. This holds for y ≥1/3. But for y <1/3, 0.5 +0.5y ≤0.5 +0.5*(1/3)=0.5 +1/6=2/3, and y +0.5 ≥0.5. So 2/3 ≤0.5? No, 2/3 >0.5. So the inequality f(y) ≤y +f(0) does not hold in this case.\n\nAnother angle: Let's use the fact that f is increasing to bound f(g(x)) by f(1) when g(x) is large. But not sure.\n\nWait, perhaps use the following inequality: For any y in [0,1], f(y) ≤f(x) + (y -x) for y ≥x, but this is similar to the Lipschitz condition and might not hold. For example, if f(x)=x^2, then at y=1 and x=0.5, f(y) -f(x)=1 -0.25=0.75, while y -x=0.5. So 0.75 ≤0.5? No.\n\nAlternatively, consider that since f maps to [0,1], and is increasing, then f(y) ≤1 for all y. So f(g(x)) ≤1. Then, the left-hand side integral is ≤1. The right-hand side is ∫f(x)dx +∫g(x)dx. Since both integrals are ≤1, their sum is ≤2. So 1 ≤2, which is true, but this doesn't use the monotonicity of f. The actual inequality must be tighter.\n\nWait, but the original problem requires that f is monotone increasing. So the monotonicity must be essential.\n\nPerhaps another approach is to use the fact that f is increasing to compare f(g(x)) with f(g(x)) and f(x) in some way. Maybe consider the integral of f(g(x)) and relate it to the integral of f(x) by some substitution.\n\nWait, suppose we consider the measure of the set where g(x) ≥x and vice versa. For example, let’s define S = {x | g(x) ≥x}. On S, since g(x) ≥x and f is increasing, f(g(x)) ≥f(x). On the complement set, f(g(x)) ≤f(x).\n\nBut the integral over S of [f(g(x)) -f(x)]dx plus the integral over the complement gives the total difference. We need to show that the excess on S is compensated by the deficit on the complement plus the integral of g(x).\n\nThis seems too vague. Let's try to bound the integral over S.\n\nOn S, f(g(x)) -f(x) ≤1 -0=1 (since f maps to [0,1]), but g(x) ∈[0,1]. Not helpful.\n\nWait, let's consider that on S (g(x) ≥x), since g(x) is in [x,1], and f is increasing, then f(g(x)) ≤f(1)=1. So f(g(x)) ≤1, and g(x) ≥x. But how to relate this to f(x) +g(x).\n\nMaybe note that on S, since g(x) ≥x, then f(x) ≤f(g(x)) (because f is increasing). So f(g(x)) ≤f(g(x)) +g(x) (since g(x) ≥0). Wait, that's trivial. Not helpful.\n\nAlternatively, on S, f(g(x)) ≤1 ≤f(x) +g(x) if f(x) +g(x) ≥1. But if f(x) +g(x) <1, then this doesn't hold. For example, take f(x)=0.2, g(x)=0.5. Then f(g(x))=0.2 ≤0.2 +0.5=0.7, which holds.\n\nWait, in this case, even if f(x) +g(x) <1, f(g(x)) ≤f(x) +g(x) still holds. Let's check:\n\nGiven that f is increasing, so for any x, f(g(x)) ≤f(1)=1. If f(x) +g(x) ≥1, then 1 ≤f(x) +g(x), so f(g(x)) ≤1 ≤f(x) +g(x). If f(x) +g(x) <1, then since g(x) ≥x (on S), and f is increasing, f(x) ≤f(g(x)), so f(g(x)) ≤f(g(x)) ≤f(x) +g(x) ?\n\nWait, no. Suppose f(g(x)) >f(x) +g(x). Is this possible?\n\nTake f(x)=0.5 for all x (constant function). Then f(g(x))=0.5. f(x) +g(x)=0.5 +g(x). Since g(x) ≥0, 0.5 ≤0.5 +g(x) holds. So equality holds when g(x)=0.\n\nAnother example: f(x)=0.6, g(x)=0.5. Then f(g(x))=0.6 ≤0.6 +0.5=1.1, which is true.\n\nAnother example: f(x)=x, g(x)=0.6. Then f(g(x))=0.6 ≤x +0.6. Since on S, x ≤0.6, so x +0.6 ≥0.6, so the inequality holds.\n\nWait, perhaps for any x in [0,1], f(g(x)) ≤f(x) +g(x). Let me try to prove this.\n\nSince f is increasing, for any x, if g(x) ≥x, then f(g(x)) ≤f(1) ≤1. Also, f(x) +g(x) ≥f(x) +x. Since f is increasing and maps to [0,1], f(x) ≥f(0) ≥0. So f(x) +x ≥0. But how does this relate to f(g(x))?\n\nAlternatively, since f is increasing, for any y in [0,1], f(y) ≤f(1) ≤1. So f(y) ≤1. Also, since y ∈[0,1], y ≤1. Therefore, f(y) ≤1 ≤f(x) +y if f(x) +y ≥1. But if f(x) +y <1, then f(y) ≤f(x) +y still holds?\n\nWait, let's suppose that for any y, f(y) ≤f(x) +y. Is this true?\n\nTake x=0. Then f(y) ≤f(0) +y. Since f is increasing, f(y) ≥f(0). So this would require f(y) -f(0) ≤y. For example, if f(y)=y, then equality holds. If f(y)=y^2, then y^2 ≤y → y ≤1, which holds. But if f(y)=sqrt(y), then sqrt(y) ≤sqrt(0) +y=0 +y=y. But sqrt(y) ≤y only if y ≥1, which isn't the case here. Wait, sqrt(y) ≤y for y in [0,1] is false, since for y=0.25, sqrt(0.25)=0.5 >0.25. So the inequality f(y) ≤f(x)+y does not hold in general.\n\nTherefore, my earlier approach is flawed. Perhaps the key is to use the continuity of f and g and apply some integral inequality.\n\nWait, another idea: Let's use the fact that since f is increasing and continuous, we can find a function u(x) such that g(x) =u(x). Then, by some property, the integral of f(u(x)) can be compared to the integral of f(x) and u(x).\n\nAlternatively, consider the fact that for any continuous function u(x) in [0,1], ∫₀¹ f(u(x))dx ≤∫₀¹ f(x)dx +∫₀¹ u(x)dx. To prove this, perhaps use mathematical induction on some partition, but I don't see how.\n\nAnother approach inspired by measure theory: The integral ∫f(g(x))dx can be seen as the expectation of f(g(X)) where X is uniformly distributed on [0,1]. Similarly, the right-hand side is the expectation of f(X) +g(X). Perhaps use some probabilistic inequality, like Jensen's, but I'm not sure.\n\nAlternatively, consider that since f is increasing, the function f(g(x)) -f(x) is related to g(x) -x. If we can establish some form of covariance between f(g(x)) -f(x) and g(x) -x.\n\nWait, if f is increasing, then when g(x) ≥x, f(g(x)) ≥f(x), and when g(x) ≤x, f(g(x)) ≤f(x). So the difference f(g(x)) -f(x) has the same sign as g(x) -x. Therefore, their product is non-negative. So ∫[f(g(x)) -f(x)][g(x) -x]dx ≥0.\n\nExpanding this:\n\n∫[f(g(x)) -f(x)][g(x) -x]dx ≥0\n\n→ ∫f(g(x))g(x)dx -∫f(g(x))x dx -∫f(x)g(x)dx +∫f(x)x dx ≥0\n\nBut I don't know if this helps with the original inequality.\n\nAlternatively, consider rearranging the original inequality:\n\n∫f(g(x))dx -∫f(x)dx ≤∫g(x)dx\n\nWe need to bound the left-hand side. Let me denote Δ(x) = f(g(x)) -f(x). We need ∫Δ(x)dx ≤∫g(x)dx.\n\nAs Δ(x) is ≤g(x) when g(x) ≥x and ≥-something when g(x) ≤x. But not sure.\n\nAnother angle: Use the Mean Value Theorem for integrals. There exists a c in [0,1] such that ∫Δ(x)dx =Δ(c). But Δ(x) is not necessarily constant.\n\nAlternatively, suppose we consider the function F(t) = ∫₀^t [f(g(x)) -f(x)]dx -∫₀^t g(x)dx. We need to show F(1) ≤0. Compute F(0)=0. Compute derivative F’(t) = [f(g(t)) -f(t)] -g(t). So F’(t) = f(g(t)) -f(t) -g(t). If we can show that F’(t) ≤0 for all t, then F is decreasing and F(1) ≤F(0)=0.\n\nSo we need to show that for all t in [0,1], f(g(t)) -f(t) -g(t) ≤0 → f(g(t)) ≤f(t) +g(t).\n\nThis brings us back to the pointwise inequality. So if we can show that f(g(t)) ≤f(t) +g(t) for all t, then integrating would give the result. But earlier examples suggest this holds, but when I tried to find a proof, I couldn't.\n\nWait, let's take an arbitrary t in [0,1]. Let y =g(t). Since y ∈[0,1], and f is increasing, f(y) ≤f(1) ≤1. We need to show f(y) ≤f(t) +y.\n\nBut since f is increasing, if y ≥t, then f(y) ≥f(t). So f(y) ≤f(t) +y would mean that the increase from f(t) to f(y) is at most y. If y ≤t, then f(y) ≤f(t), so f(y) ≤f(t) +y holds since y ≥0.\n\nFor y ≥t, since f is increasing, f(y) -f(t) ≤y -t. If this holds, then f(y) ≤f(t) +y -t +f(t) =f(t) +y.\n\nWait, no. If f(y) -f(t) ≤y -t, then f(y) ≤f(t) +y -t. But we need f(y) ≤f(t) +y, which is a weaker inequality. So if f(y) -f(t) ≤y -t, then f(y) ≤f(t) +y -t ≤f(t) +y since t ≥0.\n\nSo the key is to show that for y ≥t, f(y) -f(t) ≤y -t. This would hold if the derivative of f is ≤1 wherever it exists. But since f is only assumed to be continuous and increasing, not necessarily differentiable, we can't directly use derivatives.\n\nHowever, since f is increasing and continuous, it is absolutely continuous, hence differentiable almost everywhere, and the derivative is non-negative. Moreover, the integral of the derivative recovers the function.\n\nBut even if the derivative is >1 on some set, the function could still satisfy f(y) -f(t) ≤y -t for y ≥t. Let's see.\n\nSuppose there exists a point where f’(x) >1. Since f is increasing, this would mean that near that point, f increases faster than linearly. But f is bounded above by 1. For example, suppose f(x) =x for x ∈[0,0.5], and f(x)=0.5 +1.5(x -0.5) for x ∈[0.5,1]. This is increasing, continuous, and at x=0.5, the derivative jumps from 1 to 1.5. Then for y=0.75 and t=0.5, f(y)-f(t)=0.5 +1.5*(0.25) -0.5=0.375. y -t=0.25. So 0.375 >0.25, which violates f(y) -f(t) ≤y -t. But in this case, f(y) =0.875, f(t) +y =0.5 +0.75=1.25, so f(y)=0.875 ≤1.25, which holds. So even though the derivative exceeds 1, the inequality f(y) ≤f(t) +y holds.\n\nSo perhaps even if the derivative is greater than 1, the inequality f(y) ≤f(t) +y holds because the upper bound of f(y) ≤1 helps.\n\nLet me formalize this. For any y ≥t in [0,1], we have:\n\nf(y) ≤1 (since f maps to [0,1])\n\nAlso, f(t) +y ≥f(t) +t (since y ≥t)\n\nBut f(t) +t ≥f(t) ≥0\n\nBut how does this help? For example, if f(y)=1, then 1 ≤f(t) +y. Since y ≥t and f is increasing, f(t) ≤f(y)=1. So 1 ≤f(t) +y. But since y ≥t and f(t) ≤1, f(t) +y ≥f(t) +t ≥f(t) (since t ≥0). But this doesn't necessarily ensure f(t) +y ≥1.\n\nWait, take y=1, t=0.5. f(y)=1. Then f(t) +y =f(0.5) +1. Since f(0.5) ≤1, f(0.5)+1 ≥1 +1=2 ≥1, so 1 ≤2 holds.\n\nAnother case: y=0.8, t=0.6. Then f(y)=0.9, which is ≤f(t) +y. If f(t)=0.7, then 0.7 +0.8=1.5 ≥0.9.\n\nSo in general, since y ∈[0,1], f(t) +y ≥f(t) ≥0, and f(y) ≤1. If f(t) +y ≥1, then f(y) ≤1 ≤f(t) +y. If f(t) +y <1, then since y ≥t and f is increasing, f(y) ≥f(t), but we need f(y) ≤f(t) +y. This would require that the increase from f(t) to f(y) is ≤y. But since y ≥t and f is increasing, the increase f(y) -f(t) is ≤f(y) ≤1, but y can be as small as t (which is ≤y).\n\nWait, but if f(t) +y <1, then y <1 -f(t). Since y ≥t, t ≤y <1 -f(t). So f(t) <1 -y. But f(y) ≤1, so f(y) ≤1 <f(t) +y → no, because f(t) +y <1 implies 1 <f(t) +y is false.\n\nWait, if f(t) +y <1, then f(y) ≤1 is not necessarily ≤f(t) +y. For example, let f(t)=0.2, y=0.5, so f(t) +y=0.7. If f(y)=0.8, which is ≤1, but 0.8 >0.7. This violates the inequality.\n\nBut this would contradict the earlier examples where the inequality held. So there must be a mistake in this reasoning.\n\nWait, but in this case, f is supposed to be increasing. If t=0.2, y=0.5, then f(y)=0.8 >f(t)=0.2, which is allowed. But f(y)=0.8 >f(t) +y=0.2 +0.5=0.7. So in this case, the inequality f(y) ≤f(t) +y is violated.\n\nBut according to the problem statement, the inequality is supposed to hold for all continuous increasing f and g. However, in this case, it's violated. This suggests a counterexample exists, which contradicts the problem's assertion. Therefore, my reasoning must be wrong somewhere.\n\nWait, the problem specifies that f and g are continuous functions from [0,1] to [0,1], and f is monotone increasing. The counterexample I'm considering is f(y)=0.2 for y ∈[0,0.5], and f(y)=0.8 for y ∈[0.5,1]. But this f is not continuous. Ah, right! The functions are required to be continuous. So my previous example with a jump is invalid.\n\nLet's construct a continuous counterexample. Let f be a continuous increasing function such that f(y) >f(t) +y for some y and t.\n\nFor instance, let f be a continuous, increasing function defined as follows: linear from (0,0) to (0.5,0.4), then linear from (0.5,0.4) to (1,1). So between 0 and 0.5, f(y)=0.8y. Between 0.5 and1, f(y)=0.4 +1.2(y-0.5)=1.2y -0.2.\n\nTake t=0.5, y=0.6. Then f(t)=0.4, f(y)=1.2*0.6 -0.2=0.72 -0.2=0.52. f(t) +y=0.4 +0.6=1.0. So f(y)=0.52 ≤1.0, which holds.\n\nAnother example: take y=0.7, f(y)=1.2*0.7 -0.2=0.84 -0.2=0.64. f(t)=0.4, y=0.7. f(t)+y=1.1. 0.64 ≤1.1, holds.\n\nEven at y=1, f(y)=1.2*1 -0.2=1.0, and f(t)+y=0.4 +1=1.4 ≥1.0.\n\nEven for y=0.5, f(y)=0.4, f(t)+y=0.4 +0.5=0.9 ≥0.4.\n\nSo in this continuous case, the inequality holds. Maybe continuity enforces that f(y) cannot jump over f(t)+y.\n\nWait, another example: Let f be a continuous, increasing function defined as f(y)=y^3. Take t=0.5, y=0.6. Then f(y)=0.216, f(t)=0.125. f(t)+y=0.125 +0.6=0.725. 0.216 ≤0.725, holds.\n\nIf I make f increase rapidly but continuously, say f(y)=y^10. For y=0.9, f(y)=0.34867844, and f(t)+y for t=0.8 is f(0.8)=0.1073741824 +0.9=1.0073741824, which is greater than 0.34867844.\n\nSo perhaps for continuous functions, the inequality f(y) ≤f(t) +y holds for all y ≥t. Wait, but why?\n\nSince f is continuous and increasing, suppose for contradiction that there exists y >t such that f(y) >f(t) +y. Then, since f is continuous, there exists a point z ∈[t,y] where f(z) =f(t) +z. By the Intermediate Value Theorem, since f(y) >f(t) +y and f(t) ≤f(t) +t <f(t) +y (because t <y), then there exists z ∈(t,y) such that f(z)=f(t) +z.\n\nBut f is increasing, so for any x ∈[t,z], f(x) ≤f(z)=f(t) +z. For x=z, equality holds. For x >z, f(x) ≥f(z)=f(t) +z.\n\nBut for x=z+ε, where ε>0 small, f(x) ≥f(t) +z. But x=z+ε, so f(t) +x =f(t) +z+ε. If f(x) ≥f(t) +z, then f(x) ≤f(t) +x requires that f(t) +z ≤f(t) +x → z ≤x, which is true since x=z+ε.\n\nWait, this doesn't directly help. Alternatively, consider the function h(x) =f(x) -x. Since f is increasing, h(x) can be increasing or decreasing. I",
    "Original_Problem": "Assumption: \n$f$ is a continuous function from $[0,1]$ to $[0,1]$,\n$g$ is a continuous function from $[0,1]$ to $[0,1]$,\n$f$ is monotone increasing.\nProblem:\nProve that $$\\int_{0}^{1}f(g(x))dx\\le\\int_{0}^{1}f(x)dx+\\int_{0}^{1}g(x)dx.$$",
    "Original_Problem_with_numerical_assumption": "Assumption:\nAssumption 1: $f$ is a continuous function from $[0,1]$ to $[0,1]$,\nAssumption 2: $g$ is a continuous function from $[0,1]$ to $[0,1]$,\nAssumption 3: $f$ is monotone increasing.\nProblem:\nProve that $$\\int_{0}^{1}f(g(x))dx\\le\\int_{0}^{1}f(x)dx+\\int_{0}^{1}g(x)dx.$$",
    "Proof_problem": "Yes",
    "Redundant_assumption": "$$\\int_{0}^{1}[f(g(x))-g(x)]dx\\leq\\int_0^1 f(x)\\,dx.$$",
    "Problem_with_redundant_assumption": "Assumption:\nAssumption 1: $f$ is a continuous function from $[0,1]$ to $[0,1]$,\nAssumption 2: $g$ is a continuous function from $[0,1]$ to $[0,1]$,\nAssumption 3: $f$ is monotone increasing.\nAssumption 4: $$\\int_{0}^{1}[f(g(x))-g(x)]dx\\leq\\int_0^1 f(x)\\,dx.$$\nProblem:\nProve that $$\\int_{0}^{1}f(g(x))dx\\le\\int_{0}^{1}f(x)dx+\\int_{0}^{1}g(x)dx.$$"
}