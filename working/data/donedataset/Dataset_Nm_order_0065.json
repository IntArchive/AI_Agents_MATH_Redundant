{
    "Link_API": "https://api.stackexchange.com/2.3/questions/60284",
    "Title": "How to show that $\\det(AB) =\\det(A) \\det(B)$?",
    "Score": 94,
    "Category": "Chứng minh",
    "Tags": "linear-algebra, matrices, determinant",
    "Link": "https://math.stackexchange.com/questions/60284/how-to-show-that-detab-deta-detb",
    "Content": "Given two square matrices $A$ and $B$ , how do you show that $$\\det(AB) = \\det(A) \\det(B)$$ where $\\det(\\cdot)$ is the determinant of the matrix?",
    "AcceptedAnswer": "The proof using elementary matrices can be found e.g. on proofwiki . It's basically the same proof as given in Jyrki Lahtonen 's comment and Chandrasekhar's link. There is also a proof using block matrices, I googled a bit and I was only able to find it in this book and this paper . I like the approach which I learned from Sheldon Axler's Linear Algebra Done Right, Theorem 10.31 .\nLet me try to reproduce the proof here. We will use several results in the proof, one of them is - as far as I can say - a little less known. It is the theorem which says, that if I have two matrices $A$ and $B$ , which only differ in $k$ -th row and other rows are the same, and the matrix $C$ has as the $k$ -th row the sum of $k$ -th rows of $A$ and $B$ and other rows are the same as in $A$ and $B$ , then $|C|=|B|+|A|$ . Geometrically , this corresponds to adding two parallelepipeds with the same base. Proof. Let us denote the rows of $A$ by $\\vec\\alpha_1,\\ldots,\\vec\\alpha_n$ . Thus $$A=\n\\begin{pmatrix}\na_{11} & a_{12}& \\ldots & a_{1n}\\\\\na_{21} & a_{22}& \\ldots & a_{2n}\\\\\n\\vdots & \\vdots& \\ddots & \\vdots \\\\\na_{n1} & a_{n2}& \\ldots & a_{nn}\n\\end{pmatrix}=\n\\begin{pmatrix}\n\\vec\\alpha_1 \\\\ \\vec\\alpha_2 \\\\ \\vdots \\\\ \\vec\\alpha_n\n\\end{pmatrix}$$ Directly from the definition of matrix product we can see\nthat the rows of $A\\cdot B$ are of the form $\\vec\\alpha_kB$ , i.e., $$A\\cdot B=\\begin{pmatrix}\n\\vec\\alpha_1B \\\\ \\vec\\alpha_2B \\\\ \\vdots \\\\ \\vec\\alpha_nB\n\\end{pmatrix}$$ Since $\\vec\\alpha_k=\\sum_{i=1}^n a_{ki}\\vec e_i$ , we can\nrewrite this equality as $$A\\cdot B=\\begin{pmatrix}\n\\sum_{i_1=1}^n a_{1i_1}\\vec e_{i_1} B\\\\\n\\vdots\\\\\n\\sum_{i_n=1}^n a_{ni_n}\\vec e_{i_n} B\n\\end{pmatrix}$$ Using the theorem on the sum of determinants multiple times we get $$\n|{A\\cdot B}|= \\sum_{i_1=1}^n a_{1i_1}\n\\begin{vmatrix}\n\\vec e_{i_1}B\\\\\n\\sum_{i_2=1}^n a_{2i_2}\\vec e_{i_2} B\\\\\n\\vdots\\\\\n\\sum_{i_n=1}^n a_{ni_n}\\vec e_{i_n} B\n\\end{vmatrix}= \\ldots =\n\\sum_{i_1=1}^n \\ldots \\sum_{i_n=1}^n  a_{1i_1} a_{2i_2} \\dots\na_{ni_n}\n\\begin{vmatrix}\n\\vec e_{i_1} B \\\\ \\vec e_{i_2} B \\\\ \\vdots \\\\ \\vec e_{i_n} B\n\\end{vmatrix}\n$$ Now notice that if $i_j=i_k$ for some $j\\ne k$ , then\nthe corresponding determinant in the above sum is zero (it has two identical rows ).\nThus the only nonzero summands are those one, for which the $n$ -tuple $(i_1,i_2,\\dots,i_n)$ represents a permutation of the numbers $1,\\ldots,n$ .\nThus we get $$|{A\\cdot B}|=\\sum_{\\varphi\\in S_n} a_{1\\varphi(1)} a_{2\\varphi(2)} \\dots a_{n\\varphi(n)}\n\\begin{vmatrix}\n\\vec e_{\\varphi(1)} B \\\\ \\vec e_{\\varphi(2)} B \\\\ \\vdots \\\\ \\vec\ne_{\\varphi(n)} B\n\\end{vmatrix}$$ (Here $S_n$ denotes the set of all permutations of $\\{1,2,\\dots,n\\}$ .)\nThe matrix on the RHS of the above equality is the matrix $B$ with permuted rows. Using several transpositions of rows we can get the matrix $B$ . We will show that this can be done using $i(\\varphi)$ transpositions, where $i(\\varphi)$ denotes the number of inversions of $\\varphi$ .\nUsing this fact we get $$|{A\\cdot B}|=\\sum_{\\varphi\\in S_n} a_{1\\varphi(1)} a_{2\\varphi(2)} \\dots a_{n\\varphi(n)} (-1)^{i(\\varphi)} |{B}| =|A|\\cdot |B|.$$ It remains to show that we need $i(\\varphi)$ transpositions.\nWe can transform the \"permuted matrix\" to matrix $B$ as follows: we first move the first row of $B$ on the first place\nby exchanging it with the preceding row until it is on the correct position.\n(If it already is in the first position, we make no exchanges at all.)\nThe number of transpositions we have used is exactly the number\nof inversions of $\\varphi$ that contains the number 1.\nNow we can move the second row to the second place in the same way.\nWe will use the same number of transposition as the number of inversions of $\\varphi$ containing 2 but not containing 1. (Since the first row is already in place.)\nWe continue in the same way.\nWe see that by using this procedure we obtain the matrix $B$ after $i(\\varphi)$ row transpositions.",
    "llm_answer_create_structured_problem": "###BEGIN_OF_FORMAT###\nAssumption: \nA is a square matrix,\nB is a square matrix,\nA and B have the same dimension.\nProblem: Show that $$\\det(AB) = \\det(A) \\det(B)$$ where $\\det(\\cdot)$ is the determinant of the matrix.\n###",
    "reasoning_create_structured_problem": "Okay, let me try to tackle this problem. So the question is to show that the determinant of the product of two square matrices A and B is equal to the product of their determinants, right? That is, det(AB) = det(A) det(B). Hmm, I remember that determinants have some properties related to matrix multiplication, but I need to recall exactly how this works.\n\nFirst, let's make sure I understand the assumptions. The problem states that A and B are square matrices. So both are n×n matrices for some integer n. The determinant function det(·) is defined for square matrices, so that's good. The goal is to prove that when you multiply A and B, the determinant of the resulting matrix AB is equal to the product of their individual determinants.\n\nI think the first step is to recall some properties of determinants. For example, I know that the determinant of the identity matrix is 1, and that swapping two rows of a matrix changes the sign of the determinant. Also, multiplying a row by a scalar multiplies the determinant by that scalar. But how does this relate to matrix multiplication?\n\nWait, there's also the concept of elementary matrices. Elementary matrices represent row operations, and multiplying a matrix by an elementary matrix performs that row operation. The determinant of an elementary matrix is known based on the type of row operation it performs. Maybe if we can decompose matrices A and B into products of elementary matrices, we can use the multiplicative property for each elementary matrix and then combine the results.\n\nAlternatively, I remember that the determinant can be defined using the Leibniz formula, which involves sums over permutations and their signs. But that seems complicated for a proof. Maybe another approach is better.\n\nAnother thought: if the matrices are triangular, then the determinant is the product of the diagonal entries. If A and B are upper triangular, then AB is also upper triangular, and the diagonal entries of AB are the products of the diagonals of A and B. So det(AB) would be the product of the diagonals of AB, which is the product of the products of A's and B's diagonals, which is det(A)det(B). So maybe the property holds for triangular matrices. But how does that help for general matrices?\n\nOh, right, maybe using LU decomposition. If a matrix can be decomposed into LU (lower triangular times upper triangular), then the determinant would be the product of the determinants of L and U. But I think LU decomposition applies to invertible matrices. Not sure if that's the way to go here.\n\nWait, another approach: if A is invertible, then we can write it as a product of elementary matrices. Suppose A = E1 E2 ... Ek. Then det(A) = det(E1) det(E2) ... det(Ek). Similarly, AB = E1 E2 ... Ek B. Then det(AB) = det(E1) det(E2) ... det(Ek) det(B) = det(A) det(B). So if A is invertible, this works. But what if A is not invertible? Then det(A) is zero. If A is not invertible, then AB is also not invertible, so det(AB) is zero, and det(A) det(B) is 0 * det(B) = 0. So in that case, det(AB) = 0 = det(A) det(B). So maybe the formula holds whether A is invertible or not. Therefore, the property holds in general.\n\nBut wait, is this rigorous enough? Let me check. If A is invertible, decompose into elementary matrices, apply the property for each elementary matrix, and since determinant is multiplicative for elementary matrices, then it holds for their product. For non-invertible A, det(A)=0, and det(AB)=0 as AB is not invertible, so equality holds. That seems to cover all cases.\n\nAlternatively, some textbooks use the fact that the determinant is the unique function that is multilinear, alternating, and normalized (sending identity to 1). Then, using the properties of such functions, one can derive the multiplicative property. But that might be more abstract.\n\nAnother method is to use the exterior algebra, but that's probably beyond the scope here.\n\nAlternatively, using induction on the size of the matrices. For 1×1 matrices, it's trivial. Assume it's true for (n-1)×(n-1) matrices, then use expansion by minors for n×n matrices. But this could get messy.\n\nWait, let's try to outline the elementary matrix approach step by step. If A is invertible, then it can be expressed as a product of elementary matrices. Each elementary matrix corresponds to a row operation: swapping rows, multiplying a row by a scalar, or adding a multiple of one row to another. The determinant of each elementary matrix is known: 1 for row addition, -1 for row swap, and c for scaling a row by c. Then, when you multiply two elementary matrices, their determinants multiply. Therefore, if A is a product of elementary matrices, det(A) is the product of their determinants. Then, AB is the product of those elementary matrices times B. Applying each elementary matrix to B performs the row operation on B. The determinant of each elementary matrix times B would be det(elementary matrix) * det(B). Multiplying all these together gives det(A) det(B). \n\nIf A is not invertible, then det(A) is 0, and AB is also not invertible, so det(AB) is 0, hence 0 = 0 * det(B). Therefore, the formula holds in all cases.\n\nI think this approach covers both invertible and non-invertible matrices. So the key steps are:\n\n1. If A is invertible, decompose into elementary matrices and use multiplicative property for each step.\n2. If A is not invertible, both sides are zero, so equality holds.\n\nTherefore, the proof can be structured this way. Alternatively, there might be other approaches, but this seems manageable.\n\nAnother thing to consider: the base case when A or B is the identity matrix. Then det(AB) = det(B) = det(A) det(B) since det(A)=1. Similarly, if B is the identity. But that's a specific case.\n\nAlso, what if the matrices are 2x2? Maybe start with 2x2 and see if the formula holds, but the question is for general n x n matrices.\n\nAlternatively, use the fact that determinant is a group homomorphism from the general linear group to the multiplicative group of the field. But that's more abstract algebra.\n\nBut I think the elementary matrices approach is more elementary (pun intended) and accessible. So maybe present the proof in two cases: when A is invertible and when it's not.\n\nWait, but even for invertible matrices, how exactly does multiplying by elementary matrices affect the determinant? For example, if you have A = E1E2...Ek, then det(A) = det(E1)det(E2)...det(Ek). Then AB = E1E2...EkB, so det(AB) = det(E1)det(E2)...det(Ek)det(B) = det(A)det(B). That works because each elementary matrix multiplication affects the determinant multiplicatively.\n\nSo putting it all together, the proof would be:\n\nCase 1: A is invertible. Then A can be written as a product of elementary matrices. Since determinant is multiplicative over elementary matrices, det(AB) = det(A)det(B).\n\nCase 2: A is not invertible. Then det(A) = 0, and AB is also not invertible (since if AB were invertible, then A would be invertible as well, which is a contradiction). Therefore, det(AB) = 0 = 0 * det(B) = det(A)det(B).\n\nHence, in both cases, det(AB) = det(A)det(B).\n\nI think this is a solid proof. Alternatively, some sources might use the Leibniz formula for determinants, but that approach is more computational and might be longer.\n\nAlternatively, another approach is to use the fact that the determinant of a matrix is equal to the product of its eigenvalues, counting multiplicities. If A and B are square matrices, then the eigenvalues of AB are the same as the eigenvalues of BA (though this is true only for eigenvalues, not necessarily for singular values). However, this might not hold directly if A and B don't commute. Moreover, the determinant being the product of eigenvalues would imply det(AB) = product of eigenvalues of AB, but how does that relate to det(A)det(B)? If AB and BA have the same eigenvalues, but this might not help directly. Also, this approach might require working over an algebraically closed field, which isn't specified in the problem. So maybe this is not the best route.\n\nTherefore, the elementary matrices approach seems the most straightforward.\n\nNow, to structure the problem according to the user's format. The original question is: How to show that det(AB) = det(A) det(B) where A and B are square matrices.\n\nAssumptions needed: The problem states that A and B are square matrices. So each assumption should address one property. However, in the example given, the user listed assumptions as \"n is an integer, n is divisible by 7\". Here, the problem is to show det(AB)=det(A)det(B) for square matrices A and B. So the assumptions would be:\n\nAssumption:\nA and B are square matrices,\nA and B are of the same size (since otherwise AB isn't defined),\nMultiplication AB is defined (which is covered if they are same size),\nThe determinant is defined for square matrices.\n\nBut perhaps the user's format requires listing each assumption line by line. So:\n\nAssumption:\nA is a square matrix,\nB is a square matrix,\nA and B have the same dimensions,\nMultiplication AB is defined.\n\nBut actually, if A is n×n and B is n×n, then AB is defined and is n×n. So the key assumptions are that A and B are square matrices of the same size. So each assumption line would be:\n\nAssumption:\nA is a square matrix,\nB is a square matrix,\nA and B have the same size.\n\nAlternatively, since the problem states \"given two square matrices A and B\", maybe the only assumption needed is that A and B are square matrices of the same size. But according to the example, the user lists each assumption line by line, each addressing one property. So in the example, the assumptions were \"$n$ is an integer\" and \"$n$ is divisible by 7\". Here, the original problem mentions \"given two square matrices A and B\". So the assumptions should be:\n\nAssumption:\nA is a square matrix,\nB is a square matrix,\nA and B are of the same size.\n\nBut in the problem statement, the user just writes \"Given two square matrices A and B\", so the key assumptions are that A and B are square and same size. Therefore, in the formatted problem, the assumptions would be:\n\nAssumption:\nA is a square matrix,\nB is a square matrix,\nA and B can be multiplied (i.e., their dimensions are compatible).\n\nWait, but if A and B are square matrices, their multiplication is defined only if they are the same size. So if they are both n×n, then AB is defined. So perhaps:\n\nAssumption:\nA is a square matrix,\nB is a square matrix,\nA and B have the same dimension.\n\nBut the original problem statement doesn't mention their dimensions, just that they are square. So to make the problem well-defined, we need that A and B are square and of the same size. Therefore, in the formatted problem, these would be the assumptions.\n\nBut in the example given by the user, the assumption was that n is an integer and n is divisible by 7. The original problem had \"for every integer n\", but the formatted assumption listed specific conditions. However, in the current problem, the question is general: for any two square matrices A and B, show that det(AB) = det(A)det(B). Therefore, the only assumptions are that A and B are square matrices of the same size. However, the user's example splits into two assumptions even though the original problem states \"for every integer n\", but the formatted problem added \"n is divisible by 7\". Wait, in the example, the original problem was to prove for every integer n, but the formatted problem added an assumption that n is divisible by 7. That seems like a mistake. Wait, looking back:\n\nOriginal example problem: Prove that for every integer n, the number A(n) is divisible by 91.\n\nFormatted problem assumption: n is an integer, n is divisible by 7.\n\nWait, that seems incorrect. The original problem says \"for every integer n\", but the formatted problem adds an assumption that n is divisible by 7. That contradicts. Maybe the example is incorrect or there was a miscommunication. The user probably intended to present an example where the original problem had some conditions, and the formatted problem lists them. But in this case, the example seems to have an error. However, the user's instruction says: \"filtered the unrelated informations, and structure that problem as the following format\". So perhaps in the example, the original problem had a mistake or additional condition, and the formatted problem corrected it. But in the current problem, the user asks to structure the problem as per the format.\n\nGiven that, the original problem is: \"Given two square matrices A and B, how do you show that det(AB) = det(A) det(B)\".\n\nTherefore, the assumptions are:\n\n- A is a square matrix.\n\n- B is a square matrix.\n\n- A and B can be multiplied (i.e., they are of the same size).\n\nBut since A and B are square, being able to multiply them requires they are same size. So the key assumptions are:\n\n- A and B are square matrices of the same size.\n\nBut according to the example, each assumption is one line per property applied to one object. So we can split into:\n\nAssumption:\nA is a square matrix,\nB is a square matrix,\nA and B have the same dimension.\n\nThus, each line addresses one property for one object. Therefore, the formatted problem would have these three assumptions.\n\nSo putting it all together:\n\n###BEGIN_OF_FORMAT###\nAssumption: \nA is a square matrix,\nB is a square matrix,\nA and B have the same dimension.\nProblem: Show that $$\\det(AB) = \\det(A) \\det(B)$$ where $\\det(\\cdot)$ is the determinant of the matrix.\n###\n\nYes, that seems correct. The original problem states \"given two square matrices A and B\", which implies they are square and of the same dimension (since otherwise their product isn't defined). So the assumptions are correctly listed.",
    "Original_Problem": "Assumption: \nA is a square matrix,\nB is a square matrix,\nA and B have the same dimension.\nProblem: Show that $$\\det(AB) = \\det(A) \\det(B)$$ where $\\det(\\cdot)$ is the determinant of the matrix.",
    "Original_Problem_with_numerical_assumption": "Assumption:\nAssumption 1: A is a square matrix,\nAssumption 2: B is a square matrix,\nAssumption 3: A and B have the same dimension.\nProblem: Show that $$\\det(AB) = \\det(A) \\det(B)$$ where $\\det(\\cdot)$ is the determinant of the matrix.",
    "Proof_problem": "Yes",
    "Redundant_assumption": "$$A\\cdot B=\\begin{pmatrix}\n\\sum_{i_1=1}^n a_{1i_1}\\vec e_{i_1} B\\\\\n\\vdots\\\\\n\\sum_{i_n=1}^n a_{ni_n}\\vec e_{i_n} B\n\\end{pmatrix}$$",
    "Problem_with_redundant_assumption": "Assumption:\nAssumption 1: A is a square matrix,\nAssumption 2: B is a square matrix,\nAssumption 3: A and B have the same dimension.\nAssumption 4: $$A\\cdot B=\\begin{pmatrix}\n\\sum_{i_1=1}^n a_{1i_1}\\vec e_{i_1} B\\\\\n\\vdots\\\\\n\\sum_{i_n=1}^n a_{ni_n}\\vec e_{i_n} B\n\\end{pmatrix}$$\nProblem: Show that $$\\det(AB) = \\det(A) \\det(B)$$ where $\\det(\\cdot)$ is the determinant of the matrix."
}