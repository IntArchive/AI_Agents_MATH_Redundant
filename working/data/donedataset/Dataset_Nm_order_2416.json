{
    "Link_API": "https://api.stackexchange.com/2.3/questions/503710",
    "Title": "Distance between mean and median",
    "Score": 14,
    "Category": "Chứng minh",
    "Tags": "probability-theory, inequality, random-variables, median",
    "Link": "https://math.stackexchange.com/questions/503710/distance-between-mean-and-median",
    "Content": "I want to solve the following problem in T.Tao's random matrix theory book. Let $X$ be a random variable with finite second momment. A median $M(X)$ of $X$ saisfies $\\mathbb{P}(X>M(X)),\\mathbb{P}(X<M(X))\\leq 1/2$. Now I wan to prove that for any median:\n$$M(X)=\\mathbb{E}(X)+O((\\mathbb{Var}(X))^{1/2})$$ I think we have to use Chebyshev's inequality for this, which I have done to obtain:\n$$\\mathbb{P}(X>\\lambda \\sigma)\\leq \\mathbb{P}(|X|>\\lambda \\sigma)\\leq \\frac{1}{\\lambda ^2}$$\nwhere $\\sigma=(\\mathbb{Var}(X))^{1/2}$. But I fail to interprete this...",
    "AcceptedAnswer": "You have to use two things: $\\mathbb E(|X-c|)$ is minimized when $c=M(X)$ Proof . Jensen inequality Then you can have:\n$$\n\\left|M(X)-\\mathbb{E}(X)\\right|=\\left|\\mathbb{E}(M(X)-X)\\right| \\\\\n\\leq \\mathbb{E}(\\left|X-M(X)\\right|)\\leq \\mathbb{E}(\\left|X-\\mathbb E(X)\\right|) \\\\\n\\leq \\sqrt{\\mathbb{E}(\\left|X-\\mathbb E(X)\\right|^2)}=(\\mathbb{Var}(X))^{1/2}.\n$$",
    "llm_answer_create_structured_problem": "###BEGIN_OF_FORMAT###\nassumption: \n$X$ is a random variable,\n$X$ has finite second moment,\nA median $M(X)$ satisfies $\\mathbb{P}(X > M(X)) \\leq 1/2$ and $\\mathbb{P}(X < M(X)) \\leq 1/2$.\nproblem:\nProve that for any median $M(X)$,\n$$M(X) = \\mathbb{E}(X) + O\\left( (\\mathbb{Var}(X))^{1/2} \\right).$$\n###END_OF_FORMAT###",
    "reasoning_create_structured_problem": "Okay, let me try to work through this problem step by step. So the goal is to prove that for any median M(X) of a random variable X with finite second moment, the distance between the median and the mean is on the order of the standard deviation. In other words, M(X) = E(X) + O(σ), where σ is the square root of the variance, Var(X)^{1/2}.\n\nFirst, I need to recall some definitions. A median M(X) is a value such that both P(X ≥ M(X)) ≤ 1/2 and P(X ≤ M(X)) ≤ 1/2. The mean is E(X), and the variance is E[(X - E(X))²]. The problem mentions using Chebyshev's inequality, which I know states that for any λ > 0, P(|X - E(X)| ≥ λσ) ≤ 1/λ². The user already applied Chebyshev's and got P(X > λσ) ≤ 1/λ², but they are stuck on interpreting this.\n\nWait, maybe they tried to use Chebyshev directly on X instead of X - E(X). Let me check. Chebyshev's inequality is usually about the deviation from the mean. So perhaps they should center X around its mean first. Let's see.\n\nThe user's result is P(X > λσ) ≤ 1/λ². But if X is not centered, this might not directly relate to the median. Maybe they should consider X - E(X). Let's think about that.\n\nThe key is to relate the median to the mean. The median is a measure of central tendency, just like the mean. For distributions that are symmetric, the median and mean coincide. But in general, they can differ, and the problem states that the difference is bounded by a multiple of the standard deviation.\n\nSo how to connect the median and the mean using Chebyshev? Chebyshev gives a bound on how far X can be from the mean in terms of probability. The median, on the other hand, is about the middle value of the distribution.\n\nMaybe we can argue that if the median were too far from the mean, then the probability mass on one side would contradict Chebyshev's inequality. Let's formalize that.\n\nSuppose that M(X) is a median. Let’s assume that M(X) > E(X) + Cσ for some constant C. Then, the probability that X is greater than E(X) + Cσ is at most 1/2 (since M(X) is a median). But by Chebyshev, the probability that X is more than Cσ away from the mean is ≤ 1/C². So if we set C such that 1/C² < 1/2, then there's a contradiction because the median's position would require P(X > E(X) + Cσ) ≤ 1/2, but Chebyshev says that P(X > E(X) + Cσ) ≤ P(|X - E(X)| > Cσ) ≤ 1/C². If 1/C² < 1/2, that would mean that the probability of X being beyond E(X) + Cσ is less than 1/2, which would imply that the median can't be as high as E(X) + Cσ. Wait, maybe this is the right direction.\n\nAlternatively, let's consider the difference between the median and the mean. Let’s denote μ = E(X), and M = M(X). We need to show that |M - μ| ≤ Kσ for some constant K. To do this, suppose |M - μ| > Kσ. Then, either M > μ + Kσ or M < μ - Kσ. Let's consider the case M > μ + Kσ. Since M is a median, P(X ≥ M) ≤ 1/2. However, by Chebyshev's inequality, P(X ≥ μ + Kσ) ≤ P(|X - μ| ≥ Kσ) ≤ 1/K². If we choose K such that 1/K² < 1/2, then P(X ≥ μ + Kσ) < 1/2. But since M is a median, P(X ≥ M) ≤ 1/2. However, if M > μ + Kσ, then P(X ≥ M) ≤ P(X ≥ μ + Kσ) < 1/2. But the definition of median requires that P(X ≥ M) ≤ 1/2 and P(X ≤ M) ≤ 1/2. Wait, but if P(X ≥ M) < 1/2, then does that contradict something? Not directly. Wait, perhaps the median is the smallest value such that P(X ≥ M) ≤ 1/2. But if M is larger than μ + Kσ, then the actual probability P(X ≥ M) is less than 1/2, but the median can still be at that point. Hmm, maybe this approach isn't directly giving the contradiction.\n\nAlternatively, maybe we need to use the fact that the median minimizes the mean absolute deviation. But the problem mentions using Chebyshev, so perhaps there's another way.\n\nAnother approach: Let’s relate the median to the mean through integrals. For any a, |μ - a| ≤ E|X - a|. By the Cauchy-Schwarz inequality, E|X - a| ≤ sqrt(E[(X - a)^2]) = sqrt(Var(X) + (μ - a)^2). But I'm not sure if this is helpful here.\n\nWait, maybe consider the case where a is the median. Then, E|X - M| is minimized when M is the median. So, perhaps we can bound |μ - M| using properties of the median and mean.\n\nAlternatively, using the one-sided Chebyshev inequality. The standard Chebyshev is two-sided, but maybe we can get a one-sided version. For example, the one-sided Chebyshev inequality states that P(X - μ ≥ t) ≤ σ² / (σ² + t²) for t > 0. But I'm not sure if that's applicable here.\n\nWait, the user tried using Chebyshev and got P(X > λσ) ≤ 1/λ². If we let t = λσ, then P(X > μ + t) ≤ σ² / t². But maybe this is a different inequality. Wait, actually, Chebyshev's inequality in its standard form is P(|X - μ| ≥ kσ) ≤ 1/k². So if we consider P(X - μ ≥ kσ) ≤ P(|X - μ| ≥ kσ) ≤ 1/k². So the upper tail probability is ≤ 1/k². Similarly for the lower tail.\n\nNow, suppose that the median M is greater than μ + kσ. Then, since M is a median, P(X ≥ M) ≤ 1/2. But according to Chebyshev, P(X ≥ μ + kσ) ≤ 1/k². If we set 1/k² < 1/2, i.e., k > sqrt(2), then P(X ≥ μ + kσ) < 1/2. Therefore, if M were greater than μ + kσ, then P(X ≥ M) ≤ P(X ≥ μ + kσ) < 1/2. But the definition of median requires that P(X ≥ M) ≤ 1/2. Wait, so this doesn't contradict the definition. It just says that the tail probability is less than 1/2. So in that case, M could be larger than μ + kσ, but the probability beyond M is less than 1/2. But the median is the smallest value such that P(X ≥ M) ≤ 1/2. Hmm, maybe M can't be too much larger than μ because the tail probability beyond μ + kσ is small.\n\nWait, maybe the median can't be more than a certain distance from the mean because otherwise, the tail probabilities would force the median to be closer. Let me think again.\n\nSuppose that M > μ + kσ. Then, the probability that X is ≥ M is ≤ 1/k² (from Chebyshev). But if we take k such that 1/k² < 1/2, then this would imply that P(X ≥ M) < 1/2. However, the median is supposed to satisfy P(X ≥ M) ≤ 1/2. So if M is larger than μ + kσ, then P(X ≥ M) < 1/2, which is allowed. But then, how does that help us bound M?\n\nAlternatively, if we consider that the median is the point where P(X ≤ M) ≥ 1/2 and P(X ≥ M) ≥ 1/2. Wait, no, the definition given in the problem is that a median satisfies P(X > M) ≤ 1/2 and P(X < M) ≤ 1/2. So it's possible that P(X ≥ M) ≥ 1/2 and P(X ≤ M) ≥ 1/2. But the key is that the median splits the distribution such that neither tail has more than half the probability.\n\nSo, suppose M is a median. If we can show that if M is too far from μ, then the tail probability beyond M would be less than 1/2, contradicting the definition of the median. Wait, but actually, the median only requires that the probability beyond it is ≤ 1/2. So if the tail probability is less than 1/2, that's still okay. So perhaps the median can be in a region where the tail is less than 1/2.\n\nBut then, how do we get a lower bound on how far the median can be from the mean?\n\nWait, maybe we need to use both the upper and lower tails. Suppose M is greater than μ + kσ. Then, the probability that X is ≤ M is ≥ 1 - 1/k² (from Chebyshev: P(X ≤ μ + kσ) ≥ 1 - 1/k²). If k is such that 1 - 1/k² ≥ 1/2, then this would mean that P(X ≤ M) ≥ 1 - 1/k² ≥ 1/2, which is okay. But if k is too large, then 1 - 1/k² would be less than 1/2, which would contradict that P(X ≤ M) ≥ 1/2. Wait, no. If M is greater than μ + kσ, then P(X ≤ M) ≥ P(X ≤ μ + kσ) ≥ 1 - 1/k². So if we set 1 - 1/k² ≥ 1/2, then 1/k² ≤ 1/2 ⇒ k ≥ sqrt(2). Therefore, for k ≥ sqrt(2), P(X ≤ M) ≥ 1 - 1/k² ≥ 1/2. But this is okay because the median requires P(X ≤ M) ≥ 1/2. So this doesn't give a contradiction. Hmm.\n\nAlternatively, maybe we can use the fact that the median is within a certain distance of the mean by considering the cumulative distribution function. Let’s think about the distance between M and μ. Suppose that M > μ + Cσ. Then, by Chebyshev, the probability that X is ≤ μ + Cσ is ≥ 1 - 1/C². But since M is a median, P(X ≤ M) ≥ 1/2. Therefore, we have P(X ≤ μ + Cσ) ≥ 1 - 1/C², and since μ + Cσ < M, P(X ≤ M) ≥ P(X ≤ μ + Cσ) ≥ 1 - 1/C². For this to be ≥ 1/2, we need 1 - 1/C² ≥ 1/2 ⇒ C² ≥ 2 ⇒ C ≥ sqrt(2). Therefore, if M > μ + sqrt(2)σ, then P(X ≤ M) ≥ 1 - 1/2 = 1/2, which is compatible with the definition of the median. So this doesn't give a bound on M.\n\nWait, maybe we need a different approach. Let's recall that for any random variable with finite variance, the median is within one standard deviation of the mean. But I need to find a formal proof.\n\nAlternatively, consider the function f(t) = E|X - t|. The median minimizes this function. On the other hand, the mean minimizes E[(X - t)^2]. Maybe using the relationship between these two minimizers.\n\nBut the problem suggests using Chebyshev's inequality. Let me try again.\n\nLet’s assume that |M - μ| > kσ. Without loss of generality, suppose M > μ + kσ. Then, since M is a median, P(X ≥ M) ≤ 1/2. By Chebyshev, P(X ≥ μ + kσ) ≤ P(|X - μ| ≥ kσ) ≤ 1/k². Therefore, P(X ≥ M) ≤ P(X ≥ μ + kσ) ≤ 1/k². But since M is a median, P(X ≥ M) ≤ 1/2. So we have 1/k² ≥ 1/2, which implies that k² ≤ 2 ⇒ k ≤ sqrt(2). Wait, but this is the opposite of what we need. Wait, if we assume that M > μ + kσ, then we have P(X ≥ M) ≤ 1/k². But since M is a median, we must have P(X ≥ M) ≤ 1/2. Therefore, 1/k² must be ≥ 1/2, which implies k² ≤ 2 ⇒ k ≤ sqrt(2). Therefore, this tells us that if k > sqrt(2), then our initial assumption that M > μ + kσ leads to P(X ≥ M) ≤ 1/k² < 1/2, which is allowed. Wait, I'm getting confused here.\n\nLet me try to structure this properly.\n\nAssume that M > μ + kσ. Then, by Chebyshev:\n\nP(X ≥ μ + kσ) ≤ 1/k².\n\nSince M > μ + kσ, then P(X ≥ M) ≤ P(X ≥ μ + kσ) ≤ 1/k².\n\nBut since M is a median, we have P(X ≥ M) ≤ 1/2.\n\nTherefore, 1/k² must be ≥ 1/2 ⇒ k² ≤ 2 ⇒ k ≤ sqrt(2).\n\nWait, but this is saying that if we assume M > μ + kσ, then k must be ≤ sqrt(2). Therefore, the maximum possible k for which this can hold is sqrt(2). Thus, M cannot be greater than μ + sqrt(2)σ. Similarly, M cannot be less than μ - sqrt(2)σ. Hence, |M - μ| ≤ sqrt(2)σ.\n\nBut wait, this seems like a possible conclusion. Let me check.\n\nIf we suppose M > μ + sqrt(2)σ, then k = sqrt(2) + ε for some ε > 0. Then, P(X ≥ M) ≤ 1/(sqrt(2) + ε)^2 < 1/2. But the median requires P(X ≥ M) ≤ 1/2. There's no contradiction here because P(X ≥ M) can be less than 1/2. So this line of reasoning doesn't give an upper bound on M.\n\nWait, I'm missing something. If M is a median, then it's the smallest value such that P(X ≥ M) ≤ 1/2. So if P(X ≥ μ + kσ) ≤ 1/k², then for k > sqrt(2), 1/k² < 1/2, which means that the median could be at μ + kσ because that point already satisfies P(X ≥ μ + kσ) ≤ 1/k² < 1/2. But the median is the smallest such point. Therefore, the actual median M must be ≤ μ + kσ for k where 1/k² = 1/2, which is k = sqrt(2). Because if we go beyond that, the probability is less than 1/2, but the median is the leftmost point where the tail is ≤ 1/2. Therefore, the median cannot be greater than μ + sqrt(2)σ, because beyond that point, the tail probability is already less than 1/2, so the median must be to the left of that.\n\nYes, that makes sense. So if we consider that the median is the infimum over all m such that P(X ≥ m) ≤ 1/2, then if μ + sqrt(2)σ is such a point (since P(X ≥ μ + sqrt(2)σ) ≤ 1/2), then the median must be ≤ μ + sqrt(2)σ. Similarly, for the lower tail, the median must be ≥ μ - sqrt(2)σ. Therefore, |M - μ| ≤ sqrt(2)σ.\n\nTherefore, the constant in the big O notation would be sqrt(2), so M = μ + O(σ).\n\nBut let's verify this with an example. Take a normal distribution N(μ, σ²). The median is equal to the mean, so the difference is zero. For a distribution skewed such that the median is different from the mean, say an exponential distribution with rate λ. The mean is 1/λ, the median is ln(2)/λ. The difference is (1 - ln(2))/λ ≈ 0.3069/λ. The standard deviation is 1/λ. So the difference is about 0.3069σ, which is less than sqrt(2)σ ≈ 1.414σ. So this bound is not tight, but it's a valid upper bound.\n\nAnother example: consider a Bernoulli random variable with p = 1/2. The mean is 1/2, the median is also 1/2. If p ≠ 1/2, say p > 1/2, then the median is 1, and the mean is p. The difference is 1 - p. The variance is p(1 - p). So |M - μ| = 1 - p. The standard deviation is sqrt(p(1 - p)). Let's compute the ratio (1 - p)/sqrt(p(1 - p))) = sqrt((1 - p)/p). For p approaching 1, this ratio can become large. Wait, but according to our earlier result, the difference should be bounded by sqrt(2)σ. Let's check for p approaching 1.\n\nLet’s take p = 0.9. Then μ = 0.9, Var(X) = 0.9*0.1 = 0.09, σ = 0.3. The median is 1. So |M - μ| = 0.1. sqrt(2)σ ≈ 0.424. So 0.1 < 0.424. Okay, so in this case, it's satisfied. What if p = 0.99? Then μ = 0.99, Var = 0.99*0.01 ≈ 0.0099, σ ≈ 0.0995. The median is 1. |M - μ| = 0.01. sqrt(2)σ ≈ 0.1407. Still, 0.01 < 0.1407. So even though p approaches 1, the difference between median and mean shrinks, but the variance also shrinks. The ratio |M - μ|/σ = (1 - p)/sqrt(p(1 - p)) = sqrt((1 - p)/p). As p approaches 1, this ratio goes to 0. So in this case, the difference is much smaller than sqrt(2)σ.\n\nAnother case: take p = 0.6. Then μ = 0.6, Var = 0.24, σ ≈ 0.49. The median is 1. |M - μ| = 0.4. sqrt(2)σ ≈ 0.69. So 0.4 < 0.69. Still holds.\n\nWait, but what if the distribution is such that the median is maximally far from the mean? For example, consider a distribution where X takes value -a with probability 1/2 + ε and value b with probability 1/2 - ε. The mean μ = (-a)(1/2 + ε) + b(1/2 - ε). The variance would be E[X²] - μ². The median is -a because P(X ≤ -a) = 1/2 + ε > 1/2. So the median M = -a. The difference |M - μ| = | -a - [ -a(1/2 + ε) + b(1/2 - ε) ] |. Let's choose a and b such that the mean is zero for simplicity. Let’s set μ = 0:\n\n(-a)(1/2 + ε) + b(1/2 - ε) = 0 ⇒ b = [a(1/2 + ε)] / (1/2 - ε)\n\nTo maximize |M - μ| = | -a - 0 | = a. So we need to maximize a while keeping the variance fixed, or see what the variance is.\n\nThe variance Var(X) = E[X²] - μ² = E[X²] since μ = 0. E[X²] = a²(1/2 + ε) + b²(1/2 - ε). Substitute b from above:\n\nE[X²] = a²(1/2 + ε) + [a²(1/2 + ε)^2 / (1/2 - ε)^2] (1/2 - ε)\n\nSimplify:\n\n= a²(1/2 + ε) + a²(1/2 + ε)^2 / (1/2 - ε)\n\nFactor out a²:\n\n= a² [ (1/2 + ε) + (1/2 + ε)^2 / (1/2 - ε) ]\n\nLet’s compute this expression inside the brackets:\n\nLet’s denote p = 1/2 + ε, so 1/2 - ε = 1 - p.\n\nThen, the expression becomes:\n\np + p² / (1 - p) = [p(1 - p) + p²] / (1 - p) = [p - p² + p²] / (1 - p) = p / (1 - p)\n\nSo Var(X) = a² * p / (1 - p) = a² * (1/2 + ε) / (1/2 - ε)\n\nTherefore, the standard deviation σ = a * sqrt( (1/2 + ε) / (1/2 - ε) )\n\nWe want to express |M - μ| = a in terms of σ. From above,\n\na = σ * sqrt( (1/2 - ε) / (1/2 + ε) )\n\nTo find the maximum possible a for a given σ, we need to see how this expression behaves as ε varies. The maximum a occurs when ε approaches 1/2, but ε must be less than 1/2 to keep probabilities positive. As ε approaches 1/2 from below, (1/2 - ε) approaches 0, so sqrt( (1/2 - ε)/(1/2 + ε) ) approaches 0. Therefore, a approaches 0. So in this case, the difference |M - μ| = a approaches 0 as ε approaches 1/2.\n\nAlternatively, if ε approaches 0, then a = σ * sqrt( (1/2)/1/2 ) = σ. So |M - μ| = σ. Therefore, in this case, the difference between median and mean is equal to the standard deviation. But according to our earlier bound, it should be ≤ sqrt(2)σ. So here, it's within the bound.\n\nThis example shows that the difference can be up to σ. But our earlier analysis suggested the bound is sqrt(2)σ. Hmm. So perhaps the actual bound is 1σ, but in our analysis using Chebyshev, we got sqrt(2)σ. Maybe there is a tighter way to do this.\n\nWait, let's go back. Earlier, we assumed that M > μ + kσ, leading to P(X ≥ M) ≤ 1/k². Since M is a median, P(X ≥ M) ≤ 1/2. Therefore, 1/k² ≥ 1/2 ⇒ k ≤ sqrt(2). Therefore, M cannot be more than sqrt(2)σ away from the mean. But in the example above, we have a case where |M - μ| = σ. So our bound is not tight, but it's still correct because sigma ≤ sqrt(2)sigma.\n\nTherefore, the correct statement is that |M - μ| ≤ sqrt(2)σ. Therefore, M = μ + O(σ).\n\nBut the problem says \"for any median: M(X) = E(X) + O((Var(X))^{1/2})\". So the big O notation just requires that the difference is bounded by a constant times σ, which is satisfied with the constant being sqrt(2).\n\nAlternatively, maybe there's a better constant. Let's see. In the example above with the Bernoulli variable, the difference was sqrt((1 - p)/p) * σ. For p approaching 0, sqrt((1 - p)/p) can become large. Wait, but if p approaches 0, then Var(X) = p(1 - p) ≈ p. So σ ≈ sqrt(p). The difference |M - μ|: if p < 1/2, the median is 0, and μ = p. So |M - μ| = p. Then, p / σ = p / sqrt(p) = sqrt(p) → 0 as p → 0. So the ratio |M - μ| / σ goes to zero. Therefore, in this case, the difference is much smaller than σ.\n\nBut in the previous example with the symmetric distribution around -a and b with μ = 0, when ε = 0, we get |M - μ| = a = σ. So in that case, the difference is exactly σ. Therefore, the bound of sqrt(2)σ is not tight, but it's still a valid upper bound.\n\nSo, putting it all together, using Chebyshev's inequality, we can show that the median cannot be more than sqrt(2) standard deviations away from the mean. Therefore, M(X) = E(X) + O(σ).\n\nTo write this up formally:\n\nLet μ = E(X), σ = sqrt(Var(X)), and M be a median of X.\n\nAssume that M > μ + kσ for some k > 0. Then, by Chebyshev's inequality:\n\nP(X ≥ μ + kσ) ≤ P(|X - μ| ≥ kσ) ≤ 1/k².\n\nSince M is a median, P(X ≥ M) ≤ 1/2. But if M > μ + kσ, then P(X ≥ M) ≤ P(X ≥ μ + kσ) ≤ 1/k². To ensure that this does not contradict the definition of the median, we must have 1/k² ≥ 1/2, which implies k ≤ sqrt(2). Therefore, M ≤ μ + sqrt(2)σ.\n\nSimilarly, by considering the lower tail, M ≥ μ - sqrt(2)σ.\n\nThus, |M - μ| ≤ sqrt(2)σ, which means M = μ + O(σ).\n\nTherefore, the problem is structured with the assumptions that X has finite second moment (so variance exists), and we need to prove the bound on the median in terms of the standard deviation.",
    "Original_Problem": "assumption: \n$X$ is a random variable,\n$X$ has finite second moment,\nA median $M(X)$ satisfies $\\mathbb{P}(X > M(X)) \\leq 1/2$ and $\\mathbb{P}(X < M(X)) \\leq 1/2$.\nproblem:\nProve that for any median $M(X)$,\n$$M(X) = \\mathbb{E}(X) + O\\left( (\\mathbb{Var}(X))^{1/2} \\right).$$",
    "Original_Problem_with_numerical_assumption": "Assumption:\nAssumption 1: $X$ is a random variable,\nAssumption 2: $X$ has finite second moment,\nAssumption 3: A median $M(X)$ satisfies $\\mathbb{P}(X > M(X)) \\leq 1/2$ and $\\mathbb{P}(X < M(X)) \\leq 1/2$.\nProblem:\nProve that for any median $M(X)$,\n$$M(X) = \\mathbb{E}(X) + O\\left( (\\mathbb{Var}(X))^{1/2} \\right).$$",
    "Proof_problem": "Yes",
    "Redundant_assumption": "$$\n\\left|M(X)-\\mathbb{E}(X)\\right|=\\left|\\mathbb{E}(M(X)-X)\\right| \\\\\n\\leq \\mathbb{E}(\\left|X-M(X)\\right|)\\leq \\mathbb{E}(\\left|X-\\mathbb E(X)\\right|) \\\\\n\\leq \\sqrt{\\mathbb{E}(\\left|X-\\mathbb E(X)\\right|^2)}=(\\mathbb{Var}(X))^{1/2}.\n$$",
    "Problem_with_redundant_assumption": "Assumption:\nAssumption 1: $X$ is a random variable,\nAssumption 2: $X$ has finite second moment,\nAssumption 3: A median $M(X)$ satisfies $\\mathbb{P}(X > M(X)) \\leq 1/2$ and $\\mathbb{P}(X < M(X)) \\leq 1/2$.\nAssumption 4: $$\n\\left|M(X)-\\mathbb{E}(X)\\right|=\\left|\\mathbb{E}(M(X)-X)\\right| \\\\\n\\leq \\mathbb{E}(\\left|X-M(X)\\right|)\\leq \\mathbb{E}(\\left|X-\\mathbb E(X)\\right|) \\\\\n\\leq \\sqrt{\\mathbb{E}(\\left|X-\\mathbb E(X)\\right|^2)}=(\\mathbb{Var}(X))^{1/2}.\n$$\nProblem:\nProve that for any median $M(X)$,\n$$M(X) = \\mathbb{E}(X) + O\\left( (\\mathbb{Var}(X))^{1/2} \\right).$$"
}