{
    "Link_API": "https://api.stackexchange.com/2.3/questions/1804805",
    "Title": "How is the entropy of the normal distribution derived?",
    "Score": 15,
    "Category": "Chứng minh",
    "Tags": "normal-distribution, entropy",
    "Link": "https://math.stackexchange.com/questions/1804805/how-is-the-entropy-of-the-normal-distribution-derived",
    "Content": "Wikipedia says the entropy of the normal distribution is $\\frac{1}2 \\ln(2\\pi e\\sigma^2)$ I could not find any proof for that, though. I found some proofs that show that the maximum entropy resembles to $\\frac{1}2+\\ln(\\sqrt{2\\pi}\\sigma)$ and while I see that this can be rewritten as $\\frac{1}2\\ln(e\\sigma\\sqrt{2\\pi})$, I do not get how the square root can be get rid of and how the extra $\\sigma$ can be put into $\\ln$. It is clear that an additional summand $\\frac{1}2\\ln(\\sigma\\sqrt{2\\pi})$ would help, but where do we get it from? Probably just thinking in the wrong way here... So, what is the proof for the maximum likelihood entropy of the normal distribution?",
    "AcceptedAnswer": "Notice that $\\ln(\\color{blue}{\\sqrt{\\color{black}{x}}}) = \\ln(x^{\\color{blue}{\\frac{1}{2}}}) = \\color{blue}{\\frac{1}{2}}\\ln(x)$ and that $\\ln(y) \\color{red}{+} \\ln(z) = \\ln(y \\color{red}{\\cdot} z)$ for all $x,y,z > 0$ . Using these identities, let us re-write the maximum entropy, $\\frac{1}{2} + \\ln(\\sqrt{2\\pi}\\sigma)$ , as follows: $$\n\\begin{align}\n\\frac{1}{2} + \\ln(\\sqrt{2\\pi}\\sigma)\n&= \\frac{1}{2} + \\ln(\\color{blue}{\\sqrt{\\color{black}{2\\pi\\sigma^2}}}) \\\\\n&= \\frac{1}{2} + \\color{blue}{\\frac{1}{2}}\\ln(2\\pi\\sigma^2) \\\\\n&= \\frac{1}{2}(1 + \\ln(2\\pi\\sigma^2)) \\\\\n&= \\frac{1}{2}(\\ln(\\mathrm{e}) \\color{red}{+} \\ln(2\\pi\\sigma^2)) \n= \\frac{1}{2}\\ln(\\mathrm{e}\\color{red}{\\cdot}2\\pi\\sigma^2)\n\\end{align}\n$$ So, the entropy reported in Wikipedia is correct.",
    "llm_answer_create_structured_problem": "###BEGIN_OF_FORMAT###\nAssumption: \n$X$ is a normally distributed random variable with mean $\\mu$ and variance $\\sigma^2$,\nThe differential entropy $H$ of a continuous random variable is defined as $H = -\\int_{-\\infty}^{\\infty} f(x) \\ln(f(x)) \\, dx$,\nThe probability density function of $X$ is $f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$.\nProblem:\nProve that the entropy of the normal distribution is $\\frac{1}{2} \\ln(2\\pi e \\sigma^2)$.\n###END_OF_FORMAT###",
    "reasoning_create_structured_problem": "Okay, so the user wants me to act as a bilingual math professor and structure the given Math Stack Exchange question into the specified format. Let me first understand the original problem.\n\nThe question is about deriving the entropy of the normal distribution. The user on Math Stack Exchange is confused because they saw the entropy formula as (1/2) ln(2πeσ²) but can't find a proof. They mention some other expressions they found, like (1/2) + ln(√(2π)σ) and are unsure how the square root is removed and the σ is incorporated into the logarithm. They also mention a maximum entropy result and ask for the proof.\n\nNow, the task is to format this problem into the Assumption and Problem sections as per the example. The example for the divisibility problem listed assumptions line by line, each addressing one property. But in the entropy question, I need to identify the assumptions. However, the original question doesn't explicitly state assumptions; it's more about deriving a known result. Maybe the assumptions are related to the properties of the normal distribution and entropy?\n\nWait, the problem is to derive the entropy, so the assumptions would be the definition of differential entropy and the parameters of the normal distribution. Let me think. The entropy H of a continuous distribution with density f(x) is H = -∫ f(x) ln f(x) dx. For the normal distribution, f(x) is the density with mean μ and variance σ². But the formula given in the problem is (1/2) ln(2πeσ²), which is the entropy. So the assumptions should include the definition of entropy, the form of the normal distribution, and maybe that it's the maximum entropy distribution under certain conditions. However, the user's question mentions \"maximum likelihood entropy\" which might be a typo, perhaps they meant \"maximum entropy\". But in the original problem, the user is confused about the derivation steps, not necessarily the maximum entropy principle. Let me check the original question again.\n\nThe user wrote: \"So, what is the proof for the maximum likelihood entropy of the normal distribution?\" Hmm, \"maximum likelihood entropy\" might be a confusion between maximum likelihood estimation and maximum entropy. But the correct term here is probably the entropy of the normal distribution, which is derived using the integral formula. The user also mentions that some proofs show the maximum entropy resembles a certain expression, so perhaps the problem is about showing that the normal distribution maximizes entropy under variance constraints. But the main question is about deriving the entropy formula itself.\n\nSo, structuring the problem: The assumptions would include that X is a normally distributed random variable with mean μ and variance σ², the definition of differential entropy as H = -∫ f(x) ln f(x) dx over the support of f, and maybe that we're considering the continuous case. The problem is to prove that the entropy H is equal to (1/2) ln(2πeσ²).\n\nWait, the user's confusion is about the algebraic manipulation from the expression they found (1/2 + ln(√(2π)σ)) to the given formula. But maybe the actual problem is to derive the entropy formula from the definition, using the normal distribution's PDF. So the key assumptions would be the PDF of the normal distribution and the definition of differential entropy. The problem is to compute H using that PDF and show it equals the given formula.\n\nSo, let's list the assumptions:\n\n1. X is a continuous random variable following a normal distribution with mean μ and variance σ².\n2. The probability density function (PDF) of X is f(x) = (1/(σ√(2π))) exp(-(x-μ)²/(2σ²)).\n3. The differential entropy H of X is defined as H = -∫_{-∞}^{∞} f(x) ln(f(x)) dx.\n\nBut wait, the problem is to derive the entropy, so maybe μ is zero? Or is it general? The formula given in the problem, (1/2) ln(2πeσ²), is for the general case with variance σ², regardless of μ. So the mean μ doesn't affect the entropy because entropy is translation-invariant. Therefore, the assumption could be that the mean is μ (but it cancels out in the integral), or maybe just that the distribution is normal with variance σ².\n\nWait, in the entropy formula, the mean μ doesn't appear because when you compute the integral, the (x-μ) term gets squared and integrated over, but the entropy ends up depending only on σ². So perhaps the assumption is that the distribution is normal with variance σ². However, the PDF does include μ. So to be precise, the assumptions should include the full PDF. Alternatively, perhaps the problem is simplified by assuming μ=0, but I think the general case is needed.\n\nSo, the assumptions should be:\n\n- X is a normally distributed random variable with mean μ and variance σ².\n- The differential entropy H(X) is defined as H = -∫ f(x) ln f(x) dx, where f(x) is the probability density function of X.\n- The integral is taken over all real numbers.\n\nBut in the problem statement, the user is asking to derive the entropy, so the problem is to compute H(X) using the PDF of the normal distribution and show it's equal to (1/2) ln(2πeσ²).\n\nTherefore, the formatted problem would have assumptions about the distribution, the PDF, and the definition of entropy, each as separate points. Let me check the example again. In the example, the assumptions were \"n is an integer\" and \"n is divisible by 7\". Each is a separate line addressing one condition.\n\nSo here, the assumptions are:\n\n1. X follows a normal distribution with mean μ and variance σ².\n2. The differential entropy H of a continuous random variable with probability density function f(x) is given by H = -∫_{-∞}^∞ f(x) ln(f(x)) dx.\n3. The probability density function of X is f(x) = (1/(σ√(2π))) e^{-(x-μ)²/(2σ²)}.\n\nEach of these is a separate assumption. Then the problem is to prove that H = (1/2) ln(2πeσ²).\n\nBut wait, the original question also mentions \"maximum entropy\". The user might be conflating two things: the entropy of the normal distribution and the fact that it maximizes entropy under certain constraints. However, their main confusion is about deriving the entropy formula, so perhaps the problem should focus on that.\n\nSo, putting it all together:\n\n###BEGIN_OF_FORMAT###\nAssumption: \nX is a normally distributed random variable with mean μ and variance σ²,\nThe differential entropy H of a continuous random variable is defined as $H = -\\int_{-\\infty}^{\\infty} f(x) \\ln(f(x)) dx$,\nThe probability density function of X is $f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$.\nProblem:\nProve that the entropy of the normal distribution is $\\frac{1}{2} \\ln(2\\pi e \\sigma^2)$.\n###END_OF_FORMAT###\n\nThat seems to capture the necessary assumptions and the problem. Each assumption is a single condition applied to one object. The problem is to prove the entropy formula.",
    "Original_Problem": "Assumption: \n$X$ is a normally distributed random variable with mean $\\mu$ and variance $\\sigma^2$,\nThe differential entropy $H$ of a continuous random variable is defined as $H = -\\int_{-\\infty}^{\\infty} f(x) \\ln(f(x)) \\, dx$,\nThe probability density function of $X$ is $f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$.\nProblem:\nProve that the entropy of the normal distribution is $\\frac{1}{2} \\ln(2\\pi e \\sigma^2)$.",
    "Original_Problem_with_numerical_assumption": "Assumption:\nAssumption 1: $X$ is a normally distributed random variable with mean $\\mu$ and variance $\\sigma^2$,\nAssumption 2: The differential entropy $H$ of a continuous random variable is defined as $H = -\\int_{-\\infty}^{\\infty} f(x) \\ln(f(x)) \\, dx$,\nAssumption 3: The probability density function of $X$ is $f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$.\nProblem:\nProve that the entropy of the normal distribution is $\\frac{1}{2} \\ln(2\\pi e \\sigma^2)$.",
    "Proof_problem": "Yes",
    "Redundant_assumption": "$$\n\\begin{align}\n\\frac{1}{2} + \\ln(\\sqrt{2\\pi}\\sigma)\n&= \\frac{1}{2} + \\ln(\\color{blue}{\\sqrt{\\color{black}{2\\pi\\sigma^2}}}) \\\\\n&= \\frac{1}{2} + \\color{blue}{\\frac{1}{2}}\\ln(2\\pi\\sigma^2) \\\\\n&= \\frac{1}{2}(1 + \\ln(2\\pi\\sigma^2)) \\\\\n&= \\frac{1}{2}(\\ln(\\mathrm{e}) \\color{red}{+} \\ln(2\\pi\\sigma^2)) \n= \\frac{1}{2}\\ln(\\mathrm{e}\\color{red}{\\cdot}2\\pi\\sigma^2)\n\\end{align}\n$$",
    "Problem_with_redundant_assumption": "Assumption:\nAssumption 1: $X$ is a normally distributed random variable with mean $\\mu$ and variance $\\sigma^2$,\nAssumption 2: The differential entropy $H$ of a continuous random variable is defined as $H = -\\int_{-\\infty}^{\\infty} f(x) \\ln(f(x)) \\, dx$,\nAssumption 3: The probability density function of $X$ is $f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$.\nAssumption 4: $$\n\\begin{align}\n\\frac{1}{2} + \\ln(\\sqrt{2\\pi}\\sigma)\n&= \\frac{1}{2} + \\ln(\\color{blue}{\\sqrt{\\color{black}{2\\pi\\sigma^2}}}) \\\\\n&= \\frac{1}{2} + \\color{blue}{\\frac{1}{2}}\\ln(2\\pi\\sigma^2) \\\\\n&= \\frac{1}{2}(1 + \\ln(2\\pi\\sigma^2)) \\\\\n&= \\frac{1}{2}(\\ln(\\mathrm{e}) \\color{red}{+} \\ln(2\\pi\\sigma^2)) \n= \\frac{1}{2}\\ln(\\mathrm{e}\\color{red}{\\cdot}2\\pi\\sigma^2)\n\\end{align}\n$$\nProblem:\nProve that the entropy of the normal distribution is $\\frac{1}{2} \\ln(2\\pi e \\sigma^2)$."
}