{
    "Link_API": "https://api.stackexchange.com/2.3/questions/570348",
    "Title": "Why is expectation defined by $\\int xf(x)dx$?",
    "Score": 22,
    "Category": "Chứng minh",
    "Tags": "measure-theory, probability-theory, lebesgue-integral",
    "Link": "https://math.stackexchange.com/questions/570348/why-is-expectation-defined-by-int-xfxdx",
    "Content": "I recently found out that the expectation of a random variable $X$ in a probability space $(\\Omega, \\mathcal F,  \\mathbb P)$, $\\mathbb E(X)$, is just the term used in probability theory for the measure of the function $X$; i.e.: $$\n\\mathbb E(X)=\\mathbb P(X)\n$$ or, for a real-valued random variable $X:\\Omega\\to\\mathbb R$: $$\n\\mathbb E(X)=\\int_\\Omega X(x)\\mathbb P(dx)\n$$ The thing is, I am also familiar with the definition of the expectation of a random variable $X:\\Omega\\to\\mathbb R$ with probability distribution function $f$ to be: $$\n\\mathbb E (X)=\\int_\\mathbb R xf(x)dx\n$$ Here, the integral is with respect to the Lebesgue measure on $\\mathbb R$. We can define the function $f$ in the following way: we have $F(x)=\\int_\\mathbb R f(x)dx$, where $F(x)=\\mathbb P(X^{-1}((-\\infty,x]))$. I can't see any way to show that these two definitions are equal to one another.  The only transformation between integrals with respect to different measures that I know is that $\\mu(f^{-1}(g))=\\mu(g\\circ f)$, which gives that $\\mathbb E(g)=\\mu_X(g)$, where $\\mu_X$ is the image measure $\\mathbb P\\circ X^{-1}$, but that doesn't seem to help. Why can we write $\\mathbb E(X)=\\int_\\mathbb R xf(x)dx$?  Why are these two definitions consistent?",
    "AcceptedAnswer": "Let $(\\Omega,\\mathcal{F},P)$ be a probability space and $X:\\Omega\\to\\mathbb{R}$ a random variable, i.e. a $(\\mathcal{F},\\mathcal{B}(\\mathbb{R}))$ -measurable mapping. Then $X$ induces a probability measure on $(\\mathbb{R},\\mathcal{B}(\\mathbb{R}))$ defined by $$\nP_X(B):=P(X^{-1}(B)),\\quad B\\in\\mathcal{B}(\\mathbb{R}),\n$$ which is well-defined since $X$ is measurable. This is called the distribution of $X$ or the pushforward measure of $P$ under $X$ . The definition of the expectation of $X$ is the following Lebesgue integral on $\\Omega$ : $$\n{\\rm E}[X]:=\\int_{\\Omega} X\\,\\mathrm dP=\\int_\\Omega X(\\omega)\\,P(\\mathrm d\\omega),\n$$ given that this integral exists. This integral can always be transformed into a Lebesgue integral on $(\\mathbb{R},\\mathcal{B}(\\mathbb{R}))$ . The following holds: For any integrable random variable $X$ one has $$\n{\\rm E}[X]=\\int_{\\mathbb{R}} x\\,P_X(\\mathrm dx).\\tag{1}\n$$ In the special case where $X$ admits a density function, i.e. $P_X(B)=P(X\\in B)=\\int_B f_X(x)\\,\\mathrm dx$ for all $B\\in\\mathcal{B}(\\mathbb{R})$ and for some measurable, non-negative function $f_X$ , we can simplify $(1)$ even further: $$\n{\\rm E}[X]=\\int_{\\mathbb{R}}xf_X(x)\\,\\mathrm dx. \\tag{2}\n$$ A standard technique for showing the results in $(1)$ and $(2)$ is to a) show that it holds for indicator functions, i.e. $X=\\mathbf{1}_A$ for $A\\in\\mathcal{F}$ , b) show that if it holds for $X$ and $Y$ then it also holds for $\\alpha X+Y$ , $\\alpha\\in\\mathbb{R}$ , and c) if it holds for a sequence $(X_n)$ then it also holds for $\\lim X_n$ .",
    "llm_answer_create_structured_problem": "###BEGIN_OF_FORMAT###\nAssumption: \n$(\\Omega, \\mathcal{F}, \\mathbb{P})$ is a probability space,\n$X: \\Omega \\to \\mathbb{R}$ is a real-valued random variable,\n$\\mu_X = \\mathbb{P} \\circ X^{-1}$ is the pushforward measure (or law) of $X$ on $\\mathbb{R}$,\n$f: \\mathbb{R} \\to \\mathbb{R}$ is the probability density function (PDF) of $X$ with respect to Lebesgue measure,\n$\\mu_X$ is absolutely continuous with respect to Lebesgue measure.\nProblem: \nProve that the two definitions of expectation are consistent:  \n1. $\\mathbb{E}(X) = \\int_{\\Omega} X(\\omega) \\, d\\mathbb{P}(\\omega)$,  \n2. $\\mathbb{E}(X) = \\int_{\\mathbb{R}} x f(x) \\, dx$.\n###END_OF_FORMAT###",
    "reasoning_create_structured_problem": "Okay, let me try to work through this problem step by step. The user is asking why the expectation of a random variable X can be defined both as an integral over the sample space Ω with respect to the probability measure P, and also as an integral over the real numbers using the probability density function f(x) with respect to Lebesgue measure. They want to understand why these two definitions are consistent.\n\nFirst, I need to recall the definitions. The general definition of expectation for a random variable X on a probability space (Ω, F, P) is the integral of X over Ω with respect to P, which is written as E[X] = ∫_Ω X(ω) dP(ω). This is the measure-theoretic definition.\n\nOn the other hand, when we have a real-valued random variable, we often describe its distribution using a probability density function f(x). Then the expectation is given by E[X] = ∫_{R} x f(x) dx. The user is confused about how these two integrals relate and why they are equal.\n\nI think the key here is the concept of the pushforward measure or the law of X, often denoted as μ_X = P ∘ X^{-1}, which is a measure on R. The integral over Ω of X dP should be equal to the integral over R of x dμ_X(x) by the law of the unconscious statistician (although maybe that's for functions of X). Wait, but more formally, this is the change of variables formula in integration. Specifically, if we have a measurable function X: Ω → R, then ∫_Ω X(ω) dP(ω) = ∫_R x dμ_X(x), where μ_X is the pushforward measure. \n\nBut the user is comparing this to the integral ∫_R x f(x) dx, which is the expectation when f is the density of μ_X with respect to Lebesgue measure. So, if μ_X is absolutely continuous with respect to Lebesgue measure, then by the Radon-Nikodym theorem, there exists a density f such that dμ_X = f(x) dx. Therefore, ∫_R x dμ_X(x) = ∫_R x f(x) dx. \n\nPutting it all together, the two expressions for expectation are the same because of the change of variables (moving the integral from Ω to R via the distribution of X) and then expressing the integral with respect to the pushforward measure using the density function. \n\nWait, but maybe I should check the steps again. Let's see:\n\n1. Start with E[X] = ∫_Ω X(ω) dP(ω). \n2. By the change of variables formula (theorem), this is equal to ∫_R x dμ_X(x), where μ_X is the distribution of X, i.e., μ_X(A) = P(X ∈ A).\n3. If μ_X is absolutely continuous with respect to Lebesgue measure λ, then there exists a density f such that dμ_X = f dλ. Therefore, the integral becomes ∫_R x f(x) dx.\n\nTherefore, the two definitions are equivalent when X has a density f with respect to Lebesgue measure. If X doesn't have a density (e.g., it's discrete or singular), then the second definition wouldn't apply, but the first one still holds.\n\nThe user might be missing the connection between the pushforward measure and the density function. The key steps are the change of variables and the use of Radon-Nikodym derivative (the density f) to express the integral in terms of Lebesgue measure.\n\nAnother point is that the distribution function F(x) = μ_X((-∞, x]) = P(X ≤ x), and if F is absolutely continuous, then F(x) = ∫_{-∞}^x f(t) dt, so f is the derivative of F almost everywhere. Thus, the existence of f allows us to write the expectation as ∫ x f(x) dx.\n\nSo, putting this all together, the consistency of the two definitions comes from the change of variables and the existence of the density function when applicable. The first definition is general, the second is a special case when the distribution has a density.",
    "Original_Problem": "Assumption: \n$(\\Omega, \\mathcal{F}, \\mathbb{P})$ is a probability space,\n$X: \\Omega \\to \\mathbb{R}$ is a real-valued random variable,\n$\\mu_X = \\mathbb{P} \\circ X^{-1}$ is the pushforward measure (or law) of $X$ on $\\mathbb{R}$,\n$f: \\mathbb{R} \\to \\mathbb{R}$ is the probability density function (PDF) of $X$ with respect to Lebesgue measure,\n$\\mu_X$ is absolutely continuous with respect to Lebesgue measure.\nProblem: \nProve that the two definitions of expectation are consistent:  \n1. $\\mathbb{E}(X) = \\int_{\\Omega} X(\\omega) \\, d\\mathbb{P}(\\omega)$,  \n2. $\\mathbb{E}(X) = \\int_{\\mathbb{R}} x f(x) \\, dx$.",
    "Original_Problem_with_numerical_assumption": "Assumption:\nAssumption 1: $(\\Omega, \\mathcal{F}, \\mathbb{P})$ is a probability space,\nAssumption 2: $X: \\Omega \\to \\mathbb{R}$ is a real-valued random variable,\nAssumption 3: $\\mu_X = \\mathbb{P} \\circ X^{-1}$ is the pushforward measure (or law) of $X$ on $\\mathbb{R}$,\nAssumption 4: $f: \\mathbb{R} \\to \\mathbb{R}$ is the probability density function (PDF) of $X$ with respect to Lebesgue measure,\nAssumption 5: $\\mu_X$ is absolutely continuous with respect to Lebesgue measure.\nProblem: \nProve that the two definitions of expectation are consistent:  \n1. $\\mathbb{E}(X) = \\int_{\\Omega} X(\\omega) \\, d\\mathbb{P}(\\omega)$,  \n2. $\\mathbb{E}(X) = \\int_{\\mathbb{R}} x f(x) \\, dx$.",
    "Proof_problem": "Yes",
    "Redundant_assumption": "$$\n{\\rm E}[X]:=\\int_{\\Omega} X\\,\\mathrm dP=\\int_\\Omega X(\\omega)\\,P(\\mathrm d\\omega),\n$$",
    "Problem_with_redundant_assumption": "Assumption:\nAssumption 1: $(\\Omega, \\mathcal{F}, \\mathbb{P})$ is a probability space,\nAssumption 2: $X: \\Omega \\to \\mathbb{R}$ is a real-valued random variable,\nAssumption 3: $\\mu_X = \\mathbb{P} \\circ X^{-1}$ is the pushforward measure (or law) of $X$ on $\\mathbb{R}$,\nAssumption 4: $f: \\mathbb{R} \\to \\mathbb{R}$ is the probability density function (PDF) of $X$ with respect to Lebesgue measure,\nAssumption 5: $\\mu_X$ is absolutely continuous with respect to Lebesgue measure.\nAssumption 6: $$\n{\\rm E}[X]:=\\int_{\\Omega} X\\,\\mathrm dP=\\int_\\Omega X(\\omega)\\,P(\\mathrm d\\omega),\n$$\nProblem: \nProve that the two definitions of expectation are consistent:  \n1. $\\mathbb{E}(X) = \\int_{\\Omega} X(\\omega) \\, d\\mathbb{P}(\\omega)$,  \n2. $\\mathbb{E}(X) = \\int_{\\mathbb{R}} x f(x) \\, dx$."
}