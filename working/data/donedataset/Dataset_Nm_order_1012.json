{
    "Link_API": "https://api.stackexchange.com/2.3/questions/1766613",
    "Title": "Proof: Sum of dimension of orthogonal complement and vector subspace",
    "Score": 24,
    "Category": "Chứng minh",
    "Tags": "linear-algebra, vector-spaces, inner-products",
    "Link": "https://math.stackexchange.com/questions/1766613/proof-sum-of-dimension-of-orthogonal-complement-and-vector-subspace",
    "Content": "Let $V$ be a finite dimensional real vector space with inner product $\\langle \\, , \\rangle$ and let $W$ be a subspace of $V$. The orthogonal complement of $W$ is defined as\n  $$\nW^\\perp= \\left\\{ v \\in V \\,:\\, \\langle v,w \\rangle = 0 \\text{ for all } w \\in W \\right\\}.\n$$\n  Prove the following: $\\dim W + \\dim W^\\perp= \\dim V$. I'm not sure how to find the relationship between number of basis vectors in $W$ and $W^\\perp$.",
    "AcceptedAnswer": "Let $\\beta=\\{w_1,w_2,\\ldots,w_k\\}$ and $\\gamma=\\{x_1,x_2,\\ldots,x_m\\}$ be the bases for $W$ and $W^\\perp$, respectively. It suffices to show that\n$$\\beta\\cup\\gamma=\\{w_1,w_2,\\ldots,w_k,x_1,x_2,\\ldots,x_m\\}$$\nis a basis for $V$.\nGiven $v\\in V$, then it is well-known that $v=v_1+v_2$ for some $v_1\\in W$ and $v_2\\in W^\\perp$. Also because $\\beta$ and $\\gamma$ are bases for $W$ and $W^\\perp$, respectively, there exist scalars\n$a_1,a_2,\\ldots,a_k,b_1,b_2,\\ldots,b_m$ such that\n$v_1=\\displaystyle\\sum_{i=1}^ka_iw_i$ and $v_2=\\displaystyle\\sum_{j=1}^mb_jx_j$. Therefore\n$$v=v_1+v_2=\\sum_{i=1}^ka_iw_i+\\sum_{j=1}^mb_jx_j,$$\nwhich follows that $\\beta\\cup\\gamma$ generates $V$. Next, we show that \n$\\beta\\cup\\gamma$ is linearly independent. Given \n$c_1,c_2,\\ldots,c_k,d_1,d_2,\\ldots,d_m$ such that\n$\\displaystyle\\sum_{i=1}^kc_iw_i+\\sum_{j=1}^md_jx_j={\\it 0}$, then\n$\\displaystyle\\sum_{i=1}^kc_iw_i=-\\sum_{j=1}^md_jx_j$. It follows that \n$$\\sum_{i=1}^kc_iw_i\\in W\\cap W^\\perp\\quad\\mbox{and}\\quad\n\\sum_{j=1}^md_jx_j\\in W\\cap W^\\perp.$$\nBut since $W\\cap W^\\perp=\\{{\\it 0}\\,\\}$ (gievn $x\\in W\\cap W^\\perp$,\nwe have $\\langle x,x\\rangle=0$ and thus $x={\\it 0}\\,$), we have\n$\\displaystyle\\sum_{i=1}^kc_iw_i=\\sum_{j=1}^md_jx_j={\\it 0}$. Therefore\n$c_i=0$ and $d_j=0$ for each $i,j$ becasue $\\beta$ and $\\gamma$ are bases\nfor $W$ and $W^\\perp$, respectively. Hence we conclude that $\\beta\\cup\\gamma$ is linearly independent.",
    "llm_answer_create_structured_problem": "###BEGIN_OF_FORMAT###\nAssumption: \n$V$ is a finite dimensional real vector space,\n$V$ is equipped with an inner product $\\langle \\, , \\rangle$,\n$W$ is a subspace of $V$,\n$W^\\perp$ is the orthogonal complement of $W$ under $\\langle \\, , \\rangle$.\nProblem:\nProve that $\\dim W + \\dim W^\\perp = \\dim V$.\n###END_OF_FORMAT###",
    "reasoning_create_structured_problem": "Okay, let me try to work through this problem step by step. So, the task is to prove that the sum of the dimensions of a subspace W and its orthogonal complement W⊥ in a finite-dimensional real vector space V equals the dimension of V itself. Hmm, I remember from linear algebra that this is a fundamental result, but I need to recall exactly how the proof goes.\n\nFirst, let me restate the given information to make sure I understand it. We have a finite-dimensional real vector space V with an inner product ⟨ , ⟩. W is a subspace of V, and W⊥ is its orthogonal complement, which consists of all vectors in V that are orthogonal to every vector in W. The goal is to show that dim W + dim W⊥ = dim V.\n\nAlright, so I need to connect the dimensions of these two subspaces. Since V is finite-dimensional, both W and W⊥ are also finite-dimensional. I think the key here might involve the properties of orthonormal bases or perhaps the Gram-Schmidt process. Alternatively, maybe using the rank-nullity theorem somehow?\n\nWait, the rank-nullity theorem relates the dimensions of the image and kernel of a linear map to the dimension of the domain. Maybe I can define a linear map whose kernel or image relates to W or W⊥. Let me think. If I can find a linear transformation such that W is the kernel and W⊥ is the image (or vice versa), then rank-nullity would directly give the result. But how to construct such a map?\n\nAlternatively, maybe using the inner product to create a map. For example, consider the map from V to its dual space induced by the inner product. Then, W⊥ would be the annihilator of W. But dual spaces might complicate things since the problem is stated in terms of the same space V. Wait, but since we have an inner product, V is isomorphic to its dual, so maybe that's a path. However, I think there's a more straightforward approach without invoking dual spaces.\n\nAnother approach: Start with a basis for W and extend it to a basis for V. Then, use the Gram-Schmidt process to orthogonalize the entire basis. The vectors added to extend the basis of W to V would then form a basis for W⊥. But how to formalize that?\n\nLet me recall the Gram-Schmidt process. If I have a basis for W, say {w₁, w₂, ..., w_k}, and then extend it to a basis for V by adding vectors {v₁, v₂, ..., v_{n-k}}, then applying Gram-Schmidt would orthogonalize all these vectors. The resulting vectors would be an orthogonal basis for V, where the first k vectors span W, and the remaining n - k vectors are orthogonal to W, hence in W⊥. Therefore, the dimension of W⊥ would be n - k, so adding k + (n - k) = n, which is dim V. That seems plausible, but does this cover all possibilities?\n\nWait, but the problem states that V is a real vector space, so the Gram-Schmidt process works because we have an inner product. So this might be a valid approach. However, maybe there's a more general method that doesn't require constructing an explicit basis. Let me think about the rank-nullity theorem again.\n\nSuppose I define a linear map T: V → V such that T(v) is the projection of v onto W. Then, the kernel of T would be W⊥, since any vector in W⊥ maps to zero. The image of T would be W itself. Then, by the rank-nullity theorem, dim V = dim im T + dim ker T = dim W + dim W⊥. That seems to be exactly what we need! But wait, how is T defined? The projection map onto W is indeed a linear map, and its kernel is W⊥. So this approach should work.\n\nBut let me verify that. The projection map T: V → W is defined such that for any v in V, T(v) is the unique vector in W such that v - T(v) is in W⊥. This is the orthogonal projection. The kernel of T is exactly W⊥ because if T(v) = 0, then v must be in W⊥, and conversely, if v is in W⊥, then T(v) = 0. The image of T is W, so rank T = dim W. Then, rank-nullity gives dim V = dim W + dim W⊥. That's concise and uses the rank-nullity theorem. But is there a step missing here? For instance, do we need to show that T is indeed linear, and that its kernel and image are as claimed?\n\nAlternatively, maybe another way is to use the fact that W and W⊥ are complementary subspaces, meaning that V is the direct sum of W and W⊥. If that's the case, then the dimension of V is the sum of the dimensions of W and W⊥. But to prove they are complementary, we need to show that every vector in V can be uniquely written as the sum of a vector in W and a vector in W⊥. That's precisely the projection theorem, which holds in finite-dimensional inner product spaces. So once we have that V = W ⊕ W⊥, then the dimension formula follows directly.\n\nHowever, the projection theorem might itself rely on the dimension formula we're trying to prove, so we need to be careful about circular reasoning. Let me check. The projection theorem states that for any vector v in V, there exists a unique vector w in W such that v - w is in W⊥. To prove this, we might use the Gram-Schmidt process, which in turn requires constructing an orthonormal basis for W and extending it to V. Then, the orthogonal complement is spanned by the remaining vectors. But in that case, the dimension formula would come from counting the basis vectors.\n\nAlternatively, let's consider the inner product as a bilinear form. If we take a basis for W and then consider the matrix of the inner product restricted to W, perhaps we can find a relationship. Hmm, this seems more complicated.\n\nWait, here's another idea. Let’s pick an orthonormal basis for W, say {e₁, e₂, ..., e_k}, and extend it to an orthonormal basis for V, {e₁, ..., e_k, e_{k+1}, ..., e_n}. Then, the vectors e_{k+1}, ..., e_n are orthogonal to all e₁, ..., e_k, hence they form an orthonormal basis for W⊥. Therefore, the dimension of W⊥ is n - k, so adding k + (n - k) gives n = dim V. This works, but again, it requires the existence of an orthonormal basis, which is guaranteed by Gram-Schmidt. But does this approach count as a proof?\n\nYes, because in finite-dimensional inner product spaces, we can always construct an orthonormal basis via Gram-Schmidt. Therefore, extending an orthonormal basis of W to V gives the necessary result. The key step is that the orthogonal complement is spanned by the remaining basis vectors. Therefore, their dimensions add up.\n\nBut maybe the problem expects a proof that doesn't rely on constructing an orthonormal basis, but instead uses more general linear algebra principles. The rank-nullity approach seems elegant. Let me formalize that:\n\nDefine the linear map T: V → W as the orthogonal projection onto W. Then:\n\n1. T is linear.\n2. The image of T is W, so rank T = dim W.\n3. The kernel of T is W⊥, because T(v) = 0 iff v is orthogonal to every vector in W.\n\nBy the rank-nullity theorem, dim V = rank T + nullity T = dim W + dim W⊥.\n\nThis proof is concise and uses the rank-nullity theorem, which is a fundamental result in linear algebra. However, to fully justify this, we need to confirm that the orthogonal projection T is indeed a linear map with the specified image and kernel.\n\nAlternatively, another approach using the annihilator. Since V is finite-dimensional, the dual space V* is isomorphic to V via the inner product. The annihilator of W in V* would have dimension dim V - dim W. But under the isomorphism, W⊥ corresponds to the annihilator of W, hence dim W⊥ = dim V - dim W. But this might be more advanced than necessary, involving dual spaces which might not be in the problem's assumed background.\n\nGiven that the problem is about a real vector space with an inner product, the most straightforward proofs would be either through the rank-nullity theorem applied to the projection map or via constructing a basis as in Gram-Schmidt.\n\nAnother angle: Suppose we take a basis for W, say {w₁, w₂, ..., w_k}, and then consider the linear functionals defined by the inner product with each w_i. That is, for each w_i, define f_i: V → ℝ by f_i(v) = ⟨v, w_i⟩. Then, W⊥ is the intersection of the kernels of all f_i. Since the f_i are linear functionals, their kernels are hyperplanes, and the intersection would have dimension dim V - rank of the set {f₁, ..., f_k}. Since the w_i are linearly independent, the functionals f_i are also linearly independent. Therefore, the rank is k, so the intersection of their kernels (which is W⊥) has dimension dim V - k. Hence, dim W + dim W⊥ = k + (dim V - k) = dim V.\n\nThis is another valid approach using the rank of a set of linear functionals. However, it requires knowledge about linear functionals and their ranks. The key here is that the dimension of the solution space (W⊥) to the system of equations ⟨v, w_i⟩ = 0 for each i is dim V - rank of the system, which is k. Thus, dim W⊥ = dim V - k, leading to the result.\n\nBut perhaps this is overcomplicating. The projection map approach seems the most straightforward. Let me check the steps again for clarity:\n\n1. Define T: V → W as the orthogonal projection onto W. This map is linear.\n2. Show that im T = W. Since T projects onto W, every vector in W is the image of itself under T. Hence, im T is W, so rank T = dim W.\n3. Show that ker T = W⊥. If v is in ker T, then T(v) = 0, which implies v is orthogonal to W, so v ∈ W⊥. Conversely, if v ∈ W⊥, then its projection onto W is 0, so T(v) = 0. Hence, ker T = W⊥, so nullity T = dim W⊥.\n4. By the rank-nullity theorem, dim V = rank T + nullity T = dim W + dim W⊥.\n\nThis seems solid. The main point is confirming that T is indeed a linear map with the specified image and kernel. Since orthogonal projection is linear and satisfies these properties, the proof holds.\n\nAlternatively, if the student hasn't encountered the projection map in this form, they might approach it by constructing a basis. Start with a basis for W, extend it to a basis for V, apply Gram-Schmidt to make it orthogonal, and then note that the added vectors form a basis for W⊥. Therefore, the dimensions add up. This is more constructive but requires going through the Gram-Schmidt process.\n\nEither way, both approaches lead to the conclusion that dim W + dim W⊥ = dim V. The problem doesn't specify which method to use, so either would be acceptable. However, the rank-nullity method is more concise and leverages a fundamental theorem in linear algebra, making it perhaps the preferred approach in a proof.",
    "Original_Problem": "Assumption: \n$V$ is a finite dimensional real vector space,\n$V$ is equipped with an inner product $\\langle \\, , \\rangle$,\n$W$ is a subspace of $V$,\n$W^\\perp$ is the orthogonal complement of $W$ under $\\langle \\, , \\rangle$.\nProblem:\nProve that $\\dim W + \\dim W^\\perp = \\dim V$.",
    "Original_Problem_with_numerical_assumption": "Assumption:\nAssumption 1: $V$ is a finite dimensional real vector space,\nAssumption 2: $V$ is equipped with an inner product $\\langle \\, , \\rangle$,\nAssumption 3: $W$ is a subspace of $V$,\nAssumption 4: $W^\\perp$ is the orthogonal complement of $W$ under $\\langle \\, , \\rangle$.\nProblem:\nProve that $\\dim W + \\dim W^\\perp = \\dim V$.",
    "Proof_problem": "Yes",
    "Redundant_assumption": "$$v=v_1+v_2=\\sum_{i=1}^ka_iw_i+\\sum_{j=1}^mb_jx_j,$$",
    "Problem_with_redundant_assumption": "Assumption:\nAssumption 1: $V$ is a finite dimensional real vector space,\nAssumption 2: $V$ is equipped with an inner product $\\langle \\, , \\rangle$,\nAssumption 3: $W$ is a subspace of $V$,\nAssumption 4: $W^\\perp$ is the orthogonal complement of $W$ under $\\langle \\, , \\rangle$.\nAssumption 5: $$v=v_1+v_2=\\sum_{i=1}^ka_iw_i+\\sum_{j=1}^mb_jx_j,$$\nProblem:\nProve that $\\dim W + \\dim W^\\perp = \\dim V$."
}