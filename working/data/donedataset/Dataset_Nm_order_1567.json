{
    "Link_API": "https://api.stackexchange.com/2.3/questions/147356",
    "Title": "Proof of the L&#39;H&#244;pital Rule for $\\frac{\\infty}{\\infty}$",
    "Score": 20,
    "Category": "Chứng minh",
    "Tags": "real-analysis, limits",
    "Link": "https://math.stackexchange.com/questions/147356/proof-of-the-lh%c3%b4pital-rule-for-frac-infty-infty",
    "Content": "I ask for the proof of the L'Hôpital rule for the indeterminate form $\\frac{\\infty}{\\infty}$ utilizing the rule for the form $\\frac{0}{0}$. Theorem: Let $f,g:(a,b)\\to \\mathbb{R}$ be two differentiable functions such as that: \n$\\forall x\\in(a,b)\\ \\ g(x)\\neq 0\\text{ and }g^{\\prime}(x)\\neq 0$ and $\\lim_{x\\to a^+}f(x)=\\lim_{x\\to a^+}g(x)=+\\infty$\nIf the limit $$\\lim_{x\\to a^+}\\frac{f^{\\prime}(x)}{g^{\\prime}(x)}$$ exists and is finite, then\n $$\\lim_{x\\to a^+}\\frac{f(x)}{g(x)}=\\lim_{x\\to a^+}\\frac{f^{\\prime}(x)}{g^{\\prime}(x)}$$ My attempt: \nSince $\\lim_{x\\to a^+}f(x)=+\\infty$, $$\\exists \\delta>0:a<x<a+\\delta<b\\Rightarrow f(x)>0\\Rightarrow f(x)\\neq 0$$ \nLet $F,G:(a,a+\\delta)\\to \\mathbb{R}$, $F(x)=\\frac{1}{f(x)}$, $G(x)=\\frac{1}{g(x)}$. Then by the hypothesis $\\lim_{x\\to a^+}F(x)=\\lim_{x\\to a^+}G(x)=0$, $$\\forall x\\in(a,b)\\ \\ G(x)\\neq 0\\text{ and }G^{\\prime}(x)=-\\frac{1}{g^2(x)}g^{\\prime}(x)\\neq 0$$\nThe question is, does the limit $$\\lim_{x\\to a^+}\\frac{F^{\\prime}(x)}{G^{\\prime}(x)}=\\lim_{x\\to a^+}\\frac{-\\frac{1}{f^2(x)}f^{\\prime}(x)}{-\\frac{1}{g^2(x)}g^{\\prime}(X)}=\\lim_{x\\to a^+}\\frac{g^2(x)f^{\\prime}(x)}{f^2(x)g^{\\prime}(x)}$$ exist? The limit $$\\lim_{x\\to a^+}\\frac{f^{\\prime}(x)}{g^{\\prime}(x)}$$ exists by the hypothesis but we don't know if the limit $\\displaystyle\\lim_{x\\to a^+}\\frac{g^2(x)}{f^2(x)}$ exists to deduce that the limit $$\\lim_{x\\to a^+}\\frac{F^{\\prime}(x)}{G^{\\prime}(x)}$$ exists to use the L'Hôpital Rule for the form $\\frac{0}{0}$. EDIT: After discussing it with other users in the site, we came to the conclusion that this proof is only partial and can't logically be continued to yield the Theorem. \nAs a result, the rule for the $\\frac{0}{0}$ form can't be used to proove the rule for the  $\\frac{\\infty}{\\infty}$ form.\nMr. Tavares and myself have already given two different proofs (with the pretty much the same main idea) of the Theorem in question using Cauchy's Mean Value Theorem. You can read them below. You can also read the proof Rudin gives for a stronger version of the Theorem (that does not suppose that $\\lim_{x\\to a^+}f(x)=+\\infty$) in his book Principle of Meathematical Analysis. If you have any objections in either proofs please let me know. Thank you.",
    "AcceptedAnswer": "Let \n$$\\begin{equation}\n\\lim_{x\\rightarrow a^{+}}\\frac{f^{\\prime }(x)}{g^{\\prime }(x)}=L.  \\tag{1}\n\\end{equation}$$\nThen for each $\\delta >0$ there exists a real $\\beta \\in \\left( a,b\\right) $\nsuch that for all $x\\in \\left( a,\\beta \\right) $\n$$\\begin{equation}\n\\left\\vert \\frac{f^{\\prime }(x)}{g^{\\prime }(x)}-L\\right\\vert <\\delta . \n\\tag{2}\n\\end{equation}$$\nLet $x\\in \\left( a,\\beta \\right) ,y\\in \\left( a,\\beta \\right) ,x<y$. Since\nthe functions $f,g$ are continuous and differentiable on $\\left[ x,y\\right] $\nwe can apply the Cauchy Mean Value Theorem. Consequently, there exists a $c\\in \\left[ x,y\\right] \\subset \\left( a,\\beta \\right) $ such that\n$$\\begin{equation}\n\\frac{f(x)-f(y)}{g(x)-g(y)}=\\frac{f^{\\prime }(c)}{g^{\\prime }(c)}.  \\tag{3}\n\\end{equation}$$\nHence for $x\\in \\left( a,\\beta \\right) ,y\\in \\left( a,\\beta \\right) ,x<y$\n$$\\begin{eqnarray}\n\\left\\vert \\frac{f(x)-f(y)}{g(x)-g(y)}-L\\right\\vert  &<&\\delta   \n\\\\\n\\\\&&\\\\\n\\left\\vert \\frac{f(x)/g(x)-f(y)/g(x)}{1-g(y)/g(x)}-L\\right\\vert  &<&\\delta .\n\\tag{4}\n\\end{eqnarray}$$\nAssume $\\lim_{x\\rightarrow a^{+}}f(x)=\\lim_{x\\rightarrow a^{+}}g(x)=+\\infty $\nand fix $y$. Then $g(y)/g(x)\\rightarrow 0$ and there exists a $\\gamma \\in\n\\left( a,\\beta \\right) $ such that for $x\\in \\left( a,\\gamma \\right) $, we\nhave $g(x)>0$ and $g(x)/g(y)>1$. Inequality $(4)$ implies \n$$\\begin{equation}\n\\left( 1-\\frac{g(y)}{g(x)}\\right) \\left( L-\\delta \\right) <\\frac{f(x)}{g(x)}-\\frac{f(y)}{g(x)}<\\left( 1-\\frac{g(y)}{g(x)}\\right) \\left( L+\\delta \\right) .\n\\tag{5}\n\\end{equation}$$\nLetting $x\\rightarrow a^{+}$ we conclude that\n$$\\begin{equation}\n\\lim_{x\\rightarrow a^{+}}\\frac{f(x)}{g(x)}=L.  \\tag{6}\n\\end{equation}$$ Adapted from J. Campos Ferreira, Introdução à Análise Matemática , Teorema 11, p. 386 and J. Santos Guerreiro, Curso de Análise Matemática , Proposição 5.2.3.2, p. 314.",
    "llm_answer_create_structured_problem": "###BEGIN_OF_FORMAT###\nassumption: \n$f$ and $g$ are differentiable functions on the interval $(a,b)$,\nFor every $x$ in $(a,b)$, $g(x) \\neq 0$,\nFor every $x$ in $(a,b)$, $g'(x) \\neq 0$,\n$\\lim_{x \\to a^+} f(x) = +\\infty$,\n$\\lim_{x \\to a^+} g(x) = +\\infty$,\nThe limit $\\lim_{x \\to a^+} \\frac{f'(x)}{g'(x)}$ exists,\nThe limit $\\lim_{x \\to a^+} \\frac{f'(x)}{g'(x)}$ is finite.\nproblem:\nProve that $$\\lim_{x \\to a^+} \\frac{f(x)}{g(x)} = \\lim_{x \\to a^+} \\frac{f'(x)}{g'(x)}.$$\n###",
    "reasoning_create_structured_problem": "Okay, let me try to structure this problem according to the given format. The user wants me to act as a math professor, read the Math Stack Exchange question, filter out the unnecessary information, and present it with Assumptions and Problem sections.\n\nFirst, I need to parse the original question. The title is about proving L'Hôpital's Rule for the ∞/∞ case using the 0/0 case. The user mentions their attempt by defining F(x) = 1/f(x) and G(x) = 1/g(x), converting the ∞/∞ form to 0/0. But they ran into an issue with the existence of the limit involving F’/G’. The user then concludes that this approach is partial and suggests using Cauchy's Mean Value Theorem instead, referencing other proofs.\n\nNow, I need to extract the assumptions and the problem statement. The original problem is to prove the theorem stated. The assumptions given in the theorem are:\n\n1. f and g are differentiable on (a, b).\n2. For all x in (a, b), g(x) ≠ 0 and g’(x) ≠ 0.\n3. The limits of f(x) and g(x) as x approaches a+ are both +∞.\n4. The limit of f’(x)/g’(x) as x approaches a+ exists and is finite.\n\nEach assumption should be listed line by line, addressing one property per line. The problem is to prove that under these conditions, the limit of f(x)/g(x) equals the limit of f’(x)/g’(x).\n\nI need to check if the user added any extra conditions. The original theorem states the four points above. The user's attempt mentions converting to F and G, but that's part of their method, not part of the theorem's assumptions. The key assumptions are in the theorem statement. The problem is to prove the equality of the limits using those conditions.\n\nSo, the Assumptions section should list each condition from the theorem. The Problem is to prove the limit statement. The user's discussion about their attempt and other proofs is extraneous for the formatted problem.",
    "Original_Problem": "assumption: \n$f$ and $g$ are differentiable functions on the interval $(a,b)$,\nFor every $x$ in $(a,b)$, $g(x) \\neq 0$,\nFor every $x$ in $(a,b)$, $g'(x) \\neq 0$,\n$\\lim_{x \\to a^+} f(x) = +\\infty$,\n$\\lim_{x \\to a^+} g(x) = +\\infty$,\nThe limit $\\lim_{x \\to a^+} \\frac{f'(x)}{g'(x)}$ exists,\nThe limit $\\lim_{x \\to a^+} \\frac{f'(x)}{g'(x)}$ is finite.\nproblem:\nProve that $$\\lim_{x \\to a^+} \\frac{f(x)}{g(x)} = \\lim_{x \\to a^+} \\frac{f'(x)}{g'(x)}.$$",
    "Original_Problem_with_numerical_assumption": "Assumption:\nAssumption 1: $f$ and $g$ are differentiable functions on the interval $(a,b)$,\nAssumption 2: For every $x$ in $(a,b)$, $g(x) \\neq 0$,\nAssumption 3: For every $x$ in $(a,b)$, $g'(x) \\neq 0$,\nAssumption 4: $\\lim_{x \\to a^+} f(x) = +\\infty$,\nAssumption 5: $\\lim_{x \\to a^+} g(x) = +\\infty$,\nAssumption 6: The limit $\\lim_{x \\to a^+} \\frac{f'(x)}{g'(x)}$ exists,\nAssumption 7: The limit $\\lim_{x \\to a^+} \\frac{f'(x)}{g'(x)}$ is finite.\nProblem:\nProve that $$\\lim_{x \\to a^+} \\frac{f(x)}{g(x)} = \\lim_{x \\to a^+} \\frac{f'(x)}{g'(x)}.$$",
    "Proof_problem": "Yes",
    "Redundant_assumption": "$$\\begin{equation}\n\\lim_{x\\rightarrow a^{+}}\\frac{f(x)}{g(x)}=L.  \\tag{6}\n\\end{equation}$$",
    "Problem_with_redundant_assumption": "Assumption:\nAssumption 1: $f$ and $g$ are differentiable functions on the interval $(a,b)$,\nAssumption 2: For every $x$ in $(a,b)$, $g(x) \\neq 0$,\nAssumption 3: For every $x$ in $(a,b)$, $g'(x) \\neq 0$,\nAssumption 4: $\\lim_{x \\to a^+} f(x) = +\\infty$,\nAssumption 5: $\\lim_{x \\to a^+} g(x) = +\\infty$,\nAssumption 6: The limit $\\lim_{x \\to a^+} \\frac{f'(x)}{g'(x)}$ exists,\nAssumption 7: The limit $\\lim_{x \\to a^+} \\frac{f'(x)}{g'(x)}$ is finite.\nAssumption 8: $$\\begin{equation}\n\\lim_{x\\rightarrow a^{+}}\\frac{f(x)}{g(x)}=L.  \\tag{6}\n\\end{equation}$$\nProblem:\nProve that $$\\lim_{x \\to a^+} \\frac{f(x)}{g(x)} = \\lim_{x \\to a^+} \\frac{f'(x)}{g'(x)}.$$"
}